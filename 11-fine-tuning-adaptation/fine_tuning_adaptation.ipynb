{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning and Adaptation\n",
    "\n",
    "This notebook provides an interactive guide to adapting pretrained language models for specific tasks using various parameter-efficient methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Fine-tuning\n",
    "\n",
    "Fine-tuning adapts pretrained models to specific tasks. We'll explore methods from full fine-tuning to parameter-efficient approaches like LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import math\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fine-tuning Paradigms Overview\n",
    "\n",
    "Let's visualize different fine-tuning approaches and their trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning methods comparison\n",
    "methods_data = {\n",
    "    'Method': ['Full Fine-tuning', 'LoRA', 'QLoRA', 'Adapters', 'Prompt Tuning', \n",
    "               'Prefix Tuning', 'BitFit', 'IA³'],\n",
    "    'Trainable Params (%)': [100, 0.5, 0.5, 3, 0.01, 0.1, 0.1, 0.01],\n",
    "    'Memory Usage': [100, 20, 10, 25, 5, 5, 15, 5],\n",
    "    'Quality (%)': [100, 98, 97, 96, 90, 92, 85, 94],\n",
    "    'Training Speed': [1, 3, 2.5, 2.5, 5, 4, 4, 5]\n",
    "}\n",
    "\n",
    "df_methods = pd.DataFrame(methods_data)\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Parameters vs Quality scatter\n",
    "ax = axes[0, 0]\n",
    "scatter = ax.scatter(df_methods['Trainable Params (%)'], \n",
    "                    df_methods['Quality (%)'],\n",
    "                    s=df_methods['Memory Usage']*5,\n",
    "                    c=df_methods['Training Speed'],\n",
    "                    cmap='viridis', alpha=0.6)\n",
    "\n",
    "# Add labels\n",
    "for idx, row in df_methods.iterrows():\n",
    "    ax.annotate(row['Method'], \n",
    "               (row['Trainable Params (%)'], row['Quality (%)']),\n",
    "               xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax.set_xlabel('Trainable Parameters (%)', fontsize=12)\n",
    "ax.set_ylabel('Quality (% of full fine-tuning)', fontsize=12)\n",
    "ax.set_title('Parameter Efficiency vs Quality Trade-off', fontsize=14)\n",
    "ax.set_xscale('log')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(scatter, ax=ax)\n",
    "cbar.set_label('Training Speed (relative)', fontsize=10)\n",
    "\n",
    "# 2. Method comparison radar chart\n",
    "ax = axes[0, 1]\n",
    "categories = ['Parameters\\n(inverse)', 'Memory\\n(inverse)', 'Quality', 'Speed']\n",
    "num_vars = len(categories)\n",
    "\n",
    "# Compute angles\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "angles += angles[:1]\n",
    "\n",
    "# Select methods to compare\n",
    "methods_to_compare = ['Full Fine-tuning', 'LoRA', 'Adapters', 'Prompt Tuning']\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "ax = plt.subplot(2, 2, 2, projection='polar')\n",
    "for method, color in zip(methods_to_compare, colors):\n",
    "    row = df_methods[df_methods['Method'] == method].iloc[0]\n",
    "    \n",
    "    # Normalize values (invert params and memory for better visualization)\n",
    "    values = [\n",
    "        100 - row['Trainable Params (%)'],  # Inverse\n",
    "        100 - row['Memory Usage'],  # Inverse\n",
    "        row['Quality (%)'],\n",
    "        row['Training Speed'] * 20\n",
    "    ]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=method, color=color)\n",
    "    ax.fill(angles, values, alpha=0.15, color=color)\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_title('Method Comparison (Higher is Better)', fontsize=14, y=1.08)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "# 3. Memory usage comparison\n",
    "ax = axes[1, 0]\n",
    "methods_sorted = df_methods.sort_values('Memory Usage')\n",
    "bars = ax.barh(methods_sorted['Method'], methods_sorted['Memory Usage'])\n",
    "\n",
    "# Color bars by efficiency\n",
    "colors = plt.cm.RdYlGn(1 - methods_sorted['Memory Usage'] / 100)\n",
    "for bar, color in zip(bars, colors):\n",
    "    bar.set_color(color)\n",
    "\n",
    "ax.set_xlabel('Relative Memory Usage (%)', fontsize=12)\n",
    "ax.set_title('Memory Requirements by Method', fontsize=14)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Use case recommendations\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "recommendations = [\n",
    "    \"🎯 **Method Selection Guide**\\n\",\n",
    "    \"**Full Fine-tuning**: Best quality, use when resources available\",\n",
    "    \"**LoRA**: Best balance - high quality, low memory\",\n",
    "    \"**QLoRA**: For very large models (70B+)\",\n",
    "    \"**Adapters**: Good for multi-task scenarios\",\n",
    "    \"**Prompt/Prefix**: Minimal resources, good for few-shot\",\n",
    "    \"**BitFit**: Quick experiments, baseline\",\n",
    "    \"\\n📊 **Key Insights**:\",\n",
    "    \"• LoRA achieves 98% quality with 0.5% parameters\",\n",
    "    \"• QLoRA enables 4-bit training of huge models\",\n",
    "    \"• Prompt methods work well for classification\",\n",
    "    \"• Adapters excel at multi-task learning\"\n",
    "]\n",
    "\n",
    "text = \"\\n\".join(recommendations)\n",
    "ax.text(0.05, 0.95, text, transform=ax.transAxes, \n",
    "        fontsize=11, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding LoRA (Low-Rank Adaptation)\n",
    "\n",
    "LoRA is one of the most popular parameter-efficient fine-tuning methods. Let's explore how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Simplified LoRA layer for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, rank: int = 16, alpha: int = 16):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.scaling = alpha / rank\n",
    "        \n",
    "        # Low-rank matrices\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "\n",
    "# Demonstrate LoRA concept\n",
    "def visualize_lora_concept():\n",
    "    \"\"\"Visualize how LoRA decomposes weight updates.\"\"\"\n",
    "    \n",
    "    # Original weight dimensions\n",
    "    d_in, d_out = 768, 768\n",
    "    rank = 16\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "    \n",
    "    # 1. Original weight matrix\n",
    "    ax = axes[0]\n",
    "    W = torch.randn(d_out, d_in) * 0.02\n",
    "    im = ax.imshow(W[:100, :100], cmap='coolwarm', aspect='auto')\n",
    "    ax.set_title(f'Original Weight W\\n{d_out}×{d_in} = {d_out*d_in:,} params', fontsize=12)\n",
    "    ax.set_xlabel('Input dimension')\n",
    "    ax.set_ylabel('Output dimension')\n",
    "    \n",
    "    # 2. Low-rank decomposition\n",
    "    ax = axes[1]\n",
    "    ax.text(0.5, 0.8, 'LoRA Decomposition:', transform=ax.transAxes, \n",
    "            ha='center', fontsize=14, weight='bold')\n",
    "    ax.text(0.5, 0.6, f'ΔW = B × A', transform=ax.transAxes, \n",
    "            ha='center', fontsize=12)\n",
    "    ax.text(0.5, 0.4, f'W_new = W_pretrained + α/r × B × A', transform=ax.transAxes, \n",
    "            ha='center', fontsize=11)\n",
    "    ax.text(0.5, 0.2, f'Parameters: {d_in*rank + rank*d_out:,}\\n'\n",
    "            f'Reduction: {(1 - (d_in*rank + rank*d_out)/(d_in*d_out))*100:.1f}%', \n",
    "            transform=ax.transAxes, ha='center', fontsize=11)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 3. Matrix A (down-projection)\n",
    "    ax = axes[2]\n",
    "    A = torch.randn(d_in, rank) * 0.01\n",
    "    im = ax.imshow(A[:100, :].T, cmap='viridis', aspect='auto')\n",
    "    ax.set_title(f'Matrix A\\n{d_in}×{rank}', fontsize=12)\n",
    "    ax.set_xlabel('Input dimension')\n",
    "    ax.set_ylabel('Rank')\n",
    "    \n",
    "    # 4. Matrix B (up-projection)\n",
    "    ax = axes[3]\n",
    "    B = torch.zeros(rank, d_out)\n",
    "    im = ax.imshow(B[:, :100], cmap='plasma', aspect='auto')\n",
    "    ax.set_title(f'Matrix B\\n{rank}×{d_out}', fontsize=12)\n",
    "    ax.set_xlabel('Output dimension')\n",
    "    ax.set_ylabel('Rank')\n",
    "    \n",
    "    plt.suptitle('LoRA: Low-Rank Adaptation of Large Language Models', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_lora_concept()\n",
    "\n",
    "# Demonstrate LoRA in action\n",
    "print(\"\\n--- LoRA Implementation Demo ---\")\n",
    "\n",
    "# Create base layer and LoRA\n",
    "base_layer = nn.Linear(768, 768, bias=False)\n",
    "lora = LoRALayer(768, 768, rank=16, alpha=32)\n",
    "\n",
    "# Freeze base layer\n",
    "for param in base_layer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Forward pass\n",
    "x = torch.randn(2, 10, 768)\n",
    "with torch.no_grad():\n",
    "    base_output = base_layer(x)\n",
    "lora_output = lora(x)\n",
    "combined_output = base_output + lora_output\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Base output shape: {base_output.shape}\")\n",
    "print(f\"LoRA output shape: {lora_output.shape}\")\n",
    "print(f\"Combined output shape: {combined_output.shape}\")\n",
    "\n",
    "# Parameter counting\n",
    "base_params = sum(p.numel() for p in base_layer.parameters())\n",
    "lora_params = sum(p.numel() for p in lora.parameters())\n",
    "print(f\"\\nBase layer parameters: {base_params:,}\")\n",
    "print(f\"LoRA parameters: {lora_params:,}\")\n",
    "print(f\"Reduction: {(1 - lora_params/base_params)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing LoRA Ranks\n",
    "\n",
    "The rank `r` is a crucial hyperparameter in LoRA. Let's explore its impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lora_ranks(input_dim=768, output_dim=3072):\n",
    "    \"\"\"Analyze the effect of different LoRA ranks.\"\"\"\n",
    "    \n",
    "    ranks = [1, 2, 4, 8, 16, 32, 64, 128, 256]\n",
    "    \n",
    "    results = []\n",
    "    for rank in ranks:\n",
    "        # Calculate parameters\n",
    "        lora_params = input_dim * rank + rank * output_dim\n",
    "        full_params = input_dim * output_dim\n",
    "        \n",
    "        # Simulate quality (hypothetical)\n",
    "        quality = 100 * (1 - np.exp(-rank / 16))  # Saturating curve\n",
    "        \n",
    "        results.append({\n",
    "            'Rank': rank,\n",
    "            'Parameters': lora_params,\n",
    "            'Percentage': lora_params / full_params * 100,\n",
    "            'Quality': quality\n",
    "        })\n",
    "    \n",
    "    df_ranks = pd.DataFrame(results)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Parameters vs rank\n",
    "    ax1.plot(df_ranks['Rank'], df_ranks['Parameters'] / 1000, 'b-o', label='LoRA params')\n",
    "    ax1.axhline(y=full_params / 1000, color='red', linestyle='--', label='Full params')\n",
    "    ax1.set_xlabel('LoRA Rank (r)', fontsize=12)\n",
    "    ax1.set_ylabel('Parameters (thousands)', fontsize=12)\n",
    "    ax1.set_title('Parameter Count vs LoRA Rank', fontsize=14)\n",
    "    ax1.set_xscale('log', base=2)\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Quality vs parameters trade-off\n",
    "    ax2.plot(df_ranks['Percentage'], df_ranks['Quality'], 'g-o')\n",
    "    \n",
    "    # Add rank labels\n",
    "    for _, row in df_ranks.iterrows():\n",
    "        if row['Rank'] in [1, 4, 16, 64, 256]:\n",
    "            ax2.annotate(f\"r={row['Rank']}\", \n",
    "                        (row['Percentage'], row['Quality']),\n",
    "                        xytext=(5, -5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax2.set_xlabel('Parameters (% of full model)', fontsize=12)\n",
    "    ax2.set_ylabel('Relative Quality (%)', fontsize=12)\n",
    "    ax2.set_title('Quality vs Parameter Efficiency', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add sweet spot\n",
    "    sweet_spot = df_ranks[df_ranks['Rank'] == 16].iloc[0]\n",
    "    ax2.scatter(sweet_spot['Percentage'], sweet_spot['Quality'], \n",
    "               s=200, color='red', marker='*', zorder=5)\n",
    "    ax2.annotate('Sweet spot', \n",
    "                (sweet_spot['Percentage'], sweet_spot['Quality']),\n",
    "                xytext=(10, 10), textcoords='offset points', \n",
    "                fontsize=10, color='red', weight='bold',\n",
    "                arrowprops=dict(arrowstyle='->', color='red'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommendations\n",
    "    print(\"\\n📊 LoRA Rank Recommendations:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"{'Rank':<10} {'Use Case':<30} {'Trade-off':<20}\")\n",
    "    print(\"-\" * 50)\n",
    "    recommendations = [\n",
    "        (1, \"Extreme efficiency\", \"Limited capacity\"),\n",
    "        (4, \"Very efficient\", \"Good for simple tasks\"),\n",
    "        (8, \"Efficient\", \"Balanced\"),\n",
    "        (16, \"Recommended default\", \"Best trade-off\"),\n",
    "        (32, \"Higher capacity\", \"Diminishing returns\"),\n",
    "        (64, \"Complex tasks\", \"Less efficient\"),\n",
    "        (128, \"Maximum flexibility\", \"High memory use\")\n",
    "    ]\n",
    "    \n",
    "    for rank, use_case, tradeoff in recommendations:\n",
    "        params_pct = df_ranks[df_ranks['Rank'] == rank]['Percentage'].iloc[0]\n",
    "        print(f\"{rank:<10} {use_case:<30} {tradeoff:<20} ({params_pct:.1f}% params)\")\n",
    "\n",
    "analyze_lora_ranks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. QLoRA: Quantized LoRA\n",
    "\n",
    "QLoRA enables fine-tuning of very large models by combining quantization with LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_qlora_concept():\n",
    "    \"\"\"Visualize QLoRA's memory savings through quantization.\"\"\"\n",
    "    \n",
    "    # Model sizes and memory requirements\n",
    "    model_sizes = ['7B', '13B', '30B', '65B', '70B']\n",
    "    params = np.array([7, 13, 30, 65, 70]) * 1e9\n",
    "    \n",
    "    # Memory calculations (GB)\n",
    "    fp32_memory = params * 4 / 1e9  # 4 bytes per param\n",
    "    fp16_memory = params * 2 / 1e9  # 2 bytes per param\n",
    "    int8_memory = params * 1 / 1e9  # 1 byte per param\n",
    "    int4_memory = params * 0.5 / 1e9  # 0.5 bytes per param\n",
    "    \n",
    "    # LoRA memory (assume rank 64, ~1% extra params)\n",
    "    lora_memory = params * 0.01 * 2 / 1e9  # FP16 for LoRA\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Memory comparison\n",
    "    x = np.arange(len(model_sizes))\n",
    "    width = 0.15\n",
    "    \n",
    "    bars1 = ax1.bar(x - 1.5*width, fp32_memory, width, label='FP32', color='darkred')\n",
    "    bars2 = ax1.bar(x - 0.5*width, fp16_memory, width, label='FP16', color='orange')\n",
    "    bars3 = ax1.bar(x + 0.5*width, int8_memory, width, label='INT8', color='green')\n",
    "    bars4 = ax1.bar(x + 1.5*width, int4_memory + lora_memory, width, \n",
    "                   label='INT4 + LoRA (QLoRA)', color='darkgreen')\n",
    "    \n",
    "    ax1.set_xlabel('Model Size', fontsize=12)\n",
    "    ax1.set_ylabel('Memory Required (GB)', fontsize=12)\n",
    "    ax1.set_title('Memory Requirements by Quantization Method', fontsize=14)\n",
    "    ax1.set_xticks(x)\n",
    "    ax1.set_xticklabels(model_sizes)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add GPU memory lines\n",
    "    gpu_memories = {'A100 80GB': 80, 'A100 40GB': 40, 'V100 32GB': 32, 'T4 16GB': 16}\n",
    "    for gpu, memory in gpu_memories.items():\n",
    "        ax1.axhline(y=memory, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax1.text(len(model_sizes)-0.5, memory+2, gpu, fontsize=9, color='gray')\n",
    "    \n",
    "    # QLoRA architecture\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('QLoRA Architecture', fontsize=14)\n",
    "    \n",
    "    # Draw architecture\n",
    "    layers = [\n",
    "        {'name': 'Input', 'y': 0.9, 'color': 'lightblue'},\n",
    "        {'name': 'Frozen 4-bit Model', 'y': 0.7, 'color': 'lightcoral'},\n",
    "        {'name': 'LoRA Adapters (FP16)', 'y': 0.5, 'color': 'lightgreen'},\n",
    "        {'name': 'Output', 'y': 0.3, 'color': 'lightblue'}\n",
    "    ]\n",
    "    \n",
    "    for i, layer in enumerate(layers):\n",
    "        rect = plt.Rectangle((0.2, layer['y']-0.05), 0.6, 0.1, \n",
    "                           facecolor=layer['color'], edgecolor='black')\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(0.5, layer['y'], layer['name'], ha='center', va='center', \n",
    "                fontsize=11, weight='bold')\n",
    "        \n",
    "        if i < len(layers) - 1:\n",
    "            ax2.arrow(0.5, layer['y']-0.05, 0, -0.07, \n",
    "                     head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "    \n",
    "    # Add annotations\n",
    "    ax2.text(0.85, 0.7, 'Quantized\\n(saves memory)', fontsize=9, ha='left', va='center')\n",
    "    ax2.text(0.85, 0.5, 'Trainable\\n(high precision)', fontsize=9, ha='left', va='center')\n",
    "    \n",
    "    # Add key points\n",
    "    key_points = [\n",
    "        \"✓ 4-bit base model (NF4 quantization)\",\n",
    "        \"✓ 16-bit LoRA adapters\",\n",
    "        \"✓ Gradient checkpointing\",\n",
    "        \"✓ Paged optimizers\",\n",
    "        \"✓ Can fine-tune 65B on single GPU!\"\n",
    "    ]\n",
    "    \n",
    "    for i, point in enumerate(key_points):\n",
    "        ax2.text(0.05, 0.15 - i*0.03, point, fontsize=10, transform=ax2.transAxes)\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_qlora_concept()\n",
    "\n",
    "# Simulate QLoRA training memory usage\n",
    "print(\"\\n--- QLoRA Memory Usage Simulation ---\")\n",
    "\n",
    "def calculate_qlora_memory(model_size_b, rank=64, batch_size=4, seq_len=2048):\n",
    "    \"\"\"Calculate memory requirements for QLoRA training.\"\"\"\n",
    "    \n",
    "    # Base model (4-bit)\n",
    "    base_memory = model_size_b * 1e9 * 0.5 / 1e9  # GB\n",
    "    \n",
    "    # LoRA parameters (FP16)\n",
    "    num_lora_params = model_size_b * 1e9 * 0.01  # ~1% of model\n",
    "    lora_memory = num_lora_params * 2 / 1e9  # GB\n",
    "    \n",
    "    # Optimizer states (Adam, FP32)\n",
    "    optimizer_memory = num_lora_params * 8 / 1e9  # 2 states * 4 bytes\n",
    "    \n",
    "    # Activations (rough estimate)\n",
    "    hidden_size = int(np.sqrt(model_size_b * 1e9 / 100))  # Rough estimate\n",
    "    activation_memory = batch_size * seq_len * hidden_size * 4 * 32 / 1e9  # GB\n",
    "    \n",
    "    # Gradients\n",
    "    gradient_memory = lora_memory  # Same as parameters\n",
    "    \n",
    "    total = base_memory + lora_memory + optimizer_memory + activation_memory + gradient_memory\n",
    "    \n",
    "    return {\n",
    "        'Base Model (4-bit)': base_memory,\n",
    "        'LoRA Parameters': lora_memory,\n",
    "        'Optimizer States': optimizer_memory,\n",
    "        'Activations': activation_memory,\n",
    "        'Gradients': gradient_memory,\n",
    "        'Total': total\n",
    "    }\n",
    "\n",
    "# Calculate for different model sizes\n",
    "for model_size in [7, 13, 30, 65]:\n",
    "    memory = calculate_qlora_memory(model_size)\n",
    "    print(f\"\\n{model_size}B Model:\")\n",
    "    for component, mem in memory.items():\n",
    "        print(f\"  {component:<20}: {mem:>6.1f} GB\")\n",
    "    print(f\"  {'='*30}\")\n",
    "    print(f\"  Can fit on: \", end=\"\")\n",
    "    if memory['Total'] < 16:\n",
    "        print(\"T4 (16GB) ✓\")\n",
    "    elif memory['Total'] < 24:\n",
    "        print(\"RTX 3090/4090 (24GB) ✓\")\n",
    "    elif memory['Total'] < 40:\n",
    "        print(\"A100 40GB ✓\")\n",
    "    elif memory['Total'] < 80:\n",
    "        print(\"A100 80GB ✓\")\n",
    "    else:\n",
    "        print(\"Requires multiple GPUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Adapter Modules\n",
    "\n",
    "Adapters are another popular parameter-efficient method, especially for multi-task learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdapterModule(nn.Module):\n",
    "    \"\"\"Simple adapter module implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, adapter_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.down_project = nn.Linear(hidden_size, adapter_size)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.up_project = nn.Linear(adapter_size, hidden_size)\n",
    "        \n",
    "        # Initialize near identity\n",
    "        nn.init.normal_(self.down_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.down_project.bias)\n",
    "        nn.init.normal_(self.up_project.weight, std=1e-3)\n",
    "        nn.init.zeros_(self.up_project.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Residual connection with bottleneck\n",
    "        return x + self.up_project(self.activation(self.down_project(x)))\n",
    "\n",
    "# Visualize adapter architecture\n",
    "def visualize_adapter_architecture():\n",
    "    \"\"\"Show how adapters integrate into transformer layers.\"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    \n",
    "    # Adapter module structure\n",
    "    ax1.axis('off')\n",
    "    ax1.set_title('Adapter Module Structure', fontsize=14)\n",
    "    \n",
    "    # Draw adapter architecture\n",
    "    components = [\n",
    "        {'name': 'Input (d)', 'pos': (0.5, 0.9), 'size': (0.3, 0.08)},\n",
    "        {'name': 'Down-project\\n(d → r)', 'pos': (0.3, 0.7), 'size': (0.2, 0.08)},\n",
    "        {'name': 'ReLU', 'pos': (0.3, 0.5), 'size': (0.2, 0.08)},\n",
    "        {'name': 'Up-project\\n(r → d)', 'pos': (0.3, 0.3), 'size': (0.2, 0.08)},\n",
    "        {'name': '+', 'pos': (0.5, 0.15), 'size': (0.08, 0.08)},\n",
    "        {'name': 'Output (d)', 'pos': (0.5, 0.05), 'size': (0.3, 0.08)}\n",
    "    ]\n",
    "    \n",
    "    for comp in components:\n",
    "        if comp['name'] == '+':\n",
    "            circle = plt.Circle(comp['pos'], 0.04, facecolor='yellow', edgecolor='black')\n",
    "            ax1.add_patch(circle)\n",
    "            ax1.text(comp['pos'][0], comp['pos'][1], comp['name'], \n",
    "                    ha='center', va='center', fontsize=16, weight='bold')\n",
    "        else:\n",
    "            rect = plt.Rectangle((comp['pos'][0] - comp['size'][0]/2, \n",
    "                                comp['pos'][1] - comp['size'][1]/2),\n",
    "                               comp['size'][0], comp['size'][1],\n",
    "                               facecolor='lightblue', edgecolor='black')\n",
    "            ax1.add_patch(rect)\n",
    "            ax1.text(comp['pos'][0], comp['pos'][1], comp['name'], \n",
    "                    ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Draw connections\n",
    "    # Main path\n",
    "    ax1.arrow(0.4, 0.85, 0, -0.1, head_width=0.02, head_length=0.02, fc='blue', ec='blue')\n",
    "    ax1.arrow(0.3, 0.65, 0, -0.1, head_width=0.02, head_length=0.02, fc='blue', ec='blue')\n",
    "    ax1.arrow(0.3, 0.45, 0, -0.1, head_width=0.02, head_length=0.02, fc='blue', ec='blue')\n",
    "    ax1.arrow(0.3, 0.25, 0.15, -0.05, head_width=0.02, head_length=0.02, fc='blue', ec='blue')\n",
    "    \n",
    "    # Skip connection\n",
    "    ax1.plot([0.6, 0.6, 0.54], [0.85, 0.15, 0.15], 'r--', linewidth=2)\n",
    "    ax1.arrow(0.54, 0.15, -0.02, 0, head_width=0.02, head_length=0.01, fc='red', ec='red')\n",
    "    ax1.text(0.65, 0.5, 'Skip\\nconnection', fontsize=9, color='red')\n",
    "    \n",
    "    # Output\n",
    "    ax1.arrow(0.5, 0.11, 0, -0.02, head_width=0.02, head_length=0.02, fc='blue', ec='blue')\n",
    "    \n",
    "    ax1.set_xlim(0, 1)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Transformer with adapters\n",
    "    ax2.axis('off')\n",
    "    ax2.set_title('Adapters in Transformer Layer', fontsize=14)\n",
    "    \n",
    "    # Draw transformer layer with adapters\n",
    "    layer_components = [\n",
    "        {'name': 'Multi-Head\\nAttention', 'y': 0.85, 'color': 'lightcoral', 'frozen': True},\n",
    "        {'name': 'Adapter', 'y': 0.72, 'color': 'lightgreen', 'frozen': False},\n",
    "        {'name': 'Layer Norm', 'y': 0.6, 'color': 'lightyellow', 'frozen': True},\n",
    "        {'name': 'Feed Forward', 'y': 0.45, 'color': 'lightcoral', 'frozen': True},\n",
    "        {'name': 'Adapter', 'y': 0.32, 'color': 'lightgreen', 'frozen': False},\n",
    "        {'name': 'Layer Norm', 'y': 0.2, 'color': 'lightyellow', 'frozen': True}\n",
    "    ]\n",
    "    \n",
    "    for i, comp in enumerate(layer_components):\n",
    "        width = 0.5 if comp['name'] != 'Adapter' else 0.3\n",
    "        x_pos = 0.25 if comp['name'] != 'Adapter' else 0.35\n",
    "        \n",
    "        rect = plt.Rectangle((x_pos, comp['y']-0.04), width, 0.08,\n",
    "                           facecolor=comp['color'], \n",
    "                           edgecolor='black',\n",
    "                           linewidth=2 if not comp['frozen'] else 1,\n",
    "                           linestyle='-' if not comp['frozen'] else '--')\n",
    "        ax2.add_patch(rect)\n",
    "        \n",
    "        text = comp['name']\n",
    "        if comp['frozen']:\n",
    "            text += ' ❄️'\n",
    "        else:\n",
    "            text += ' 🔥'\n",
    "            \n",
    "        ax2.text(0.5, comp['y'], text, ha='center', va='center', fontsize=10,\n",
    "                weight='bold' if not comp['frozen'] else 'normal')\n",
    "        \n",
    "        if i < len(layer_components) - 1:\n",
    "            ax2.arrow(0.5, comp['y']-0.04, 0, -0.04, \n",
    "                     head_width=0.02, head_length=0.01, fc='black', ec='black')\n",
    "    \n",
    "    # Add legend\n",
    "    ax2.text(0.8, 0.9, '❄️ Frozen', fontsize=10)\n",
    "    ax2.text(0.8, 0.85, '🔥 Trainable', fontsize=10)\n",
    "    \n",
    "    # Add parameter counts\n",
    "    ax2.text(0.5, 0.05, 'Only adapter parameters are updated during training\\n'\n",
    "                        'Typically 1-5% of model parameters', \n",
    "            ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_adapter_architecture()\n",
    "\n",
    "# Demonstrate adapter effectiveness\n",
    "print(\"\\n--- Adapter Module Demo ---\")\n",
    "\n",
    "hidden_size = 768\n",
    "adapter_sizes = [8, 16, 32, 64, 128, 256]\n",
    "\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "print(f\"\\n{'Adapter Size':<15} {'Parameters':<15} {'Compression':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for adapter_size in adapter_sizes:\n",
    "    adapter = AdapterModule(hidden_size, adapter_size)\n",
    "    params = sum(p.numel() for p in adapter.parameters())\n",
    "    compression = hidden_size * hidden_size / params\n",
    "    \n",
    "    print(f\"{adapter_size:<15} {params:<15,} {compression:<15.1f}x\")\n",
    "\n",
    "# Test adapter forward pass\n",
    "adapter = AdapterModule(hidden_size, 64)\n",
    "x = torch.randn(2, 10, hidden_size)\n",
    "output = adapter(x)\n",
    "\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Residual preserved: {torch.allclose(output, x, atol=1e-1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prompt Tuning Methods\n",
    "\n",
    "Prompt tuning methods add learnable parameters to the input while keeping the model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_prompt_methods():\n",
    "    \"\"\"Compare different prompt-based fine-tuning methods.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Discrete Prompting\n",
    "    ax = axes[0, 0]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Discrete Prompting (Zero/Few-shot)', fontsize=14)\n",
    "    \n",
    "    # Example prompt\n",
    "    prompt_parts = [\n",
    "        ('Task: Sentiment Analysis\\n', 'orange'),\n",
    "        ('Example: \"Great product!\" → Positive\\n', 'lightblue'),\n",
    "        ('Example: \"Terrible service\" → Negative\\n\\n', 'lightblue'),\n",
    "        ('Input: \"Amazing experience\"\\n', 'lightgreen'),\n",
    "        ('Output: ', 'gray')\n",
    "    ]\n",
    "    \n",
    "    y_pos = 0.8\n",
    "    for text, color in prompt_parts:\n",
    "        lines = text.count('\\n')\n",
    "        rect = plt.Rectangle((0.1, y_pos - lines*0.05), 0.8, lines*0.05 + 0.03,\n",
    "                           facecolor=color, alpha=0.3, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.5, y_pos - lines*0.025 + 0.015, text.strip(), \n",
    "               ha='center', va='center', fontsize=10)\n",
    "        y_pos -= lines*0.05 + 0.04\n",
    "    \n",
    "    ax.text(0.5, 0.1, '❌ No gradient updates\\n✅ No training required', \n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. Soft Prompt Tuning\n",
    "    ax = axes[0, 1]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Soft Prompt Tuning', fontsize=14)\n",
    "    \n",
    "    # Visualize soft prompts\n",
    "    tokens = ['[P1]', '[P2]', '[P3]', '[P4]', '[P5]', 'The', 'movie', 'was', '...']\n",
    "    colors = ['red', 'red', 'red', 'red', 'red', 'lightblue', 'lightblue', 'lightblue', 'lightblue']\n",
    "    \n",
    "    for i, (token, color) in enumerate(zip(tokens, colors)):\n",
    "        rect = plt.Rectangle((0.05 + i*0.1, 0.6), 0.08, 0.1,\n",
    "                           facecolor=color, alpha=0.5 if color == 'red' else 0.3,\n",
    "                           edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.09 + i*0.1, 0.65, token, ha='center', va='center', \n",
    "               fontsize=9, weight='bold' if color == 'red' else 'normal')\n",
    "    \n",
    "    # Embeddings\n",
    "    ax.text(0.5, 0.5, '↓ Embeddings ↓', ha='center', fontsize=11)\n",
    "    \n",
    "    # Embedding matrix\n",
    "    for i in range(9):\n",
    "        for j in range(4):\n",
    "            val = np.random.randn() * 0.1\n",
    "            color = 'red' if i < 5 else 'blue'\n",
    "            alpha = 0.7 if i < 5 else 0.3\n",
    "            rect = plt.Rectangle((0.05 + i*0.1, 0.3 - j*0.04), 0.08, 0.03,\n",
    "                               facecolor=color, alpha=alpha)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    ax.text(0.5, 0.1, '🔥 Learnable soft prompts (continuous)\\n❄️ Frozen input embeddings', \n",
    "           ha='center', fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 0.8)\n",
    "    \n",
    "    # 3. Prefix Tuning\n",
    "    ax = axes[1, 0]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Prefix Tuning', fontsize=14)\n",
    "    \n",
    "    # Draw transformer layers\n",
    "    n_layers = 4\n",
    "    for layer in range(n_layers):\n",
    "        y = 0.8 - layer * 0.15\n",
    "        \n",
    "        # Prefix keys/values\n",
    "        for i in range(3):\n",
    "            rect = plt.Rectangle((0.1 + i*0.08, y), 0.06, 0.08,\n",
    "                               facecolor='red', alpha=0.5, edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(0.19, y+0.04, 'Prefix K,V', ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Regular keys/values\n",
    "        for i in range(5):\n",
    "            rect = plt.Rectangle((0.4 + i*0.08, y), 0.06, 0.08,\n",
    "                               facecolor='lightblue', alpha=0.3, edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "        \n",
    "        ax.text(0.6, y+0.04, 'Input K,V', ha='center', va='center', fontsize=8)\n",
    "        ax.text(0.05, y+0.04, f'L{layer+1}', ha='center', va='center', fontsize=10, weight='bold')\n",
    "    \n",
    "    ax.text(0.5, 0.15, 'Learnable prefix prepended to keys/values in each layer\\n'\n",
    "                       'Affects attention computation throughout the model',\n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 4. Method comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    methods = ['Manual\\nPrompt', 'Soft\\nPrompt', 'Prefix\\nTuning', 'P-Tuning\\nv2']\n",
    "    params = [0, 0.01, 0.1, 0.1]\n",
    "    flexibility = [20, 70, 85, 90]\n",
    "    ease = [100, 90, 70, 60]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    \n",
    "    bars1 = ax.bar(x - width, params, width, label='Parameters (%)', color='lightcoral')\n",
    "    bars2 = ax.bar(x, flexibility, width, label='Flexibility', color='lightgreen')\n",
    "    bars3 = ax.bar(x + width, ease, width, label='Ease of Use', color='lightblue')\n",
    "    \n",
    "    ax.set_xlabel('Method', fontsize=12)\n",
    "    ax.set_ylabel('Score', fontsize=12)\n",
    "    ax.set_title('Prompt Method Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_prompt_methods()\n",
    "\n",
    "# Demonstrate soft prompt implementation\n",
    "print(\"\\n--- Soft Prompt Implementation ---\")\n",
    "\n",
    "class SoftPromptModel(nn.Module):\n",
    "    def __init__(self, n_prompts=10, embed_dim=768, vocab_size=50000):\n",
    "        super().__init__()\n",
    "        # Frozen embeddings (would be from pretrained model)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embeddings.requires_grad = False\n",
    "        \n",
    "        # Learnable soft prompts\n",
    "        self.soft_prompts = nn.Parameter(torch.randn(n_prompts, embed_dim) * 0.01)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        batch_size = input_ids.shape[0]\n",
    "        \n",
    "        # Get input embeddings\n",
    "        input_embeds = self.embeddings(input_ids)\n",
    "        \n",
    "        # Expand soft prompts for batch\n",
    "        prompt_embeds = self.soft_prompts.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate\n",
    "        combined_embeds = torch.cat([prompt_embeds, input_embeds], dim=1)\n",
    "        \n",
    "        return combined_embeds\n",
    "\n",
    "# Create and test model\n",
    "model = SoftPromptModel(n_prompts=5, embed_dim=768)\n",
    "input_ids = torch.randint(0, 50000, (2, 10))\n",
    "output = model(input_ids)\n",
    "\n",
    "print(f\"Model created with:\")\n",
    "print(f\"  Soft prompts: {model.soft_prompts.shape}\")\n",
    "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"\\nInput shape: {input_ids.shape}\")\n",
    "print(f\"Output shape: {output.shape} (includes {model.soft_prompts.shape[0]} prompt tokens)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Strategies and Best Practices\n",
    "\n",
    "Let's explore optimal training strategies for different fine-tuning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate schedules for fine-tuning\n",
    "def plot_finetuning_schedules():\n",
    "    \"\"\"Visualize recommended learning rate schedules.\"\"\"\n",
    "    \n",
    "    steps = np.arange(0, 10000)\n",
    "    \n",
    "    # Different schedules\n",
    "    def linear_warmup_cosine(step, warmup=500, total=10000, max_lr=5e-5):\n",
    "        if step < warmup:\n",
    "            return max_lr * step / warmup\n",
    "        progress = (step - warmup) / (total - warmup)\n",
    "        return max_lr * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    def linear_warmup_linear(step, warmup=500, total=10000, max_lr=5e-5):\n",
    "        if step < warmup:\n",
    "            return max_lr * step / warmup\n",
    "        return max_lr * (1 - (step - warmup) / (total - warmup))\n",
    "    \n",
    "    def constant_warmup(step, warmup=500, max_lr=5e-5):\n",
    "        if step < warmup:\n",
    "            return max_lr * step / warmup\n",
    "        return max_lr\n",
    "    \n",
    "    # Calculate schedules\n",
    "    schedules = {\n",
    "        'Cosine (recommended)': [linear_warmup_cosine(s) for s in steps],\n",
    "        'Linear': [linear_warmup_linear(s) for s in steps],\n",
    "        'Constant': [constant_warmup(s) for s in steps]\n",
    "    }\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot schedules\n",
    "    for name, schedule in schedules.items():\n",
    "        ax1.plot(steps, schedule, label=name, linewidth=2)\n",
    "    \n",
    "    ax1.axvline(x=500, color='red', linestyle='--', alpha=0.5)\n",
    "    ax1.text(500, 4e-5, 'Warmup', rotation=90, va='bottom', ha='right', color='red')\n",
    "    ax1.set_xlabel('Training Step', fontsize=12)\n",
    "    ax1.set_ylabel('Learning Rate', fontsize=12)\n",
    "    ax1.set_title('Learning Rate Schedules for Fine-tuning', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Method-specific recommendations\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    recommendations = pd.DataFrame({\n",
    "        'Method': ['Full Fine-tuning', 'LoRA', 'QLoRA', 'Adapters', 'Prompt Tuning'],\n",
    "        'Learning Rate': ['2e-5', '1e-4', '2e-4', '1e-3', '1e-2'],\n",
    "        'Warmup': ['6%', '3%', '3%', '1%', '0%'],\n",
    "        'Batch Size': ['16-32', '128-256', '4-16', '64-128', '256-512'],\n",
    "        'Epochs': ['3-5', '10-20', '3-5', '10-20', '20-50']\n",
    "    })\n",
    "    \n",
    "    # Create table\n",
    "    table = ax2.table(cellText=recommendations.values,\n",
    "                     colLabels=recommendations.columns,\n",
    "                     cellLoc='center',\n",
    "                     loc='center')\n",
    "    \n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 2)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(recommendations.columns)):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    ax2.set_title('Hyperparameter Recommendations by Method', fontsize=14, pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_finetuning_schedules()\n",
    "\n",
    "# Training tips\n",
    "print(\"\\n🎯 Fine-tuning Best Practices:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "tips = [\n",
    "    (\"1. Start with LoRA\", \"Good balance of efficiency and performance\"),\n",
    "    (\"2. Use gradient checkpointing\", \"Trade compute for memory\"),\n",
    "    (\"3. Monitor validation loss\", \"Stop early if overfitting\"),\n",
    "    (\"4. Layer-wise learning rates\", \"Lower LR for earlier layers\"),\n",
    "    (\"5. Mixed precision training\", \"2x speedup with minimal impact\"),\n",
    "    (\"6. Careful data preprocessing\", \"Match pretrained model's format\"),\n",
    "    (\"7. Warmup is crucial\", \"Prevents catastrophic forgetting\"),\n",
    "    (\"8. Save checkpoints frequently\", \"Resume from best checkpoint\")\n",
    "]\n",
    "\n",
    "for tip, explanation in tips:\n",
    "    print(f\"\\n{tip}\")\n",
    "    print(f\"   → {explanation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Multi-task Fine-tuning\n",
    "\n",
    "Adapters and LoRA are particularly well-suited for multi-task scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_multitask_finetuning():\n",
    "    \"\"\"Show different approaches to multi-task fine-tuning.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Separate models\n",
    "    ax = axes[0, 0]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Approach 1: Separate Models', fontsize=14)\n",
    "    \n",
    "    tasks = ['Sentiment', 'NER', 'QA']\n",
    "    colors = ['lightcoral', 'lightgreen', 'lightblue']\n",
    "    \n",
    "    for i, (task, color) in enumerate(zip(tasks, colors)):\n",
    "        # Base model\n",
    "        rect = plt.Rectangle((0.1 + i*0.3, 0.3), 0.2, 0.4,\n",
    "                           facecolor=color, edgecolor='black', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.2 + i*0.3, 0.5, f'Full Model\\n{task}', \n",
    "               ha='center', va='center', fontsize=10, weight='bold')\n",
    "        \n",
    "        # Task head\n",
    "        rect = plt.Rectangle((0.1 + i*0.3, 0.75), 0.2, 0.1,\n",
    "                           facecolor='yellow', edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.2 + i*0.3, 0.8, 'Task Head', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax.text(0.5, 0.15, '❌ 3x memory\\n❌ No knowledge sharing\\n✅ Best per-task performance',\n",
    "           ha='center', fontsize=10)\n",
    "    \n",
    "    # 2. Shared model + task heads\n",
    "    ax = axes[0, 1]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Approach 2: Shared Model + Task Heads', fontsize=14)\n",
    "    \n",
    "    # Shared base\n",
    "    rect = plt.Rectangle((0.3, 0.2), 0.4, 0.4,\n",
    "                       facecolor='lightgray', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.5, 0.4, 'Shared\\nBase Model', ha='center', va='center', \n",
    "           fontsize=12, weight='bold')\n",
    "    \n",
    "    # Task heads\n",
    "    for i, (task, color) in enumerate(zip(tasks, colors)):\n",
    "        rect = plt.Rectangle((0.2 + i*0.2, 0.65), 0.15, 0.1,\n",
    "                           facecolor=color, edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(0.275 + i*0.2, 0.7, task, ha='center', va='center', fontsize=8)\n",
    "        \n",
    "        # Connection\n",
    "        ax.plot([0.275 + i*0.2, 0.5], [0.65, 0.6], 'k-', linewidth=1)\n",
    "    \n",
    "    ax.text(0.5, 0.05, '✅ Memory efficient\\n⚠️ Task interference\\n✅ Some knowledge sharing',\n",
    "           ha='center', fontsize=10)\n",
    "    \n",
    "    # 3. Adapter-based\n",
    "    ax = axes[1, 0]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Approach 3: Task-Specific Adapters', fontsize=14)\n",
    "    \n",
    "    # Frozen base\n",
    "    rect = plt.Rectangle((0.3, 0.2), 0.4, 0.4,\n",
    "                       facecolor='lightblue', edgecolor='black', \n",
    "                       linewidth=2, linestyle='--')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(0.5, 0.4, 'Frozen\\nBase Model', ha='center', va='center', \n",
    "           fontsize=12, weight='bold')\n",
    "    ax.text(0.72, 0.4, '❄️', fontsize=20)\n",
    "    \n",
    "    # Adapters\n",
    "    adapter_positions = [(0.35, 0.3), (0.5, 0.35), (0.65, 0.3)]\n",
    "    for i, ((x, y), color) in enumerate(zip(adapter_positions, colors)):\n",
    "        rect = plt.Rectangle((x-0.04, y), 0.08, 0.15,\n",
    "                           facecolor=color, edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x, y+0.18, f'A{i+1}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Task routing\n",
    "    for i, task in enumerate(tasks):\n",
    "        ax.text(0.2 + i*0.2, 0.75, task, ha='center', va='center', \n",
    "               fontsize=10, bbox=dict(boxstyle=\"round\", facecolor=colors[i], alpha=0.5))\n",
    "        ax.arrow(0.2 + i*0.2, 0.72, \n",
    "                adapter_positions[i][0] - (0.2 + i*0.2), \n",
    "                adapter_positions[i][1] + 0.15 - 0.72,\n",
    "                head_width=0.02, head_length=0.02, fc='black', ec='black')\n",
    "    \n",
    "    ax.text(0.5, 0.05, '✅ Very memory efficient\\n✅ No task interference\\n✅ Easy to add tasks',\n",
    "           ha='center', fontsize=10)\n",
    "    \n",
    "    # 4. Performance comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    methods = ['Separate\\nModels', 'Shared\\nBase', 'Adapters', 'LoRA\\nMixture']\n",
    "    memory = [300, 120, 105, 110]\n",
    "    performance = [100, 92, 95, 97]\n",
    "    flexibility = [60, 70, 95, 90]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, memory, width, label='Memory (%)', color='lightcoral')\n",
    "    ax.bar(x, performance, width, label='Performance (%)', color='lightgreen')\n",
    "    ax.bar(x + width, flexibility, width, label='Flexibility (%)', color='lightblue')\n",
    "    \n",
    "    ax.set_ylabel('Relative Score', fontsize=12)\n",
    "    ax.set_title('Multi-task Approaches Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_multitask_finetuning()\n",
    "\n",
    "# Multi-task adapter implementation\n",
    "print(\"\\n--- Multi-task Adapter Implementation ---\")\n",
    "\n",
    "class MultiTaskAdapterModel(nn.Module):\n",
    "    def __init__(self, base_model_dim=768, tasks=['sentiment', 'ner', 'qa'], \n",
    "                 adapter_size=64):\n",
    "        super().__init__()\n",
    "        self.tasks = tasks\n",
    "        \n",
    "        # Task-specific adapters\n",
    "        self.adapters = nn.ModuleDict({\n",
    "            task: AdapterModule(base_model_dim, adapter_size)\n",
    "            for task in tasks\n",
    "        })\n",
    "        \n",
    "        # Task-specific heads\n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            'sentiment': nn.Linear(base_model_dim, 3),  # 3 classes\n",
    "            'ner': nn.Linear(base_model_dim, 9),        # 9 NER tags\n",
    "            'qa': nn.Linear(base_model_dim, 2)          # Start/end positions\n",
    "        })\n",
    "        \n",
    "    def forward(self, x, task):\n",
    "        # Apply task-specific adapter\n",
    "        x = self.adapters[task](x)\n",
    "        \n",
    "        # Apply task-specific head\n",
    "        output = self.task_heads[task](x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model and test\n",
    "model = MultiTaskAdapterModel()\n",
    "x = torch.randn(2, 10, 768)  # [batch, seq_len, hidden_dim]\n",
    "\n",
    "print(\"Multi-task model created:\")\n",
    "print(f\"  Tasks: {model.tasks}\")\n",
    "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"  Parameters per adapter: {sum(p.numel() for p in model.adapters['sentiment'].parameters()):,}\")\n",
    "\n",
    "print(\"\\nTask-specific outputs:\")\n",
    "for task in model.tasks:\n",
    "    output = model(x, task)\n",
    "    print(f\"  {task}: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Recommendations\n",
    "\n",
    "Let's create a decision tree to help choose the right fine-tuning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decision_guide():\n",
    "    \"\"\"Create a visual decision guide for choosing fine-tuning methods.\"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Title\n",
    "    ax.text(0.5, 0.95, 'Fine-tuning Method Decision Guide', \n",
    "           ha='center', fontsize=16, weight='bold')\n",
    "    \n",
    "    # Decision nodes\n",
    "    decisions = [\n",
    "        # Level 1\n",
    "        {'text': 'Model Size?', 'pos': (0.5, 0.85), 'color': 'lightblue'},\n",
    "        \n",
    "        # Level 2\n",
    "        {'text': '<7B params', 'pos': (0.3, 0.7), 'color': 'lightgreen'},\n",
    "        {'text': '>7B params', 'pos': (0.7, 0.7), 'color': 'lightcoral'},\n",
    "        \n",
    "        # Level 3\n",
    "        {'text': 'Memory\\nConstraints?', 'pos': (0.2, 0.55), 'color': 'lightyellow'},\n",
    "        {'text': 'Multi-task?', 'pos': (0.4, 0.55), 'color': 'lightyellow'},\n",
    "        {'text': 'GPU Memory?', 'pos': (0.6, 0.55), 'color': 'lightyellow'},\n",
    "        {'text': 'Quality\\nPriority?', 'pos': (0.8, 0.55), 'color': 'lightyellow'},\n",
    "    ]\n",
    "    \n",
    "    # Draw decision nodes\n",
    "    for decision in decisions:\n",
    "        circle = plt.Circle(decision['pos'], 0.06, \n",
    "                          facecolor=decision['color'], edgecolor='black')\n",
    "        ax.add_patch(circle)\n",
    "        ax.text(decision['pos'][0], decision['pos'][1], decision['text'], \n",
    "               ha='center', va='center', fontsize=9, weight='bold')\n",
    "    \n",
    "    # Recommendations\n",
    "    recommendations = [\n",
    "        {'text': 'BitFit or\\nPrompt Tuning', 'pos': (0.1, 0.35), 'color': 'lightsteelblue'},\n",
    "        {'text': 'Full\\nFine-tuning', 'pos': (0.25, 0.35), 'color': 'lightsteelblue'},\n",
    "        {'text': 'Adapters', 'pos': (0.4, 0.35), 'color': 'lightsteelblue'},\n",
    "        {'text': 'QLoRA', 'pos': (0.55, 0.35), 'color': 'lightsteelblue'},\n",
    "        {'text': 'LoRA', 'pos': (0.7, 0.35), 'color': 'lightsteelblue'},\n",
    "        {'text': 'LoRA or\\nFull FT', 'pos': (0.85, 0.35), 'color': 'lightsteelblue'},\n",
    "    ]\n",
    "    \n",
    "    for rec in recommendations:\n",
    "        rect = plt.Rectangle((rec['pos'][0] - 0.05, rec['pos'][1] - 0.03), \n",
    "                           0.1, 0.06,\n",
    "                           facecolor=rec['color'], edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(rec['pos'][0], rec['pos'][1], rec['text'], \n",
    "               ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Draw connections\n",
    "    connections = [\n",
    "        # From root\n",
    "        ((0.5, 0.85), (0.3, 0.7)),\n",
    "        ((0.5, 0.85), (0.7, 0.7)),\n",
    "        # From <7B\n",
    "        ((0.3, 0.7), (0.2, 0.55)),\n",
    "        ((0.3, 0.7), (0.4, 0.55)),\n",
    "        # From >7B\n",
    "        ((0.7, 0.7), (0.6, 0.55)),\n",
    "        ((0.7, 0.7), (0.8, 0.55)),\n",
    "        # To recommendations\n",
    "        ((0.2, 0.55), (0.1, 0.35)),\n",
    "        ((0.2, 0.55), (0.25, 0.35)),\n",
    "        ((0.4, 0.55), (0.4, 0.35)),\n",
    "        ((0.6, 0.55), (0.55, 0.35)),\n",
    "        ((0.6, 0.55), (0.7, 0.35)),\n",
    "        ((0.8, 0.55), (0.85, 0.35)),\n",
    "    ]\n",
    "    \n",
    "    for start, end in connections:\n",
    "        ax.plot([start[0], end[0]], [start[1], end[1]], 'k-', linewidth=1)\n",
    "    \n",
    "    # Add labels on connections\n",
    "    ax.text(0.15, 0.45, 'Yes', fontsize=8, color='red')\n",
    "    ax.text(0.3, 0.45, 'No', fontsize=8, color='green')\n",
    "    ax.text(0.5, 0.45, '<40GB', fontsize=8, color='red')\n",
    "    ax.text(0.65, 0.45, '>40GB', fontsize=8, color='green')\n",
    "    \n",
    "    # Key insights box\n",
    "    insights = [\n",
    "        \"📌 Key Insights:\",\n",
    "        \"• LoRA: Best default choice for most cases\",\n",
    "        \"• QLoRA: Essential for very large models\",\n",
    "        \"• Adapters: Excellent for multi-task scenarios\",\n",
    "        \"• Full FT: Use when you have resources and need best quality\",\n",
    "        \"• Prompt methods: Great for quick experiments\"\n",
    "    ]\n",
    "    \n",
    "    box_text = \"\\n\".join(insights)\n",
    "    ax.text(0.5, 0.15, box_text, ha='center', va='center',\n",
    "           bbox=dict(boxstyle=\"round,pad=0.3\", facecolor='wheat', alpha=0.5),\n",
    "           fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_decision_guide()\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n🎓 Fine-tuning Methods Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "summary_data = [\n",
    "    [\"Method\", \"When to Use\", \"Pros\", \"Cons\"],\n",
    "    [\"-\" * 15, \"-\" * 25, \"-\" * 20, \"-\" * 20],\n",
    "    [\"Full Fine-tuning\", \"Small models, best quality\", \"Highest quality\", \"Resource intensive\"],\n",
    "    [\"LoRA\", \"General purpose, default\", \"Great balance\", \"Slightly lower quality\"],\n",
    "    [\"QLoRA\", \"Very large models (>30B)\", \"Enables huge models\", \"Slower training\"],\n",
    "    [\"Adapters\", \"Multi-task scenarios\", \"Task isolation\", \"More complex\"],\n",
    "    [\"Prompt Tuning\", \"Quick experiments\", \"Minimal params\", \"Limited flexibility\"],\n",
    "    [\"BitFit\", \"Baseline, quick test\", \"Very fast\", \"Lower quality\"]\n",
    "]\n",
    "\n",
    "for row in summary_data:\n",
    "    print(f\"{row[0]:<20} {row[1]:<30} {row[2]:<25} {row[3]:<20}\")\n",
    "\n",
    "print(\"\\n✅ Ready to fine-tune! Start with LoRA for the best balance of efficiency and quality.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}