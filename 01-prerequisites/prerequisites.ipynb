{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites & Mathematical Foundations - Interactive Notebook\n",
    "\n",
    "Welcome! This notebook provides hands-on practice with the mathematical concepts underlying transformers. Work through each cell to build your intuition.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Helper function for pretty printing\n",
    "def print_matrix(matrix, name=\"Matrix\"):\n",
    "    print(f\"{name} (shape: {matrix.shape}):\")\n",
    "    print(matrix)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra Essentials\n",
    "\n",
    "### 1.1 Vectors and Dot Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two vectors\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(\"v1:\", v1)\n",
    "print(\"v2:\", v2)\n",
    "\n",
    "# Dot product - three ways to compute it\n",
    "dot_method1 = np.dot(v1, v2)\n",
    "dot_method2 = v1 @ v2  # Python 3.5+ matrix multiplication operator\n",
    "dot_method3 = sum(a * b for a, b in zip(v1, v2))\n",
    "\n",
    "print(f\"\\nDot product:\")\n",
    "print(f\"  np.dot: {dot_method1}\")\n",
    "print(f\"  @ operator: {dot_method2}\")\n",
    "print(f\"  Manual: {dot_method3}\")\n",
    "\n",
    "# Visualize what dot product means\n",
    "print(f\"\\nBreaking it down: {v1[0]}×{v2[0]} + {v1[1]}×{v2[1]} + {v1[2]}×{v2[2]} = {dot_method1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Compute these dot products by hand, then verify with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try these exercises:\n",
    "a = np.array([2, 3])\n",
    "b = np.array([4, -1])\n",
    "\n",
    "# YOUR CODE: Compute a · b\n",
    "# Expected: 2*4 + 3*(-1) = 8 - 3 = 5\n",
    "\n",
    "dot_ab = # YOUR CODE HERE\n",
    "print(f\"a · b = {dot_ab}\")\n",
    "\n",
    "# What happens with orthogonal vectors?\n",
    "c = np.array([1, 0])\n",
    "d = np.array([0, 1])\n",
    "\n",
    "dot_cd = # YOUR CODE HERE\n",
    "print(f\"\\nc · d = {dot_cd} (orthogonal vectors have dot product = 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Understanding Similarity Through Dot Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is crucial for understanding attention!\n",
    "# Let's see how dot product measures similarity\n",
    "\n",
    "# Word vectors (simplified 2D for visualization)\n",
    "word_vectors = {\n",
    "    \"king\": np.array([0.9, 0.1]),\n",
    "    \"queen\": np.array([0.85, 0.15]),\n",
    "    \"man\": np.array([0.8, -0.1]),\n",
    "    \"woman\": np.array([0.75, -0.05]),\n",
    "    \"apple\": np.array([-0.2, 0.9]),\n",
    "    \"orange\": np.array([-0.15, 0.85])\n",
    "}\n",
    "\n",
    "# Compute similarity between king and all other words\n",
    "query = \"king\"\n",
    "similarities = {}\n",
    "\n",
    "for word, vector in word_vectors.items():\n",
    "    similarity = np.dot(word_vectors[query], vector)\n",
    "    similarities[word] = similarity\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_similarities = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"Words most similar to '{query}':\")\n",
    "for word, sim in sorted_similarities:\n",
    "    print(f\"  {word}: {sim:.3f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "for word, vec in word_vectors.items():\n",
    "    plt.scatter(vec[0], vec[1])\n",
    "    plt.annotate(word, (vec[0], vec[1]), xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.title('Word Vectors in 2D Space')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Matrix Multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication step by step\n",
    "A = np.array([[1, 2],\n",
    "              [3, 4]])\n",
    "B = np.array([[5, 6],\n",
    "              [7, 8]])\n",
    "\n",
    "print_matrix(A, \"A\")\n",
    "print_matrix(B, \"B\")\n",
    "\n",
    "# Manual multiplication to understand the process\n",
    "C_manual = np.zeros((2, 2))\n",
    "\n",
    "# C[0,0] = A[0,0]*B[0,0] + A[0,1]*B[1,0]\n",
    "C_manual[0, 0] = A[0, 0] * B[0, 0] + A[0, 1] * B[1, 0]\n",
    "print(f\"C[0,0] = {A[0,0]}×{B[0,0]} + {A[0,1]}×{B[1,0]} = {C_manual[0,0]}\")\n",
    "\n",
    "# Fill in the rest\n",
    "C_manual[0, 1] = A[0, 0] * B[0, 1] + A[0, 1] * B[1, 1]\n",
    "C_manual[1, 0] = A[1, 0] * B[0, 0] + A[1, 1] * B[1, 0]\n",
    "C_manual[1, 1] = A[1, 0] * B[0, 1] + A[1, 1] * B[1, 1]\n",
    "\n",
    "print(\"\\nManual result:\")\n",
    "print_matrix(C_manual, \"C (manual)\")\n",
    "\n",
    "# NumPy result\n",
    "C_numpy = A @ B\n",
    "print_matrix(C_numpy, \"C (NumPy)\")\n",
    "\n",
    "# Verify they're the same\n",
    "print(f\"Results match: {np.allclose(C_manual, C_numpy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Matrix Multiplication Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given these matrices, predict the output shape and compute the result\n",
    "X = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])  # Shape: (2, 3)\n",
    "\n",
    "W = np.array([[0.1, 0.2],\n",
    "              [0.3, 0.4],\n",
    "              [0.5, 0.6]])  # Shape: (3, 2)\n",
    "\n",
    "# YOUR CODE: What will be the shape of X @ W?\n",
    "# Remember: (2,3) @ (3,2) = (2,2)\n",
    "\n",
    "result = # YOUR CODE HERE\n",
    "print(f\"Result shape: {result.shape}\")\n",
    "print_matrix(result, \"X @ W\")\n",
    "\n",
    "# This is exactly what happens in neural networks!\n",
    "# X could be your input data (2 samples, 3 features)\n",
    "# W could be your weight matrix (3 inputs, 2 outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Neural Network from Scratch\n",
    "\n",
    "### 2.1 The Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNeuralNetwork:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * 0.01\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * 0.01\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation: max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"Sigmoid activation: 1 / (1 + e^(-x))\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def forward(self, X, verbose=False):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Layer 1: Input -> Hidden\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.relu(self.z1)\n",
    "        \n",
    "        # Layer 2: Hidden -> Output\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Input shape: {X.shape}\")\n",
    "            print(f\"After layer 1: {self.a1.shape}\")\n",
    "            print(f\"Output shape: {self.a2.shape}\")\n",
    "            print(f\"\\nLayer 1 activations (first sample):\\n{self.a1[0]}\")\n",
    "            print(f\"\\nOutput (first sample): {self.a2[0]}\")\n",
    "        \n",
    "        return self.a2\n",
    "\n",
    "# Create a small network\n",
    "nn = SimpleNeuralNetwork(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Sample input (2 samples, 3 features each)\n",
    "X = np.array([[0.5, 0.3, 0.2],\n",
    "              [0.8, 0.1, 0.9]])\n",
    "\n",
    "# Forward pass\n",
    "output = nn.forward(X, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualizing the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize what happens in each layer\n",
    "def visualize_activations(nn, X):\n",
    "    # Forward pass\n",
    "    output = nn.forward(X)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Input\n",
    "    axes[0].imshow(X, cmap='RdBu', aspect='auto')\n",
    "    axes[0].set_title('Input')\n",
    "    axes[0].set_xlabel('Features')\n",
    "    axes[0].set_ylabel('Samples')\n",
    "    axes[0].colorbar = plt.colorbar(axes[0].images[0], ax=axes[0])\n",
    "    \n",
    "    # Hidden layer activations\n",
    "    axes[1].imshow(nn.a1, cmap='RdBu', aspect='auto')\n",
    "    axes[1].set_title('Hidden Layer (after ReLU)')\n",
    "    axes[1].set_xlabel('Hidden Units')\n",
    "    axes[1].set_ylabel('Samples')\n",
    "    plt.colorbar(axes[1].images[0], ax=axes[1])\n",
    "    \n",
    "    # Output\n",
    "    axes[2].imshow(nn.a2, cmap='RdBu', aspect='auto')\n",
    "    axes[2].set_title('Output (after Sigmoid)')\n",
    "    axes[2].set_xlabel('Output Units')\n",
    "    axes[2].set_ylabel('Samples')\n",
    "    plt.colorbar(axes[2].images[0], ax=axes[2])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate more samples\n",
    "X_batch = np.random.randn(10, 3)\n",
    "visualize_activations(nn, X_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Backpropagation\n",
    "\n",
    "### 3.1 Computing Gradients Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement backpropagation for our simple network\n",
    "class NeuralNetworkWithBackprop(SimpleNeuralNetwork):\n",
    "    def backward(self, X, y_true, learning_rate=0.01):\n",
    "        m = X.shape[0]  # number of samples\n",
    "        \n",
    "        # Compute the gradient of the loss w.r.t output\n",
    "        # For binary cross-entropy with sigmoid, this simplifies to:\n",
    "        dz2 = self.a2 - y_true\n",
    "        \n",
    "        # Gradients for layer 2\n",
    "        dW2 = (1/m) * self.a1.T @ dz2\n",
    "        db2 = (1/m) * np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        # Backpropagate to layer 1\n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * (self.z1 > 0)  # ReLU derivative\n",
    "        \n",
    "        # Gradients for layer 1\n",
    "        dW1 = (1/m) * X.T @ dz1\n",
    "        db1 = (1/m) * np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        # Update weights\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        \n",
    "        return dW1, dW2\n",
    "    \n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        # Binary cross-entropy\n",
    "        epsilon = 1e-7\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Example: Training on XOR problem\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Create network\n",
    "nn_backprop = NeuralNetworkWithBackprop(input_size=2, hidden_size=4, output_size=1)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(1000):\n",
    "    # Forward pass\n",
    "    y_pred = nn_backprop.forward(X_xor)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = nn_backprop.compute_loss(y_pred, y_xor)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Backward pass\n",
    "    nn_backprop.backward(X_xor, y_xor, learning_rate=0.5)\n",
    "    \n",
    "    if epoch % 200 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Binary Cross-Entropy Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Check final predictions\n",
    "print(\"\\nFinal predictions:\")\n",
    "final_pred = nn_backprop.forward(X_xor)\n",
    "for i, (x, pred, true) in enumerate(zip(X_xor, final_pred, y_xor)):\n",
    "    print(f\"Input: {x}, Predicted: {pred[0]:.3f}, True: {true[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Gradient Flow Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize how gradients flow through the network\n",
    "def visualize_gradient_flow(nn, X, y):\n",
    "    # Forward pass\n",
    "    y_pred = nn.forward(X)\n",
    "    \n",
    "    # Store initial weights\n",
    "    W1_before = nn.W1.copy()\n",
    "    W2_before = nn.W2.copy()\n",
    "    \n",
    "    # Backward pass\n",
    "    dW1, dW2 = nn.backward(X, y, learning_rate=0.0)  # Don't update, just get gradients\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Weight matrices\n",
    "    im1 = axes[0, 0].imshow(W1_before, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title('W1 (Weights)')\n",
    "    axes[0, 0].set_xlabel('Hidden Units')\n",
    "    axes[0, 0].set_ylabel('Input Units')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    im2 = axes[0, 1].imshow(W2_before, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 1].set_title('W2 (Weights)')\n",
    "    axes[0, 1].set_xlabel('Output Units')\n",
    "    axes[0, 1].set_ylabel('Hidden Units')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    # Gradient matrices\n",
    "    im3 = axes[1, 0].imshow(dW1, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 0].set_title('dW1 (Gradients)')\n",
    "    axes[1, 0].set_xlabel('Hidden Units')\n",
    "    axes[1, 0].set_ylabel('Input Units')\n",
    "    plt.colorbar(im3, ax=axes[1, 0])\n",
    "    \n",
    "    im4 = axes[1, 1].imshow(dW2, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 1].set_title('dW2 (Gradients)')\n",
    "    axes[1, 1].set_xlabel('Output Units')\n",
    "    axes[1, 1].set_ylabel('Hidden Units')\n",
    "    plt.colorbar(im4, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a fresh network and visualize\n",
    "nn_vis = NeuralNetworkWithBackprop(input_size=2, hidden_size=4, output_size=1)\n",
    "visualize_gradient_flow(nn_vis, X_xor, y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimization Algorithms\n",
    "\n",
    "### 4.1 Comparing SGD, Momentum, and Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement different optimizers\n",
    "class Optimizers:\n",
    "    @staticmethod\n",
    "    def sgd(params, grads, learning_rate=0.01, state=None):\n",
    "        \"\"\"Vanilla SGD\"\"\"\n",
    "        for param, grad in zip(params, grads):\n",
    "            param -= learning_rate * grad\n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def momentum(params, grads, learning_rate=0.01, state=None, beta=0.9):\n",
    "        \"\"\"SGD with momentum\"\"\"\n",
    "        if state is None:\n",
    "            state = [np.zeros_like(p) for p in params]\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            state[i] = beta * state[i] + (1 - beta) * grad\n",
    "            param -= learning_rate * state[i]\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    @staticmethod\n",
    "    def adam(params, grads, learning_rate=0.001, state=None, \n",
    "             beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"Adam optimizer\"\"\"\n",
    "        if state is None:\n",
    "            state = {\n",
    "                'm': [np.zeros_like(p) for p in params],\n",
    "                'v': [np.zeros_like(p) for p in params],\n",
    "                't': 0\n",
    "            }\n",
    "        \n",
    "        state['t'] += 1\n",
    "        t = state['t']\n",
    "        \n",
    "        for i, (param, grad) in enumerate(zip(params, grads)):\n",
    "            # Update biased first moment\n",
    "            state['m'][i] = beta1 * state['m'][i] + (1 - beta1) * grad\n",
    "            # Update biased second moment\n",
    "            state['v'][i] = beta2 * state['v'][i] + (1 - beta2) * grad**2\n",
    "            \n",
    "            # Bias correction\n",
    "            m_hat = state['m'][i] / (1 - beta1**t)\n",
    "            v_hat = state['v'][i] / (1 - beta2**t)\n",
    "            \n",
    "            # Update parameters\n",
    "            param -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        return state\n",
    "\n",
    "# Visualize optimizer behavior on a simple 2D function\n",
    "def rosenbrock(x, y):\n",
    "    \"\"\"Rosenbrock function - a classic test for optimizers\"\"\"\n",
    "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
    "\n",
    "def rosenbrock_grad(x, y):\n",
    "    \"\"\"Gradient of Rosenbrock function\"\"\"\n",
    "    dx = -2 * (1 - x) - 400 * x * (y - x**2)\n",
    "    dy = 200 * (y - x**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Run different optimizers\n",
    "start_point = np.array([-1.0, 2.0])\n",
    "optimizers = {\n",
    "    'SGD': (Optimizers.sgd, 0.001),\n",
    "    'Momentum': (Optimizers.momentum, 0.001),\n",
    "    'Adam': (Optimizers.adam, 0.01)\n",
    "}\n",
    "\n",
    "trajectories = {}\n",
    "for name, (optimizer, lr) in optimizers.items():\n",
    "    point = start_point.copy()\n",
    "    trajectory = [point.copy()]\n",
    "    state = None\n",
    "    \n",
    "    for _ in range(200):\n",
    "        grad = rosenbrock_grad(point[0], point[1])\n",
    "        state = optimizer([point], [grad], learning_rate=lr, state=state)\n",
    "        trajectory.append(point.copy())\n",
    "    \n",
    "    trajectories[name] = np.array(trajectory)\n",
    "\n",
    "# Plot the optimization paths\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create contour plot\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-1, 3, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = rosenbrock(X, Y)\n",
    "\n",
    "contours = ax.contour(X, Y, Z, levels=np.logspace(-1, 3, 20), alpha=0.3)\n",
    "\n",
    "# Plot trajectories\n",
    "colors = {'SGD': 'red', 'Momentum': 'blue', 'Adam': 'green'}\n",
    "for name, trajectory in trajectories.items():\n",
    "    ax.plot(trajectory[:, 0], trajectory[:, 1], \n",
    "            'o-', color=colors[name], label=name, \n",
    "            markersize=3, alpha=0.7)\n",
    "\n",
    "# Mark the optimum\n",
    "ax.plot(1, 1, 'k*', markersize=15, label='Optimum')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_title('Optimizer Comparison on Rosenbrock Function')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. PyTorch Comparison\n",
    "\n",
    "### 5.1 Our Implementation vs PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare our implementation with PyTorch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Our NumPy implementation\n",
    "class NumpyNet:\n",
    "    def __init__(self):\n",
    "        self.W1 = np.array([[0.1, 0.2], [0.3, 0.4]])\n",
    "        self.b1 = np.array([[0.1, 0.2]])\n",
    "        self.W2 = np.array([[0.5], [0.6]])\n",
    "        self.b2 = np.array([[0.3]])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = x @ self.W1 + self.b1\n",
    "        a1 = np.maximum(0, z1)  # ReLU\n",
    "        z2 = a1 @ self.W2 + self.b2\n",
    "        return z2\n",
    "\n",
    "# PyTorch implementation\n",
    "class TorchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 2, bias=True)\n",
    "        self.fc2 = nn.Linear(2, 1, bias=True)\n",
    "        \n",
    "        # Set same weights as NumPy version\n",
    "        with torch.no_grad():\n",
    "            self.fc1.weight = nn.Parameter(torch.tensor([[0.1, 0.3], [0.2, 0.4]]))\n",
    "            self.fc1.bias = nn.Parameter(torch.tensor([0.1, 0.2]))\n",
    "            self.fc2.weight = nn.Parameter(torch.tensor([[0.5, 0.6]]))\n",
    "            self.fc2.bias = nn.Parameter(torch.tensor([0.3]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Test both implementations\n",
    "test_input = np.array([[1.0, 2.0]])\n",
    "\n",
    "# NumPy\n",
    "numpy_net = NumpyNet()\n",
    "numpy_output = numpy_net.forward(test_input)\n",
    "\n",
    "# PyTorch\n",
    "torch_net = TorchNet()\n",
    "torch_input = torch.tensor(test_input, dtype=torch.float32)\n",
    "torch_output = torch_net(torch_input)\n",
    "\n",
    "print(\"Input:\", test_input)\n",
    "print(\"NumPy output:\", numpy_output)\n",
    "print(\"PyTorch output:\", torch_output.detach().numpy())\n",
    "print(\"\\nOutputs match:\", np.allclose(numpy_output, torch_output.detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Automatic Differentiation in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch's autograd is magic - let's see it in action\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "\n",
    "# Some operations\n",
    "z = x + y\n",
    "w = z * z\n",
    "loss = w.mean()\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "print(f\"x = {x.data}\")\n",
    "print(f\"y = {y.data}\")\n",
    "print(f\"z = x + y = {z.data}\")\n",
    "print(f\"w = z * z = {w.data}\")\n",
    "print(f\"loss = mean(w) = {loss.data}\")\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "print(\"\\nGradients:\")\n",
    "print(f\"∂loss/∂x = {x.grad}\")\n",
    "print(f\"∂loss/∂y = {y.grad}\")\n",
    "\n",
    "# Verify manually\n",
    "print(\"\\nManual verification:\")\n",
    "print(\"loss = mean((x + y)²)\")\n",
    "print(\"∂loss/∂x = 2(x + y)/3\")\n",
    "manual_grad_x = 2 * (x.data + y.data) / 3\n",
    "print(f\"Manual gradient: {manual_grad_x}\")\n",
    "print(f\"Matches autograd: {torch.allclose(x.grad, manual_grad_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Takeaways\n",
    "\n",
    "### What You've Learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Dot product measures similarity\n",
    "vectors = np.random.randn(5, 2)\n",
    "similarity_matrix = vectors @ vectors.T\n",
    "im1 = axes[0, 0].imshow(similarity_matrix, cmap='RdBu')\n",
    "axes[0, 0].set_title('Dot Products = Similarity Matrix')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# 2. Matrix multiplication transforms data\n",
    "data = np.random.randn(50, 2)\n",
    "W = np.array([[0.7, -0.7], [0.7, 0.7]])  # Rotation matrix\n",
    "transformed = data @ W\n",
    "axes[0, 1].scatter(data[:, 0], data[:, 1], alpha=0.5, label='Original')\n",
    "axes[0, 1].scatter(transformed[:, 0], transformed[:, 1], alpha=0.5, label='Transformed')\n",
    "axes[0, 1].set_title('Matrix Multiplication = Transformation')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Neural networks stack transformations\n",
    "x = np.linspace(-2, 2, 100)\n",
    "axes[1, 0].plot(x, x, label='Linear')\n",
    "axes[1, 0].plot(x, np.maximum(0, x), label='ReLU(Linear)')\n",
    "axes[1, 0].plot(x, 1/(1 + np.exp(-x)), label='Sigmoid(Linear)')\n",
    "axes[1, 0].set_title('Activation Functions Add Non-linearity')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Gradients flow backward\n",
    "layer_names = ['Input', 'Hidden 1', 'Hidden 2', 'Output']\n",
    "gradient_magnitudes = [0.001, 0.01, 0.1, 1.0]\n",
    "axes[1, 1].bar(layer_names, gradient_magnitudes)\n",
    "axes[1, 1].set_title('Gradient Flow in Backpropagation')\n",
    "axes[1, 1].set_ylabel('Gradient Magnitude')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🎉 Congratulations! You now understand:\")\n",
    "print(\"✓ Linear algebra operations (dot products, matrix multiplication)\")\n",
    "print(\"✓ How neural networks transform data\")\n",
    "print(\"✓ Backpropagation and gradient flow\")\n",
    "print(\"✓ Different optimization algorithms\")\n",
    "print(\"\\n🚀 Ready for the next topic: Sequence Modeling!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exercises\n",
    "\n",
    "Try these exercises to solidify your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement a 3-layer network\n",
    "# TODO: Create a network with architecture: input(4) -> hidden1(8) -> hidden2(4) -> output(2)\n",
    "\n",
    "# Exercise 2: Implement dropout\n",
    "# TODO: Add dropout to the forward pass (hint: randomly zero out activations)\n",
    "\n",
    "# Exercise 3: Implement batch normalization\n",
    "# TODO: Normalize activations to have mean=0, std=1\n",
    "\n",
    "# Exercise 4: Visualize the decision boundary\n",
    "# TODO: Train a network on 2D data and plot its decision boundary\n",
    "\n",
    "print(\"Exercises are waiting for you! 💪\")\n",
    "print(\"Solutions are in the prerequisites.py file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
