# Prerequisites & Mathematical Foundations

Welcome to the first topic in our transformer journey! This module covers the essential mathematical concepts and neural network fundamentals you'll need to understand transformers deeply.

## üéØ Learning Objectives

By the end of this module, you will understand:
- Linear algebra operations crucial for transformers
- How neural networks learn through backpropagation
- Matrix operations and their computational efficiency
- The role of activation functions and gradients

## üìö Table of Contents

1. [Linear Algebra Essentials](#1-linear-algebra-essentials)
2. [Neural Network Fundamentals](#2-neural-network-fundamentals)
3. [Backpropagation Deep Dive](#3-backpropagation-deep-dive)
4. [Computational Considerations](#4-computational-considerations)
5. [Python Implementation](#5-python-implementation)
6. [Exercises](#6-exercises)

## 1. Linear Algebra Essentials

### 1.1 Vectors and Matrices

Transformers are fundamentally about manipulating high-dimensional vectors and matrices. Let's start with the basics:

**Vector**: A 1-dimensional array of numbers
```
v = [1, 2, 3, 4]  # Shape: (4,)
```

**Matrix**: A 2-dimensional array of numbers
```
M = [[1, 2, 3],
     [4, 5, 6]]   # Shape: (2, 3)
```

### 1.2 Dot Product

The dot product is the foundation of attention mechanisms:

```
a ¬∑ b = Œ£(a·µ¢ √ó b·µ¢)
```

For vectors a = [1, 2, 3] and b = [4, 5, 6]:
```
a ¬∑ b = (1√ó4) + (2√ó5) + (3√ó6) = 4 + 10 + 18 = 32
```

**Why it matters**: The dot product measures similarity between vectors - a key concept in attention!

### 1.3 Matrix Multiplication

Matrix multiplication is how we transform embeddings:

```
C = A @ B
```

Where `C[i,j] = Œ£(A[i,k] √ó B[k,j])`

**Rules**:
- A (m√ón) @ B (n√óp) = C (m√óp)
- Inner dimensions must match!

### 1.4 Transpose

Flipping a matrix along its diagonal:

```
A = [[1, 2],     A·µÄ = [[1, 3],
     [3, 4]]           [2, 4]]
```

### 1.5 Broadcasting

NumPy/PyTorch automatically expand dimensions for element-wise operations:

```
[1, 2, 3] + 10 = [11, 12, 13]
```

## 2. Neural Network Fundamentals

### 2.1 The Perceptron

The building block of neural networks:

```
output = activation(W¬∑x + b)
```

Where:
- `x`: input vector
- `W`: weight matrix
- `b`: bias vector
- `activation`: non-linear function

### 2.2 Activation Functions

**ReLU (Rectified Linear Unit)**:
```
ReLU(x) = max(0, x)
```

**Sigmoid**:
```
œÉ(x) = 1 / (1 + e^(-x))
```

**Tanh**:
```
tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))
```

**GELU (Gaussian Error Linear Unit)** - Used in transformers:
```
GELU(x) ‚âà 0.5x(1 + tanh(‚àö(2/œÄ)(x + 0.044715x¬≥)))
```

### 2.3 Loss Functions

**Mean Squared Error (MSE)**:
```
MSE = (1/n) Œ£(y·µ¢ - ≈∑·µ¢)¬≤
```

**Cross-Entropy** (for classification):
```
CE = -Œ£(y·µ¢ log(≈∑·µ¢))
```

## 3. Backpropagation Deep Dive

### 3.1 The Chain Rule

The heart of deep learning:

```
‚àÇL/‚àÇw = ‚àÇL/‚àÇz √ó ‚àÇz/‚àÇw
```

### 3.2 Computing Gradients

For a simple network: `z = Wx + b`, `y = œÉ(z)`, `L = (y - t)¬≤`

1. **Forward pass**: Compute outputs
2. **Backward pass**: Compute gradients
   ```
   ‚àÇL/‚àÇy = 2(y - t)
   ‚àÇL/‚àÇz = ‚àÇL/‚àÇy √ó œÉ'(z)
   ‚àÇL/‚àÇW = ‚àÇL/‚àÇz √ó x·µÄ
   ‚àÇL/‚àÇb = ‚àÇL/‚àÇz
   ```

### 3.3 Gradient Descent

Update rule:
```
W_new = W_old - Œ± √ó ‚àÇL/‚àÇW
```

Where Œ± is the learning rate.

### 3.4 Variants of Gradient Descent

**Stochastic Gradient Descent (SGD)**:
- Update after each sample

**Mini-batch Gradient Descent**:
- Update after small batches (typical in transformers)

**Adam Optimizer** (most common for transformers):
```
m_t = Œ≤‚ÇÅm_{t-1} + (1-Œ≤‚ÇÅ)g_t       # Momentum
v_t = Œ≤‚ÇÇv_{t-1} + (1-Œ≤‚ÇÇ)g_t¬≤      # RMSprop
W_t = W_{t-1} - Œ± √ó m_t/‚àö(v_t + Œµ)
```

## 4. Computational Considerations

### 4.1 Memory Layout

Row-major vs Column-major storage affects performance:
- PyTorch uses row-major (C-style)
- Operations along contiguous memory are faster

### 4.2 Vectorization

Replace loops with matrix operations:

**Slow**:
```python
for i in range(n):
    for j in range(m):
        C[i,j] = A[i,j] + B[i,j]
```

**Fast**:
```python
C = A + B  # Vectorized
```

### 4.3 GPU Acceleration

GPUs excel at parallel matrix operations:
- Thousands of cores for parallel computation
- High memory bandwidth
- Optimized for matrix multiplication

## 5. Python Implementation

See `prerequisites.py` for complete implementations of:
- Vector and matrix operations
- Neural network forward/backward pass
- Gradient descent optimizers
- Performance comparisons

Key libraries we'll use:
```python
import numpy as np      # Numerical operations
import torch           # Deep learning framework
import matplotlib.pyplot as plt  # Visualization
```

## 6. Exercises

### 6.1 Pen and Paper

1. Compute the dot product of [1, 2, 3] and [4, 5, 6]
2. Multiply matrices:
   ```
   [[1, 2],    [[5, 6],
    [3, 4]]  @  [7, 8]]
   ```
3. Derive the gradient of MSE loss

### 6.2 Coding Exercises

1. Implement matrix multiplication from scratch
2. Build a 2-layer neural network using only NumPy
3. Implement backpropagation for your network
4. Compare your implementation with PyTorch

### 6.3 Conceptual Questions

1. Why do we need non-linear activation functions?
2. What happens if we initialize all weights to zero?
3. How does batch size affect gradient descent?
4. Why is the dot product useful for measuring similarity?

## üîç Further Reading

- [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/)
- [Understanding Backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)
- [PyTorch Autograd Tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)

## üìù Summary

You've learned the mathematical foundations needed for transformers:
- **Linear algebra**: Vectors, matrices, dot products
- **Neural networks**: Forward pass, activations, loss functions
- **Backpropagation**: Chain rule, gradient computation
- **Optimization**: Gradient descent and its variants

These concepts will appear repeatedly as we build up to transformers. Make sure you're comfortable with matrix multiplication and the chain rule - they're the bread and butter of deep learning!

## ‚û°Ô∏è Next Steps

Ready to see why we need something better than RNNs? Head to [Topic 2: Introduction to Sequence Modeling](../02-sequence-modeling/) to understand the motivation behind attention mechanisms.