{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling and Optimization\n",
    "\n",
    "This notebook explores techniques for scaling transformers efficiently, including model compression, quantization, optimized inference engines, and deployment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to Model Optimization\n",
    "\n",
    "As models grow larger, optimization becomes crucial for practical deployment. We'll explore various techniques to reduce memory, improve speed, and enable deployment on resource-constrained devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Model Optimization Challenges\n",
    "\n",
    "Let's visualize the challenges of deploying large transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model scaling challenges\n",
    "def visualize_scaling_challenges():\n",
    "    \"\"\"Visualize the challenges of scaling transformer models.\"\"\"\n",
    "    \n",
    "    # Model sizes and requirements\n",
    "    models = ['BERT-Base', 'BERT-Large', 'GPT-2', 'GPT-3', 'LLaMA-7B', 'LLaMA-65B', 'GPT-4*']\n",
    "    params = [110e6, 340e6, 1.5e9, 175e9, 7e9, 65e9, 1.7e12]\n",
    "    \n",
    "    # Memory requirements (GB)\n",
    "    fp32_memory = [p * 4 / 1e9 for p in params]  # 4 bytes per param\n",
    "    fp16_memory = [p * 2 / 1e9 for p in params]  # 2 bytes per param\n",
    "    int8_memory = [p * 1 / 1e9 for p in params]  # 1 byte per param\n",
    "    int4_memory = [p * 0.5 / 1e9 for p in params]  # 0.5 bytes per param\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Memory requirements by precision\n",
    "    ax = axes[0, 0]\n",
    "    x = np.arange(len(models))\n",
    "    width = 0.2\n",
    "    \n",
    "    ax.bar(x - 1.5*width, fp32_memory, width, label='FP32', color='darkred')\n",
    "    ax.bar(x - 0.5*width, fp16_memory, width, label='FP16', color='orange')\n",
    "    ax.bar(x + 0.5*width, int8_memory, width, label='INT8', color='green')\n",
    "    ax.bar(x + 1.5*width, int4_memory, width, label='INT4', color='darkgreen')\n",
    "    \n",
    "    # Add GPU memory lines\n",
    "    gpu_limits = {'T4': 16, 'V100': 32, 'A100': 80}\n",
    "    for gpu, limit in gpu_limits.items():\n",
    "        ax.axhline(y=limit, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.text(len(models)-0.5, limit+5, gpu, fontsize=9, color='gray')\n",
    "    \n",
    "    ax.set_xlabel('Model', fontsize=12)\n",
    "    ax.set_ylabel('Memory Required (GB)', fontsize=12)\n",
    "    ax.set_title('Memory Requirements by Precision', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(models, rotation=45, ha='right')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Inference speed vs model size\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    # Simulated inference times (ms per token)\n",
    "    inference_times = [5, 8, 15, 500, 30, 200, 2000]\n",
    "    \n",
    "    scatter = ax.scatter(params, inference_times, s=100, c=range(len(models)), \n",
    "                        cmap='viridis', alpha=0.6)\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        ax.annotate(model, (params[i], inference_times[i]), \n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    ax.set_xlabel('Model Parameters', fontsize=12)\n",
    "    ax.set_ylabel('Inference Time (ms/token)', fontsize=12)\n",
    "    ax.set_title('Model Size vs Inference Speed', fontsize=14)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Optimization techniques impact\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    techniques = ['Original', 'Pruning\\n(50%)', 'Quantization\\n(INT8)', \n",
    "                 'Distillation\\n(6x smaller)', 'Combined\\nOptimization']\n",
    "    memory_reduction = [100, 60, 25, 17, 10]\n",
    "    speed_improvement = [100, 140, 180, 600, 800]\n",
    "    quality_retention = [100, 98, 97, 95, 93]\n",
    "    \n",
    "    x = np.arange(len(techniques))\n",
    "    width = 0.25\n",
    "    \n",
    "    ax.bar(x - width, memory_reduction, width, label='Memory (%)', color='lightcoral')\n",
    "    ax.bar(x, speed_improvement, width, label='Speed (%)', color='lightgreen')\n",
    "    ax.bar(x + width, quality_retention, width, label='Quality (%)', color='lightblue')\n",
    "    \n",
    "    ax.set_ylabel('Relative Performance (%)', fontsize=12)\n",
    "    ax.set_title('Impact of Optimization Techniques', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(techniques)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 4. Deployment scenarios\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    deployment_text = \"\"\"\n",
    "    🚀 Deployment Scenarios and Recommended Optimizations:\n",
    "    \n",
    "    📱 Mobile/Edge (< 1GB memory):\n",
    "       • Extreme quantization (INT4)\n",
    "       • Model distillation (10-50x smaller)\n",
    "       • Pruning + quantization combo\n",
    "    \n",
    "    💻 Desktop/Laptop (4-16GB):\n",
    "       • INT8 quantization\n",
    "       • Dynamic quantization\n",
    "       • Flash attention for long sequences\n",
    "    \n",
    "    🖥️ Server (16-80GB):\n",
    "       • FP16 mixed precision\n",
    "       • KV caching for generation\n",
    "       • Batch optimization\n",
    "    \n",
    "    ☁️ Cloud (Multi-GPU):\n",
    "       • Model parallelism\n",
    "       • Pipeline parallelism\n",
    "       • Optimized serving frameworks\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, deployment_text, transform=ax.transAxes,\n",
    "           fontsize=11, verticalalignment='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"* GPT-4 parameters are estimated\")\n",
    "\n",
    "visualize_scaling_challenges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Pruning Techniques\n",
    "\n",
    "Pruning removes unnecessary weights to reduce model size and improve inference speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MagnitudePruning:\n",
    "    \"\"\"Simple magnitude-based pruning implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, sparsity: float = 0.5):\n",
    "        self.sparsity = sparsity\n",
    "        \n",
    "    def prune_weights(self, weight: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Prune weights based on magnitude.\"\"\"\n",
    "        # Calculate threshold\n",
    "        threshold = torch.quantile(torch.abs(weight), self.sparsity)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = torch.abs(weight) > threshold\n",
    "        \n",
    "        # Apply mask\n",
    "        pruned_weight = weight * mask\n",
    "        \n",
    "        return pruned_weight, mask\n",
    "    \n",
    "    def calculate_sparsity(self, weight: torch.Tensor) -> float:\n",
    "        \"\"\"Calculate actual sparsity of tensor.\"\"\"\n",
    "        return (weight == 0).sum().item() / weight.numel()\n",
    "\n",
    "# Demonstrate pruning\n",
    "def demonstrate_pruning():\n",
    "    \"\"\"Show pruning effects on weight matrices.\"\"\"\n",
    "    \n",
    "    # Create a sample weight matrix\n",
    "    weight = torch.randn(256, 256) * 0.1\n",
    "    \n",
    "    # Apply different sparsity levels\n",
    "    sparsity_levels = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, sparsity in enumerate(sparsity_levels):\n",
    "        pruner = MagnitudePruning(sparsity=sparsity)\n",
    "        pruned_weight, mask = pruner.prune_weights(weight)\n",
    "        actual_sparsity = pruner.calculate_sparsity(pruned_weight)\n",
    "        \n",
    "        # Visualize weight matrix\n",
    "        im = axes[i].imshow(pruned_weight[:50, :50].numpy(), cmap='coolwarm', \n",
    "                           vmin=-0.3, vmax=0.3)\n",
    "        axes[i].set_title(f'Sparsity: {sparsity:.0%} (Actual: {actual_sparsity:.1%})',\n",
    "                         fontsize=12)\n",
    "        axes[i].set_xlabel('Input dimension')\n",
    "        axes[i].set_ylabel('Output dimension')\n",
    "        \n",
    "        # Calculate statistics\n",
    "        results.append({\n",
    "            'Target Sparsity': f'{sparsity:.0%}',\n",
    "            'Actual Sparsity': f'{actual_sparsity:.1%}',\n",
    "            'Non-zero Weights': (~mask).sum().item(),\n",
    "            'Compression Ratio': f'{1/(1-actual_sparsity):.1f}x'\n",
    "        })\n",
    "    \n",
    "    # Remove extra subplot\n",
    "    axes[-1].axis('off')\n",
    "    \n",
    "    plt.suptitle('Effect of Magnitude Pruning on Weight Matrices', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show statistics\n",
    "    df_results = pd.DataFrame(results)\n",
    "    print(\"\\nPruning Statistics:\")\n",
    "    display(df_results)\n",
    "    \n",
    "    # Visualize pruning distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Weight magnitude distribution\n",
    "    weights_flat = weight.flatten().numpy()\n",
    "    ax1.hist(weights_flat, bins=50, alpha=0.7, label='Original', density=True)\n",
    "    \n",
    "    pruned_50 = pruned_weight.flatten().numpy()\n",
    "    pruned_50_nonzero = pruned_50[pruned_50 != 0]\n",
    "    ax1.hist(pruned_50_nonzero, bins=50, alpha=0.7, label='After 50% pruning', density=True)\n",
    "    \n",
    "    ax1.set_xlabel('Weight Value', fontsize=12)\n",
    "    ax1.set_ylabel('Density', fontsize=12)\n",
    "    ax1.set_title('Weight Distribution Before/After Pruning', fontsize=14)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sparsity pattern\n",
    "    ax2.spy(mask[:100, :100].numpy(), markersize=1)\n",
    "    ax2.set_title('Sparsity Pattern (50% pruning, first 100x100)', fontsize=14)\n",
    "    ax2.set_xlabel('Input dimension')\n",
    "    ax2.set_ylabel('Output dimension')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_pruning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Quantization Techniques\n",
    "\n",
    "Quantization reduces numerical precision to save memory and accelerate computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantizationDemo:\n",
    "    \"\"\"Demonstrate different quantization techniques.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def quantize_tensor(tensor: torch.Tensor, bits: int = 8, \n",
    "                       symmetric: bool = True) -> Tuple[torch.Tensor, float, float]:\n",
    "        \"\"\"Quantize tensor to specified bits.\"\"\"\n",
    "        if symmetric:\n",
    "            # Symmetric quantization\n",
    "            qmin = -(2 ** (bits - 1))\n",
    "            qmax = 2 ** (bits - 1) - 1\n",
    "            scale = tensor.abs().max() / qmax\n",
    "            zero_point = 0\n",
    "        else:\n",
    "            # Asymmetric quantization\n",
    "            qmin = 0\n",
    "            qmax = 2 ** bits - 1\n",
    "            scale = (tensor.max() - tensor.min()) / (qmax - qmin)\n",
    "            zero_point = qmin - tensor.min() / scale\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.round(tensor / scale + zero_point)\n",
    "        quantized = torch.clamp(quantized, qmin, qmax)\n",
    "        \n",
    "        return quantized.to(torch.int8), scale.item(), zero_point\n",
    "    \n",
    "    @staticmethod\n",
    "    def dequantize_tensor(quantized: torch.Tensor, scale: float, \n",
    "                         zero_point: float) -> torch.Tensor:\n",
    "        \"\"\"Dequantize tensor back to float.\"\"\"\n",
    "        return (quantized.float() - zero_point) * scale\n",
    "\n",
    "# Visualize quantization effects\n",
    "def visualize_quantization():\n",
    "    \"\"\"Show effects of different quantization schemes.\"\"\"\n",
    "    \n",
    "    # Create sample data\n",
    "    x = torch.linspace(-2, 2, 1000)\n",
    "    y = torch.sin(2 * np.pi * x) + 0.1 * torch.randn_like(x)\n",
    "    \n",
    "    quantizer = QuantizationDemo()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Different bit widths\n",
    "    bit_widths = [32, 16, 8, 4, 2, 1]\n",
    "    \n",
    "    for i, (ax, bits) in enumerate(zip(axes.flatten(), bit_widths)):\n",
    "        if bits == 32:\n",
    "            # Original (float32)\n",
    "            ax.plot(x, y, 'b-', alpha=0.8, label='Original')\n",
    "            quantized_y = y\n",
    "            error = 0\n",
    "        else:\n",
    "            # Quantize and dequantize\n",
    "            q_tensor, scale, zp = quantizer.quantize_tensor(y, bits=bits)\n",
    "            quantized_y = quantizer.dequantize_tensor(q_tensor, scale, zp)\n",
    "            \n",
    "            ax.plot(x, y, 'b-', alpha=0.3, label='Original')\n",
    "            ax.plot(x, quantized_y, 'r-', alpha=0.8, label=f'{bits}-bit')\n",
    "            \n",
    "            # Calculate error\n",
    "            error = torch.mean((y - quantized_y) ** 2).item()\n",
    "        \n",
    "        ax.set_title(f'{bits}-bit Quantization\\nMSE: {error:.6f}', fontsize=12)\n",
    "        ax.set_xlabel('x')\n",
    "        ax.set_ylabel('y')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Effect of Quantization on Signal Representation', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Quantization error analysis\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Error vs bit width\n",
    "    errors = []\n",
    "    compression_ratios = []\n",
    "    \n",
    "    for bits in range(1, 17):\n",
    "        q_tensor, scale, zp = quantizer.quantize_tensor(y, bits=bits)\n",
    "        dequantized = quantizer.dequantize_tensor(q_tensor, scale, zp)\n",
    "        error = torch.mean((y - dequantized) ** 2).item()\n",
    "        errors.append(error)\n",
    "        compression_ratios.append(32 / bits)\n",
    "    \n",
    "    ax1.semilogy(range(1, 17), errors, 'bo-')\n",
    "    ax1.set_xlabel('Bit Width', fontsize=12)\n",
    "    ax1.set_ylabel('Mean Squared Error', fontsize=12)\n",
    "    ax1.set_title('Quantization Error vs Bit Width', fontsize=14)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for common bit widths\n",
    "    for bits in [1, 2, 4, 8, 16]:\n",
    "        idx = bits - 1\n",
    "        ax1.annotate(f'{bits}bit', (bits, errors[idx]), \n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    # Compression ratio\n",
    "    ax2.plot(range(1, 17), compression_ratios, 'go-')\n",
    "    ax2.set_xlabel('Bit Width', fontsize=12)\n",
    "    ax2.set_ylabel('Compression Ratio', fontsize=12)\n",
    "    ax2.set_title('Memory Compression vs Bit Width', fontsize=14)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_quantization()\n",
    "\n",
    "# Compare quantization schemes\n",
    "print(\"\\n--- Quantization Schemes Comparison ---\")\n",
    "\n",
    "# Create a sample weight matrix\n",
    "weight = torch.randn(100, 100) * 0.5\n",
    "\n",
    "schemes = [\n",
    "    ('Symmetric INT8', 8, True),\n",
    "    ('Asymmetric INT8', 8, False),\n",
    "    ('INT4', 4, True),\n",
    "    ('Binary', 1, True)\n",
    "]\n",
    "\n",
    "quantizer = QuantizationDemo()\n",
    "results = []\n",
    "\n",
    "for name, bits, symmetric in schemes:\n",
    "    q_weight, scale, zp = quantizer.quantize_tensor(weight, bits, symmetric)\n",
    "    dq_weight = quantizer.dequantize_tensor(q_weight, scale, zp)\n",
    "    \n",
    "    error = torch.mean((weight - dq_weight) ** 2).item()\n",
    "    compression = 32 / bits\n",
    "    \n",
    "    results.append({\n",
    "        'Scheme': name,\n",
    "        'Bits': bits,\n",
    "        'Scale': f'{scale:.4f}',\n",
    "        'Zero Point': f'{zp:.1f}',\n",
    "        'MSE': f'{error:.6f}',\n",
    "        'Compression': f'{compression:.1f}x'\n",
    "    })\n",
    "\n",
    "df_quant = pd.DataFrame(results)\n",
    "display(df_quant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Knowledge Distillation\n",
    "\n",
    "Distillation transfers knowledge from a large teacher model to a smaller student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_distillation():\n",
    "    \"\"\"Visualize knowledge distillation concept and process.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Distillation concept\n",
    "    ax = axes[0, 0]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Knowledge Distillation Concept', fontsize=14)\n",
    "    \n",
    "    # Teacher model\n",
    "    teacher_rect = plt.Rectangle((0.1, 0.5), 0.3, 0.4, \n",
    "                                facecolor='lightblue', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(teacher_rect)\n",
    "    ax.text(0.25, 0.7, 'Teacher\\nModel\\n(Large)', ha='center', va='center', \n",
    "           fontsize=12, weight='bold')\n",
    "    \n",
    "    # Student model\n",
    "    student_rect = plt.Rectangle((0.6, 0.6), 0.2, 0.2, \n",
    "                                facecolor='lightgreen', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(student_rect)\n",
    "    ax.text(0.7, 0.7, 'Student\\nModel\\n(Small)', ha='center', va='center', \n",
    "           fontsize=10, weight='bold')\n",
    "    \n",
    "    # Knowledge transfer\n",
    "    ax.arrow(0.4, 0.7, 0.15, 0, head_width=0.03, head_length=0.03, \n",
    "            fc='red', ec='red', linewidth=2)\n",
    "    ax.text(0.475, 0.75, 'Knowledge\\nTransfer', ha='center', fontsize=10, color='red')\n",
    "    \n",
    "    # Inputs/Outputs\n",
    "    ax.text(0.25, 0.4, 'Input', ha='center', fontsize=10)\n",
    "    ax.text(0.25, 0.3, 'Soft Labels\\n(Probabilities)', ha='center', fontsize=9, style='italic')\n",
    "    ax.text(0.7, 0.5, 'Input', ha='center', fontsize=10)\n",
    "    ax.text(0.7, 0.4, 'Learn from\\nSoft Labels', ha='center', fontsize=9, style='italic')\n",
    "    \n",
    "    # Size comparison\n",
    "    ax.text(0.5, 0.1, 'Teacher: 340M params → Student: 60M params (5.7x compression)', \n",
    "           ha='center', fontsize=11, bbox=dict(boxstyle='round', facecolor='wheat'))\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 2. Temperature effect\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    # Softmax with different temperatures\n",
    "    logits = torch.tensor([2.0, 1.0, 0.5, 0.3, -0.5])\n",
    "    temperatures = [0.5, 1.0, 3.0, 10.0]\n",
    "    \n",
    "    x = np.arange(len(logits))\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        probs = F.softmax(logits / temp, dim=0)\n",
    "        ax.plot(x, probs, 'o-', label=f'T={temp}', markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Class', fontsize=12)\n",
    "    ax.set_ylabel('Probability', fontsize=12)\n",
    "    ax.set_title('Effect of Temperature on Softmax', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Cat', 'Dog', 'Bird', 'Fish', 'Other'])\n",
    "    \n",
    "    # 3. Loss composition\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    # Pie chart of loss components\n",
    "    sizes = [70, 30]  # Distillation loss, Student loss\n",
    "    labels = ['Distillation Loss\\n(KL Divergence)', 'Student Loss\\n(Cross Entropy)']\n",
    "    colors = ['lightcoral', 'lightblue']\n",
    "    explode = (0.1, 0)\n",
    "    \n",
    "    ax.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.0f%%',\n",
    "          shadow=True, startangle=90)\n",
    "    ax.set_title('Typical Loss Composition (α=0.7)', fontsize=14)\n",
    "    \n",
    "    # 4. Performance comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    methods = ['Teacher\\n(BERT-L)', 'Student\\n(No KD)', 'Student\\n(With KD)', \n",
    "              'Student\\n(KD+Data Aug)']\n",
    "    accuracy = [93.5, 85.2, 91.8, 92.5]\n",
    "    params = [340, 60, 60, 60]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, accuracy, width, label='Accuracy (%)', color='lightgreen')\n",
    "    \n",
    "    # Add parameter count on top\n",
    "    for i, (bar, param) in enumerate(zip(bars1, params)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "               f'{param}M', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "    ax.set_title('Distillation Performance Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.set_ylim(80, 95)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add performance retention percentage\n",
    "    teacher_acc = accuracy[0]\n",
    "    for i in range(1, len(accuracy)):\n",
    "        retention = accuracy[i] / teacher_acc * 100\n",
    "        ax.text(i, 82, f'{retention:.1f}%\\nretention', ha='center', fontsize=9, \n",
    "               color='darkgreen', weight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_distillation()\n",
    "\n",
    "# Distillation loss implementation\n",
    "print(\"\\n--- Distillation Loss Example ---\")\n",
    "\n",
    "def distillation_loss(student_logits, teacher_logits, labels, temperature=3.0, alpha=0.7):\n",
    "    \"\"\"Calculate combined distillation and student loss.\"\"\"\n",
    "    \n",
    "    # Soft targets from teacher\n",
    "    soft_targets = F.softmax(teacher_logits / temperature, dim=-1)\n",
    "    \n",
    "    # Student predictions with temperature\n",
    "    soft_predictions = F.log_softmax(student_logits / temperature, dim=-1)\n",
    "    \n",
    "    # KL divergence loss (distillation)\n",
    "    distill_loss = F.kl_div(soft_predictions, soft_targets, reduction='batchmean')\n",
    "    distill_loss *= temperature ** 2  # Scale by T^2\n",
    "    \n",
    "    # Standard cross-entropy loss\n",
    "    student_loss = F.cross_entropy(student_logits, labels)\n",
    "    \n",
    "    # Combined loss\n",
    "    total_loss = alpha * distill_loss + (1 - alpha) * student_loss\n",
    "    \n",
    "    return total_loss, distill_loss, student_loss\n",
    "\n",
    "# Example\n",
    "batch_size = 4\n",
    "num_classes = 10\n",
    "\n",
    "teacher_logits = torch.randn(batch_size, num_classes) * 2\n",
    "student_logits = torch.randn(batch_size, num_classes)\n",
    "labels = torch.randint(0, num_classes, (batch_size,))\n",
    "\n",
    "total, distill, student = distillation_loss(student_logits, teacher_logits, labels)\n",
    "\n",
    "print(f\"Distillation Loss: {distill:.4f}\")\n",
    "print(f\"Student Loss: {student:.4f}\")\n",
    "print(f\"Total Loss: {total:.4f}\")\n",
    "print(f\"\\nLoss Breakdown:\")\n",
    "print(f\"  Distillation: {0.7 * distill / total * 100:.1f}%\")\n",
    "print(f\"  Student: {0.3 * student / total * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Optimized Attention Mechanisms\n",
    "\n",
    "Efficient attention implementations are crucial for scaling to longer sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_optimizations():\n",
    "    \"\"\"Visualize different attention optimization techniques.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    seq_len = 16\n",
    "    \n",
    "    # 1. Standard Attention\n",
    "    ax = axes[0, 0]\n",
    "    standard_mask = np.ones((seq_len, seq_len))\n",
    "    im = ax.imshow(standard_mask, cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_title('Standard Attention\\nO(n²) memory', fontsize=12)\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Queries')\n",
    "    \n",
    "    # 2. Sparse Attention\n",
    "    ax = axes[0, 1]\n",
    "    sparse_mask = np.zeros((seq_len, seq_len))\n",
    "    # Local attention window\n",
    "    window_size = 4\n",
    "    for i in range(seq_len):\n",
    "        for j in range(max(0, i-window_size//2), min(seq_len, i+window_size//2+1)):\n",
    "            sparse_mask[i, j] = 1\n",
    "    # Global attention tokens\n",
    "    sparse_mask[0, :] = 1\n",
    "    sparse_mask[:, 0] = 1\n",
    "    \n",
    "    ax.imshow(sparse_mask, cmap='Greens', vmin=0, vmax=1)\n",
    "    ax.set_title('Sparse Attention\\nO(n·w) memory', fontsize=12)\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Queries')\n",
    "    \n",
    "    # 3. Flash Attention blocks\n",
    "    ax = axes[0, 2]\n",
    "    block_size = 4\n",
    "    flash_visual = np.zeros((seq_len, seq_len))\n",
    "    \n",
    "    # Show block processing\n",
    "    for i in range(0, seq_len, block_size):\n",
    "        for j in range(0, seq_len, block_size):\n",
    "            # Different colors for different blocks\n",
    "            color = (i//block_size + j//block_size) % 3 + 1\n",
    "            flash_visual[i:i+block_size, j:j+block_size] = color / 3\n",
    "    \n",
    "    ax.imshow(flash_visual, cmap='viridis')\n",
    "    ax.set_title('Flash Attention\\nBlock-wise computation', fontsize=12)\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Queries')\n",
    "    \n",
    "    # 4. Multi-Query Attention\n",
    "    ax = axes[1, 0]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Visualize MQA\n",
    "    num_heads = 8\n",
    "    head_dim = 64\n",
    "    \n",
    "    # Standard MHA\n",
    "    ax.text(0.2, 0.9, 'Multi-Head Attention', fontsize=12, weight='bold')\n",
    "    for i in range(num_heads):\n",
    "        # Q, K, V for each head\n",
    "        y_pos = 0.7 - i * 0.08\n",
    "        for j, (label, color) in enumerate([('Q', 'lightblue'), \n",
    "                                           ('K', 'lightgreen'), \n",
    "                                           ('V', 'lightcoral')]):\n",
    "            rect = plt.Rectangle((0.1 + j*0.06, y_pos), 0.05, 0.06,\n",
    "                               facecolor=color, edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    # MQA\n",
    "    ax.text(0.6, 0.9, 'Multi-Query Attention', fontsize=12, weight='bold')\n",
    "    # Multiple Q heads\n",
    "    for i in range(num_heads):\n",
    "        y_pos = 0.7 - i * 0.08\n",
    "        rect = plt.Rectangle((0.5, y_pos), 0.05, 0.06,\n",
    "                           facecolor='lightblue', edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    # Single K, V\n",
    "    rect_k = plt.Rectangle((0.56, 0.4), 0.05, 0.06,\n",
    "                         facecolor='lightgreen', edgecolor='black', linewidth=2)\n",
    "    rect_v = plt.Rectangle((0.62, 0.4), 0.05, 0.06,\n",
    "                         facecolor='lightcoral', edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect_k)\n",
    "    ax.add_patch(rect_v)\n",
    "    \n",
    "    ax.text(0.2, 0.05, f'Memory: {num_heads * 3 * head_dim} per token', \n",
    "           ha='center', fontsize=10)\n",
    "    ax.text(0.6, 0.05, f'Memory: {num_heads + 2} * {head_dim} per token\\n'\n",
    "                       f'({(num_heads + 2) / (num_heads * 3) * 100:.0f}% of MHA)', \n",
    "           ha='center', fontsize=10)\n",
    "    \n",
    "    ax.set_xlim(0, 0.8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 5. Attention complexity comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    seq_lengths = np.array([512, 1024, 2048, 4096, 8192, 16384])\n",
    "    \n",
    "    # Memory requirements (relative)\n",
    "    standard_mem = seq_lengths ** 2\n",
    "    sparse_mem = seq_lengths * 256  # window size 256\n",
    "    flash_mem = seq_lengths * 64  # block size 64\n",
    "    \n",
    "    ax.loglog(seq_lengths, standard_mem, 'b-o', label='Standard Attention')\n",
    "    ax.loglog(seq_lengths, sparse_mem, 'g-s', label='Sparse Attention')\n",
    "    ax.loglog(seq_lengths, flash_mem, 'r-^', label='Flash Attention')\n",
    "    \n",
    "    ax.set_xlabel('Sequence Length', fontsize=12)\n",
    "    ax.set_ylabel('Memory (relative)', fontsize=12)\n",
    "    ax.set_title('Attention Memory Scaling', fontsize=14)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Speed comparison\n",
    "    ax = axes[1, 2]\n",
    "    \n",
    "    methods = ['Standard', 'Flash\\nAttention', 'xFormers', 'Sparse\\n(BigBird)']\n",
    "    relative_speed = [1.0, 2.5, 2.2, 3.8]\n",
    "    memory_usage = [100, 30, 35, 25]\n",
    "    \n",
    "    x = np.arange(len(methods))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, relative_speed, width, label='Speed (x)', color='lightgreen')\n",
    "    bars2 = ax.bar(x + width/2, memory_usage, width, label='Memory (%)', color='lightcoral')\n",
    "    \n",
    "    ax.set_ylabel('Relative Performance', fontsize=12)\n",
    "    ax.set_title('Attention Implementation Comparison\\n(8K sequence length)', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(methods)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention_optimizations()\n",
    "\n",
    "# KV Cache demonstration\n",
    "print(\"\\n--- KV Cache for Generation ---\")\n",
    "\n",
    "class KVCacheDemo:\n",
    "    \"\"\"Simple KV cache demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_seq_len=2048, num_heads=8, head_dim=64):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        \n",
    "        # Pre-allocate cache\n",
    "        self.k_cache = torch.zeros(1, num_heads, max_seq_len, head_dim)\n",
    "        self.v_cache = torch.zeros(1, num_heads, max_seq_len, head_dim)\n",
    "        self.cache_len = 0\n",
    "        \n",
    "    def update(self, k_new, v_new):\n",
    "        \"\"\"Update cache with new key-value pairs.\"\"\"\n",
    "        seq_len = k_new.shape[2]\n",
    "        \n",
    "        # Store in cache\n",
    "        self.k_cache[:, :, self.cache_len:self.cache_len + seq_len] = k_new\n",
    "        self.v_cache[:, :, self.cache_len:self.cache_len + seq_len] = v_new\n",
    "        self.cache_len += seq_len\n",
    "        \n",
    "        return self.k_cache[:, :, :self.cache_len], self.v_cache[:, :, :self.cache_len]\n",
    "    \n",
    "    def memory_usage(self):\n",
    "        \"\"\"Calculate memory usage in MB.\"\"\"\n",
    "        total_elements = 2 * self.max_seq_len * self.num_heads * self.head_dim\n",
    "        return total_elements * 4 / (1024 ** 2)  # float32 in MB\n",
    "\n",
    "# Demonstrate cache usage\n",
    "cache = KVCacheDemo()\n",
    "print(f\"KV Cache initialized:\")\n",
    "print(f\"  Max sequence length: {cache.max_seq_len}\")\n",
    "print(f\"  Memory allocated: {cache.memory_usage():.1f} MB\")\n",
    "\n",
    "# Simulate generation\n",
    "print(\"\\nSimulating token generation:\")\n",
    "for i in range(5):\n",
    "    # New token generates new K, V\n",
    "    k_new = torch.randn(1, cache.num_heads, 1, cache.head_dim)\n",
    "    v_new = torch.randn(1, cache.num_heads, 1, cache.head_dim)\n",
    "    \n",
    "    k_full, v_full = cache.update(k_new, v_new)\n",
    "    print(f\"  Step {i+1}: Cache contains {cache.cache_len} tokens\")\n",
    "\n",
    "print(f\"\\nWithout cache: {5 * (5+1) / 2} attention computations\")\n",
    "print(f\"With cache: {5} attention computations (5x speedup!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deployment Optimization Strategies\n",
    "\n",
    "Let's explore strategies for efficient model deployment in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_deployment_strategies():\n",
    "    \"\"\"Visualize different deployment optimization strategies.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Batching strategies\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    # Timeline visualization\n",
    "    requests = [\n",
    "        {'id': 'A', 'arrival': 0, 'length': 100},\n",
    "        {'id': 'B', 'arrival': 10, 'length': 50},\n",
    "        {'id': 'C', 'arrival': 15, 'length': 150},\n",
    "        {'id': 'D', 'arrival': 20, 'length': 75},\n",
    "        {'id': 'E', 'arrival': 25, 'length': 100}\n",
    "    ]\n",
    "    \n",
    "    # Static batching\n",
    "    y_static = 3\n",
    "    batch_size = 3\n",
    "    for i in range(0, len(requests), batch_size):\n",
    "        batch = requests[i:i+batch_size]\n",
    "        start = max(r['arrival'] for r in batch)\n",
    "        duration = max(r['length'] for r in batch)\n",
    "        \n",
    "        rect = plt.Rectangle((start, y_static), duration, 0.8,\n",
    "                           facecolor='lightblue', edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        \n",
    "        for j, req in enumerate(batch):\n",
    "            ax.text(start + 5, y_static + 0.2 + j*0.2, req['id'], fontsize=8)\n",
    "    \n",
    "    ax.text(-20, y_static + 0.4, 'Static\\nBatching', fontsize=10, va='center')\n",
    "    \n",
    "    # Dynamic batching\n",
    "    y_dynamic = 1.5\n",
    "    current_time = 0\n",
    "    while requests:\n",
    "        # Collect requests within time window\n",
    "        batch = []\n",
    "        for req in requests[:]:\n",
    "            if req['arrival'] <= current_time + 10:\n",
    "                batch.append(req)\n",
    "                requests.remove(req)\n",
    "            if len(batch) >= 3:\n",
    "                break\n",
    "        \n",
    "        if batch:\n",
    "            start = current_time\n",
    "            duration = max(r['length'] for r in batch)\n",
    "            \n",
    "            rect = plt.Rectangle((start, y_dynamic), duration, 0.8,\n",
    "                               facecolor='lightgreen', edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "            \n",
    "            for j, req in enumerate(batch):\n",
    "                ax.text(start + 5, y_dynamic + 0.2 + j*0.2, req['id'], fontsize=8)\n",
    "            \n",
    "            current_time = start + duration\n",
    "        else:\n",
    "            current_time += 10\n",
    "    \n",
    "    ax.text(-20, y_dynamic + 0.4, 'Dynamic\\nBatching', fontsize=10, va='center')\n",
    "    \n",
    "    # Continuous batching\n",
    "    y_continuous = 0\n",
    "    ax.text(-20, y_continuous + 0.4, 'Continuous\\nBatching', fontsize=10, va='center')\n",
    "    ax.text(50, y_continuous + 0.4, '(Iteration-level batching for generation)',\n",
    "           fontsize=9, style='italic')\n",
    "    \n",
    "    ax.set_xlim(-30, 200)\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "    ax.set_title('Batching Strategies Comparison', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # 2. Model serving architecture\n",
    "    ax = axes[0, 1]\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Production Serving Architecture', fontsize=14)\n",
    "    \n",
    "    # Components\n",
    "    components = [\n",
    "        {'name': 'Load\\nBalancer', 'pos': (0.5, 0.9), 'color': 'lightcoral'},\n",
    "        {'name': 'Model\\nServer 1', 'pos': (0.2, 0.6), 'color': 'lightblue'},\n",
    "        {'name': 'Model\\nServer 2', 'pos': (0.5, 0.6), 'color': 'lightblue'},\n",
    "        {'name': 'Model\\nServer 3', 'pos': (0.8, 0.6), 'color': 'lightblue'},\n",
    "        {'name': 'KV Cache\\nStore', 'pos': (0.2, 0.3), 'color': 'lightyellow'},\n",
    "        {'name': 'Model\\nRegistry', 'pos': (0.5, 0.3), 'color': 'lightgreen'},\n",
    "        {'name': 'Metrics\\nCollector', 'pos': (0.8, 0.3), 'color': 'lightgray'}\n",
    "    ]\n",
    "    \n",
    "    for comp in components:\n",
    "        rect = plt.Rectangle((comp['pos'][0] - 0.08, comp['pos'][1] - 0.05),\n",
    "                           0.16, 0.1, facecolor=comp['color'], edgecolor='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(comp['pos'][0], comp['pos'][1], comp['name'], \n",
    "               ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    # Connections\n",
    "    connections = [\n",
    "        ((0.5, 0.85), (0.2, 0.65)),\n",
    "        ((0.5, 0.85), (0.5, 0.65)),\n",
    "        ((0.5, 0.85), (0.8, 0.65)),\n",
    "        ((0.2, 0.55), (0.2, 0.35)),\n",
    "        ((0.5, 0.55), (0.5, 0.35)),\n",
    "        ((0.8, 0.55), (0.8, 0.35))\n",
    "    ]\n",
    "    \n",
    "    for start, end in connections:\n",
    "        ax.plot([start[0], end[0]], [start[1], end[1]], 'k-', linewidth=1)\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # 3. Optimization techniques comparison\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    techniques = ['Baseline', 'Quantized\\n(INT8)', 'Pruned\\n(50%)', \n",
    "                 'Distilled\\n(6x)', 'All\\nCombined']\n",
    "    throughput = [100, 180, 150, 600, 1000]\n",
    "    latency = [50, 30, 35, 10, 8]\n",
    "    \n",
    "    x = np.arange(len(techniques))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, throughput, width, label='Throughput', color='lightgreen')\n",
    "    bars2 = ax2.bar(x + width/2, latency, width, label='Latency', color='lightcoral')\n",
    "    \n",
    "    ax.set_xlabel('Optimization Technique', fontsize=12)\n",
    "    ax.set_ylabel('Throughput (req/s)', fontsize=12, color='green')\n",
    "    ax2.set_ylabel('Latency (ms)', fontsize=12, color='red')\n",
    "    ax.set_title('Performance Impact of Optimizations', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(techniques)\n",
    "    \n",
    "    # Legends\n",
    "    ax.legend(loc='upper left')\n",
    "    ax2.legend(loc='upper right')\n",
    "    \n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Deployment checklist\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    checklist = \"\"\"\n",
    "    ✅ Deployment Optimization Checklist:\n",
    "    \n",
    "    🔧 Model Optimization:\n",
    "    □ Quantization (INT8/INT4)\n",
    "    □ Pruning (structured/unstructured)\n",
    "    □ Knowledge distillation\n",
    "    □ Operator fusion\n",
    "    \n",
    "    🚀 Inference Optimization:\n",
    "    □ KV caching for generation\n",
    "    □ Flash/Memory-efficient attention\n",
    "    □ Dynamic batching\n",
    "    □ Continuous batching\n",
    "    \n",
    "    🖥️ Infrastructure:\n",
    "    □ GPU/Hardware selection\n",
    "    □ Model parallelism setup\n",
    "    □ Load balancing\n",
    "    □ Auto-scaling policies\n",
    "    \n",
    "    📊 Monitoring:\n",
    "    □ Latency tracking (p50, p95, p99)\n",
    "    □ Throughput metrics\n",
    "    □ GPU utilization\n",
    "    □ Memory usage\n",
    "    □ Error rates\n",
    "    \"\"\"\n",
    "    \n",
    "    ax.text(0.05, 0.95, checklist, transform=ax.transAxes,\n",
    "           fontsize=10, verticalalignment='top', family='monospace',\n",
    "           bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_deployment_strategies()\n",
    "\n",
    "# Performance benchmarking example\n",
    "print(\"\\n--- Performance Benchmarking Example ---\")\n",
    "\n",
    "def benchmark_optimization(model_size_mb=1000, optimization='none'):\n",
    "    \"\"\"Simulate performance metrics for different optimizations.\"\"\"\n",
    "    \n",
    "    # Base metrics\n",
    "    base_latency = model_size_mb * 0.05  # ms\n",
    "    base_throughput = 1000 / base_latency  # req/s\n",
    "    base_memory = model_size_mb  # MB\n",
    "    \n",
    "    # Apply optimization effects\n",
    "    optimizations = {\n",
    "        'none': {'latency': 1.0, 'throughput': 1.0, 'memory': 1.0, 'quality': 1.0},\n",
    "        'int8': {'latency': 0.6, 'throughput': 1.8, 'memory': 0.25, 'quality': 0.98},\n",
    "        'int4': {'latency': 0.5, 'throughput': 2.2, 'memory': 0.125, 'quality': 0.95},\n",
    "        'pruning': {'latency': 0.7, 'throughput': 1.5, 'memory': 0.6, 'quality': 0.97},\n",
    "        'distillation': {'latency': 0.2, 'throughput': 6.0, 'memory': 0.17, 'quality': 0.93},\n",
    "        'combined': {'latency': 0.15, 'throughput': 10.0, 'memory': 0.1, 'quality': 0.90}\n",
    "    }\n",
    "    \n",
    "    opt = optimizations[optimization]\n",
    "    \n",
    "    return {\n",
    "        'optimization': optimization,\n",
    "        'latency_ms': base_latency * opt['latency'],\n",
    "        'throughput_rps': base_throughput * opt['throughput'],\n",
    "        'memory_mb': base_memory * opt['memory'],\n",
    "        'quality_retention': opt['quality'] * 100,\n",
    "        'speedup': 1 / opt['latency'],\n",
    "        'compression': 1 / opt['memory']\n",
    "    }\n",
    "\n",
    "# Benchmark different optimizations\n",
    "print(f\"Benchmarking 1GB model with different optimizations:\\n\")\n",
    "\n",
    "results = []\n",
    "for opt in ['none', 'int8', 'int4', 'pruning', 'distillation', 'combined']:\n",
    "    metrics = benchmark_optimization(1000, opt)\n",
    "    results.append(metrics)\n",
    "\n",
    "df_benchmark = pd.DataFrame(results)\n",
    "display(df_benchmark.round(1))\n",
    "\n",
    "# Cost analysis\n",
    "print(\"\\n--- Deployment Cost Analysis ---\")\n",
    "\n",
    "gpu_cost_per_hour = 2.0  # $/hour for A100\n",
    "requests_per_day = 1_000_000\n",
    "\n",
    "print(f\"Assumptions: {requests_per_day:,} requests/day, ${gpu_cost_per_hour}/GPU-hour\\n\")\n",
    "\n",
    "for _, row in df_benchmark.iterrows():\n",
    "    # Calculate required GPUs\n",
    "    rps_per_gpu = row['throughput_rps']\n",
    "    gpus_needed = np.ceil(requests_per_day / (rps_per_gpu * 86400))\n",
    "    daily_cost = gpus_needed * gpu_cost_per_hour * 24\n",
    "    \n",
    "    print(f\"{row['optimization']:12} | GPUs: {int(gpus_needed):2} | \"\n",
    "          f\"Cost: ${daily_cost:6.0f}/day | \"\n",
    "          f\"Quality: {row['quality_retention']:.0f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hardware-Specific Optimizations\n",
    "\n",
    "Different hardware platforms require different optimization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardware optimization guide\n",
    "hardware_optimizations = pd.DataFrame({\n",
    "    'Platform': ['NVIDIA GPU', 'TPU', 'CPU', 'Mobile/Edge', 'Apple Silicon'],\n",
    "    'Key Optimizations': [\n",
    "        'TensorRT, FP16, Flash Attention, CUDA graphs',\n",
    "        'XLA compilation, bfloat16, TPU embeddings',\n",
    "        'ONNX Runtime, INT8, OpenVINO, operator fusion',\n",
    "        'TFLite, Core ML, NNAPI, extreme quantization',\n",
    "        'Core ML, Metal Performance Shaders, ANE'\n",
    "    ],\n",
    "    'Best Practices': [\n",
    "        'Use mixed precision, optimize memory transfers',\n",
    "        'Batch efficiently, use TPU-specific ops',\n",
    "        'Vectorize operations, use MKL-DNN',\n",
    "        'Model splitting, on-device caching',\n",
    "        'Leverage unified memory, use Metal'\n",
    "    ],\n",
    "    'Typical Speedup': ['5-10x', '10-20x', '2-5x', '10-50x', '5-15x']\n",
    "})\n",
    "\n",
    "print(\"\\n🔧 Hardware-Specific Optimization Guide\")\n",
    "print(\"=\" * 80)\n",
    "display(hardware_optimizations)\n",
    "\n",
    "# Memory bandwidth analysis\n",
    "print(\"\\n📊 Memory Bandwidth Requirements\")\n",
    "\n",
    "def calculate_bandwidth_requirements(model_size_gb, batch_size, seq_len, dtype_bytes=2):\n",
    "    \"\"\"Calculate memory bandwidth requirements for transformer inference.\"\"\"\n",
    "    \n",
    "    # Model weights read\n",
    "    weights_read = model_size_gb * 1e9 * dtype_bytes\n",
    "    \n",
    "    # Activations (rough estimate)\n",
    "    hidden_size = int((model_size_gb * 1e9 / 100) ** 0.5)\n",
    "    num_layers = 32  # typical\n",
    "    \n",
    "    activation_memory = (\n",
    "        batch_size * seq_len * hidden_size * num_layers * dtype_bytes * 4\n",
    "    )\n",
    "    \n",
    "    # KV cache\n",
    "    kv_memory = batch_size * seq_len * hidden_size * num_layers * 2 * dtype_bytes\n",
    "    \n",
    "    total_memory = weights_read + activation_memory + kv_memory\n",
    "    \n",
    "    # Assume 100ms latency target\n",
    "    bandwidth_gbps = total_memory / 1e9 / 0.1\n",
    "    \n",
    "    return {\n",
    "        'weights_gb': weights_read / 1e9,\n",
    "        'activations_gb': activation_memory / 1e9,\n",
    "        'kv_cache_gb': kv_memory / 1e9,\n",
    "        'total_gb': total_memory / 1e9,\n",
    "        'bandwidth_gbps': bandwidth_gbps\n",
    "    }\n",
    "\n",
    "# Compare different scenarios\n",
    "scenarios = [\n",
    "    ('7B model, batch=1', 7, 1, 2048),\n",
    "    ('7B model, batch=32', 7, 32, 2048),\n",
    "    ('70B model, batch=1', 70, 1, 2048),\n",
    "    ('70B model, batch=8', 70, 8, 2048)\n",
    "]\n",
    "\n",
    "bandwidth_results = []\n",
    "for name, size, batch, seq in scenarios:\n",
    "    reqs = calculate_bandwidth_requirements(size, batch, seq)\n",
    "    reqs['scenario'] = name\n",
    "    bandwidth_results.append(reqs)\n",
    "\n",
    "df_bandwidth = pd.DataFrame(bandwidth_results)\n",
    "df_bandwidth = df_bandwidth[['scenario', 'weights_gb', 'activations_gb', \n",
    "                           'kv_cache_gb', 'total_gb', 'bandwidth_gbps']]\n",
    "\n",
    "print(\"\\nMemory bandwidth requirements (100ms latency target):\")\n",
    "display(df_bandwidth.round(1))\n",
    "\n",
    "# Hardware capabilities\n",
    "print(\"\\n🖥️ Hardware Memory Bandwidth Capabilities:\")\n",
    "hardware_bandwidth = {\n",
    "    'CPU (DDR4)': 100,\n",
    "    'V100': 900,\n",
    "    'A100': 1555,\n",
    "    'H100': 3350,\n",
    "    'TPU v4': 1200\n",
    "}\n",
    "\n",
    "for hw, bw in hardware_bandwidth.items():\n",
    "    print(f\"  {hw:<15}: {bw:>5} GB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. End-to-End Optimization Pipeline\n",
    "\n",
    "Let's create a complete optimization pipeline for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationPipeline:\n",
    "    \"\"\"Complete optimization pipeline for deployment.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"transformer-base\"):\n",
    "        self.model_name = model_name\n",
    "        self.optimization_steps = []\n",
    "        \n",
    "    def add_optimization(self, name: str, speedup: float, compression: float, \n",
    "                        quality_impact: float):\n",
    "        \"\"\"Add an optimization step.\"\"\"\n",
    "        self.optimization_steps.append({\n",
    "            'name': name,\n",
    "            'speedup': speedup,\n",
    "            'compression': compression,\n",
    "            'quality_impact': quality_impact\n",
    "        })\n",
    "        \n",
    "    def analyze(self, base_params: int = 1e9, base_latency: float = 100):\n",
    "        \"\"\"Analyze cumulative effects of optimizations.\"\"\"\n",
    "        results = [{\n",
    "            'step': 'Baseline',\n",
    "            'params': base_params,\n",
    "            'latency_ms': base_latency,\n",
    "            'speedup': 1.0,\n",
    "            'compression': 1.0,\n",
    "            'quality': 100.0\n",
    "        }]\n",
    "        \n",
    "        current_params = base_params\n",
    "        current_latency = base_latency\n",
    "        current_quality = 100.0\n",
    "        cumulative_speedup = 1.0\n",
    "        cumulative_compression = 1.0\n",
    "        \n",
    "        for opt in self.optimization_steps:\n",
    "            current_params /= opt['compression']\n",
    "            current_latency /= opt['speedup']\n",
    "            current_quality *= opt['quality_impact']\n",
    "            cumulative_speedup *= opt['speedup']\n",
    "            cumulative_compression *= opt['compression']\n",
    "            \n",
    "            results.append({\n",
    "                'step': opt['name'],\n",
    "                'params': current_params,\n",
    "                'latency_ms': current_latency,\n",
    "                'speedup': cumulative_speedup,\n",
    "                'compression': cumulative_compression,\n",
    "                'quality': current_quality\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "# Create optimization pipeline\n",
    "pipeline = OptimizationPipeline(\"BERT-Large\")\n",
    "\n",
    "# Add optimization steps\n",
    "pipeline.add_optimization(\"Pruning (50% sparsity)\", speedup=1.5, compression=1.8, \n",
    "                         quality_impact=0.99)\n",
    "pipeline.add_optimization(\"INT8 Quantization\", speedup=1.8, compression=4.0, \n",
    "                         quality_impact=0.98)\n",
    "pipeline.add_optimization(\"Knowledge Distillation\", speedup=3.0, compression=6.0, \n",
    "                         quality_impact=0.96)\n",
    "pipeline.add_optimization(\"Flash Attention\", speedup=2.0, compression=1.0, \n",
    "                         quality_impact=1.0)\n",
    "pipeline.add_optimization(\"Operator Fusion\", speedup=1.2, compression=1.0, \n",
    "                         quality_impact=1.0)\n",
    "\n",
    "# Analyze pipeline\n",
    "results = pipeline.analyze(base_params=340e6, base_latency=50)\n",
    "\n",
    "print(\"\\n🚀 Optimization Pipeline Analysis\")\n",
    "print(\"=\" * 80)\n",
    "display(results.round(2))\n",
    "\n",
    "# Visualize optimization progression\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Speedup and quality\n",
    "ax1.plot(results['step'], results['speedup'], 'g-o', label='Speedup', markersize=8)\n",
    "ax1_twin = ax1.twinx()\n",
    "ax1_twin.plot(results['step'], results['quality'], 'r-s', label='Quality', markersize=8)\n",
    "\n",
    "ax1.set_xlabel('Optimization Step', fontsize=12)\n",
    "ax1.set_ylabel('Speedup (x)', fontsize=12, color='green')\n",
    "ax1_twin.set_ylabel('Quality (%)', fontsize=12, color='red')\n",
    "ax1.set_title('Optimization Impact on Speed and Quality', fontsize=14)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotations\n",
    "for i, row in results.iterrows():\n",
    "    if i > 0:  # Skip baseline\n",
    "        ax1.annotate(f'{row[\"speedup\"]:.1f}x', \n",
    "                    (i, row['speedup']), \n",
    "                    xytext=(0, 10), textcoords='offset points', \n",
    "                    fontsize=9, ha='center')\n",
    "\n",
    "# Memory and latency\n",
    "ax2.bar(results['step'], results['params'] / 1e6, color='lightblue', alpha=0.7)\n",
    "ax2_twin = ax2.twinx()\n",
    "ax2_twin.plot(results['step'], results['latency_ms'], 'ko-', markersize=8)\n",
    "\n",
    "ax2.set_xlabel('Optimization Step', fontsize=12)\n",
    "ax2.set_ylabel('Model Size (M params)', fontsize=12, color='blue')\n",
    "ax2_twin.set_ylabel('Latency (ms)', fontsize=12)\n",
    "ax2.set_title('Model Size and Latency Progression', fontsize=14)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final summary\n",
    "final = results.iloc[-1]\n",
    "original = results.iloc[0]\n",
    "\n",
    "print(f\"\\n✅ Final Optimization Results:\")\n",
    "print(f\"  Total Speedup: {final['speedup']:.1f}x\")\n",
    "print(f\"  Total Compression: {final['compression']:.1f}x\")\n",
    "print(f\"  Quality Retention: {final['quality']:.1f}%\")\n",
    "print(f\"  Latency: {original['latency_ms']:.1f}ms → {final['latency_ms']:.1f}ms\")\n",
    "print(f\"  Model Size: {original['params']/1e6:.0f}M → {final['params']/1e6:.1f}M params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "Let's summarize the key optimization techniques and provide practical recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimization decision matrix\n",
    "print(\"\\n🎯 Optimization Technique Selection Guide\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "decision_matrix = pd.DataFrame({\n",
    "    'Technique': ['Pruning', 'INT8 Quantization', 'INT4 Quantization', \n",
    "                 'Knowledge Distillation', 'Flash Attention', 'KV Caching'],\n",
    "    'Use When': [\n",
    "        'Model has redundancy, structured pruning preferred',\n",
    "        'Need 4x memory reduction with minimal quality loss',\n",
    "        'Extreme memory constraints, can tolerate 2-5% quality loss',\n",
    "        'Can train smaller model, have teacher model available',\n",
    "        'Long sequences (>1K tokens), memory-bound',\n",
    "        'Autoregressive generation, multiple tokens per sequence'\n",
    "    ],\n",
    "    'Speedup': ['1.5-2x', '1.5-3x', '2-4x', '5-10x', '2-10x', '5-100x'],\n",
    "    'Quality Impact': ['< 1%', '1-2%', '2-5%', '2-10%', '0%', '0%'],\n",
    "    'Implementation': ['Medium', 'Easy', 'Medium', 'Hard', 'Hard', 'Easy']\n",
    "})\n",
    "\n",
    "display(decision_matrix)\n",
    "\n",
    "# Best practices summary\n",
    "print(\"\\n📋 Optimization Best Practices:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "best_practices = [\n",
    "    \"1. **Profile First**: Identify bottlenecks before optimizing\",\n",
    "    \"2. **Combine Techniques**: Use multiple optimizations together\",\n",
    "    \"3. **Measure Quality**: Always validate model quality after optimization\",\n",
    "    \"4. **Hardware-Aware**: Optimize for your specific deployment hardware\",\n",
    "    \"5. **Iterative Approach**: Start with least invasive optimizations\",\n",
    "    \"6. **Monitor Production**: Track latency, throughput, and errors\",\n",
    "    \"7. **Cache Strategically**: Use KV cache for generation workloads\",\n",
    "    \"8. **Batch Dynamically**: Implement dynamic batching for better utilization\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"\\n{practice}\")\n",
    "\n",
    "# Optimization workflow\n",
    "print(\"\\n\\n🔄 Recommended Optimization Workflow:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "workflow = \"\"\"\n",
    "1. Baseline Benchmarking\n",
    "   └─ Profile model performance\n",
    "   └─ Identify bottlenecks\n",
    "   \n",
    "2. Quick Wins\n",
    "   └─ Mixed precision (FP16)\n",
    "   └─ Operator fusion\n",
    "   └─ Batch size optimization\n",
    "   \n",
    "3. Memory Optimization\n",
    "   └─ Quantization (INT8 first)\n",
    "   └─ KV caching\n",
    "   └─ Memory-efficient attention\n",
    "   \n",
    "4. Model Compression\n",
    "   └─ Pruning (if applicable)\n",
    "   └─ Distillation (if feasible)\n",
    "   \n",
    "5. Deployment Optimization\n",
    "   └─ Dynamic batching\n",
    "   └─ Model serving framework\n",
    "   └─ Hardware-specific optimizations\n",
    "   \n",
    "6. Production Monitoring\n",
    "   └─ A/B testing\n",
    "   └─ Performance tracking\n",
    "   └─ Continuous optimization\n",
    "\"\"\"\n",
    "\n",
    "print(workflow)\n",
    "\n",
    "print(\"\\n\\n✅ You're now ready to optimize transformer models for production deployment!\")\n",
    "print(\"Remember: Measure twice, optimize once! 📊\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
