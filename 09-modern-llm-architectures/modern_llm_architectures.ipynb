{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern LLM Architectures\n",
    "\n",
    "This notebook explores cutting-edge LLM architectures including LLaMA, Mixtral (MoE), Flash Attention, and other recent innovations that have pushed the boundaries of what's possible with language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evolution of LLM Architectures\n",
    "\n",
    "Let's visualize how LLM architectures have evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timeline of LLM innovations\n",
    "timeline_data = {\n",
    "    'Year': [2018, 2019, 2020, 2022, 2023, 2023, 2024],\n",
    "    'Model': ['GPT-1', 'GPT-2', 'GPT-3', 'ChatGPT', 'GPT-4', 'LLaMA', 'Mixtral'],\n",
    "    'Parameters': [0.117, 1.5, 175, 175, 1700, 65, 47],  # In billions\n",
    "    'Key Innovation': [\n",
    "        'Unsupervised pre-training',\n",
    "        'Zero-shot transfer',\n",
    "        'In-context learning',\n",
    "        'RLHF at scale',\n",
    "        'Multimodal + MoE',\n",
    "        'Efficient open model',\n",
    "        'Open MoE'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_timeline = pd.DataFrame(timeline_data)\n",
    "\n",
    "# Create figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Plot 1: Parameter count over time\n",
    "ax1.scatter(df_timeline['Year'], df_timeline['Parameters'], \n",
    "           s=np.log10(df_timeline['Parameters'] + 1) * 100, \n",
    "           alpha=0.6, c=range(len(df_timeline)))\n",
    "\n",
    "for i, row in df_timeline.iterrows():\n",
    "    ax1.annotate(row['Model'], (row['Year'], row['Parameters']), \n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Parameters (Billions)', fontsize=12)\n",
    "ax1.set_title('Evolution of LLM Scale', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Key innovations timeline\n",
    "y_positions = range(len(df_timeline))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(df_timeline)))\n",
    "\n",
    "for i, row in df_timeline.iterrows():\n",
    "    ax2.barh(i, 1, left=row['Year']-2018, height=0.8, \n",
    "            color=colors[i], alpha=0.7)\n",
    "    ax2.text(row['Year']-2017.5, i, f\"{row['Model']}: {row['Key Innovation']}\", \n",
    "            va='center', fontsize=10)\n",
    "\n",
    "ax2.set_xlim(0, 7)\n",
    "ax2.set_ylim(-0.5, len(df_timeline)-0.5)\n",
    "ax2.set_xlabel('Years since 2018', fontsize=12)\n",
    "ax2.set_yticks([])\n",
    "ax2.set_title('Key Innovations in LLM Development', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. LLaMA Architecture Deep Dive\n",
    "\n",
    "LLaMA introduced several key innovations that made it more efficient than GPT-3 despite being smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSNorm implementation\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"Root Mean Square Layer Normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.variance_epsilon = eps\n",
    "        \n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "# Compare RMSNorm vs LayerNorm\n",
    "def compare_normalizations():\n",
    "    hidden_size = 512\n",
    "    batch_size = 4\n",
    "    seq_len = 32\n",
    "    \n",
    "    # Create random input\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # Apply normalizations\n",
    "    rmsnorm = RMSNorm(hidden_size)\n",
    "    layernorm = nn.LayerNorm(hidden_size)\n",
    "    \n",
    "    rms_out = rmsnorm(x)\n",
    "    ln_out = layernorm(x)\n",
    "    \n",
    "    # Visualize distributions\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Original distribution\n",
    "    axes[0, 0].hist(x.flatten().numpy(), bins=50, alpha=0.7, color='gray')\n",
    "    axes[0, 0].set_title('Original Distribution')\n",
    "    axes[0, 0].set_ylabel('LayerNorm Row')\n",
    "    \n",
    "    axes[1, 0].hist(x.flatten().numpy(), bins=50, alpha=0.7, color='gray')\n",
    "    axes[1, 0].set_ylabel('RMSNorm Row')\n",
    "    \n",
    "    # After normalization\n",
    "    axes[0, 1].hist(ln_out.flatten().detach().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "    axes[0, 1].set_title('After Normalization')\n",
    "    \n",
    "    axes[1, 1].hist(rms_out.flatten().detach().numpy(), bins=50, alpha=0.7, color='green')\n",
    "    \n",
    "    # Statistics\n",
    "    ln_mean = ln_out.mean(dim=-1).detach()\n",
    "    ln_std = ln_out.std(dim=-1).detach()\n",
    "    rms_mean = rms_out.mean(dim=-1).detach()\n",
    "    rms_std = rms_out.std(dim=-1).detach()\n",
    "    \n",
    "    axes[0, 2].scatter(ln_mean.flatten(), ln_std.flatten(), alpha=0.5)\n",
    "    axes[0, 2].set_title('Mean vs Std')\n",
    "    axes[0, 2].set_xlabel('Mean')\n",
    "    axes[0, 2].set_ylabel('Std')\n",
    "    \n",
    "    axes[1, 2].scatter(rms_mean.flatten(), rms_std.flatten(), alpha=0.5)\n",
    "    axes[1, 2].set_xlabel('Mean')\n",
    "    axes[1, 2].set_ylabel('Std')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key differences:\")\n",
    "    print(f\"LayerNorm - Mean: {ln_out.mean():.4f}, Std: {ln_out.std():.4f}\")\n",
    "    print(f\"RMSNorm - Mean: {rms_out.mean():.4f}, Std: {rms_out.std():.4f}\")\n",
    "    print(\"\\nRMSNorm is ~10% faster as it doesn't compute/subtract mean!\")\n",
    "\n",
    "compare_normalizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotary Position Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rope():\n",
    "    \"\"\"Visualize how RoPE works.\"\"\"\n",
    "    # Simple 2D example for visualization\n",
    "    dim = 2\n",
    "    max_len = 16\n",
    "    base = 10000\n",
    "    \n",
    "    # Compute frequencies\n",
    "    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "    t = torch.arange(max_len).float()\n",
    "    freqs = torch.einsum('i,j->ij', t, inv_freq)\n",
    "    \n",
    "    # Create rotation matrices\n",
    "    cos_m = freqs.cos()\n",
    "    sin_m = freqs.sin()\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot 1: Rotation angles\n",
    "    ax = axes[0, 0]\n",
    "    positions = np.arange(max_len)\n",
    "    angles = freqs.numpy()\n",
    "    ax.plot(positions, angles, marker='o')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Rotation Angle (radians)')\n",
    "    ax.set_title('Rotation Angles by Position')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Visualize rotations in 2D\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    # Original vector\n",
    "    v = np.array([1, 0])\n",
    "    \n",
    "    # Apply rotations for different positions\n",
    "    for pos in [0, 4, 8, 12]:\n",
    "        angle = angles[pos, 0]\n",
    "        cos_a, sin_a = np.cos(angle), np.sin(angle)\n",
    "        \n",
    "        # Rotation matrix\n",
    "        rot_matrix = np.array([[cos_a, -sin_a], [sin_a, cos_a]])\n",
    "        v_rot = rot_matrix @ v\n",
    "        \n",
    "        # Plot\n",
    "        ax.arrow(0, 0, v_rot[0], v_rot[1], \n",
    "                head_width=0.05, head_length=0.05, \n",
    "                fc=plt.cm.viridis(pos/12), ec=plt.cm.viridis(pos/12),\n",
    "                label=f'Pos {pos}')\n",
    "    \n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    ax.set_title('2D Rotation Visualization')\n",
    "    \n",
    "    # Plot 3: Sine and Cosine components\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(positions, cos_m.numpy(), label='cos', marker='o')\n",
    "    ax.plot(positions, sin_m.numpy(), label='sin', marker='s')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('Sine and Cosine Components')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Distance preservation\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Create two vectors at different positions\n",
    "    v1 = torch.tensor([1.0, 0.5])\n",
    "    v2 = torch.tensor([0.7, 0.8])\n",
    "    \n",
    "    distances = []\n",
    "    for pos in range(max_len):\n",
    "        angle = freqs[pos, 0].item()\n",
    "        cos_a, sin_a = torch.cos(angle), torch.sin(angle)\n",
    "        \n",
    "        # Apply rotation to both vectors\n",
    "        v1_rot = torch.tensor([v1[0] * cos_a - v1[1] * sin_a,\n",
    "                              v1[0] * sin_a + v1[1] * cos_a])\n",
    "        v2_rot = torch.tensor([v2[0] * cos_a - v2[1] * sin_a,\n",
    "                              v2[0] * sin_a + v2[1] * cos_a])\n",
    "        \n",
    "        # Compute distance\n",
    "        dist = torch.norm(v1_rot - v2_rot).item()\n",
    "        distances.append(dist)\n",
    "    \n",
    "    ax.plot(positions, distances, marker='o')\n",
    "    ax.axhline(y=torch.norm(v1 - v2).item(), color='r', linestyle='--', \n",
    "              label='Original distance')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Distance between vectors')\n",
    "    ax.set_title('RoPE Preserves Relative Distances')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insights about RoPE:\")\n",
    "    print(\"1. Each position gets a unique rotation angle\")\n",
    "    print(\"2. Relative positions are encoded in the phase difference\")\n",
    "    print(\"3. Distance between vectors is preserved (rotation is orthogonal)\")\n",
    "    print(\"4. No learned parameters - purely deterministic\")\n",
    "\n",
    "visualize_rope()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SwiGLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_activations():\n",
    "    \"\"\"Compare different activation functions used in transformers.\"\"\"\n",
    "    x = torch.linspace(-3, 3, 1000)\n",
    "    \n",
    "    # Different activations\n",
    "    relu = F.relu(x)\n",
    "    gelu = F.gelu(x)\n",
    "    silu = F.silu(x)  # Swish\n",
    "    \n",
    "    # SwiGLU is gated - simulate with two inputs\n",
    "    x1 = x.unsqueeze(1)\n",
    "    x2 = -x.unsqueeze(1)  # Different linear projection\n",
    "    swiglu = F.silu(x1) * x2\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot activations\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(x, relu, label='ReLU', linewidth=2)\n",
    "    ax.plot(x, gelu, label='GELU', linewidth=2)\n",
    "    ax.plot(x, silu, label='SiLU (Swish)', linewidth=2)\n",
    "    ax.set_xlabel('Input')\n",
    "    ax.set_ylabel('Output')\n",
    "    ax.set_title('Standard Activation Functions')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot derivatives\n",
    "    ax = axes[0, 1]\n",
    "    relu_grad = (x > 0).float()\n",
    "    gelu_grad = torch.autograd.grad(gelu.sum(), x, create_graph=True)[0]\n",
    "    silu_grad = torch.autograd.grad(silu.sum(), x, create_graph=True)[0]\n",
    "    \n",
    "    ax.plot(x.detach(), relu_grad, label='ReLU\\'', linewidth=2)\n",
    "    ax.plot(x.detach(), gelu_grad.detach(), label='GELU\\'', linewidth=2)\n",
    "    ax.plot(x.detach(), silu_grad.detach(), label='SiLU\\'', linewidth=2)\n",
    "    ax.set_xlabel('Input')\n",
    "    ax.set_ylabel('Gradient')\n",
    "    ax.set_title('Activation Derivatives')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # SwiGLU visualization\n",
    "    ax = axes[1, 0]\n",
    "    im = ax.imshow(swiglu.T, aspect='auto', origin='lower', \n",
    "                   extent=[-3, 3, -3, 3], cmap='RdBu_r')\n",
    "    ax.set_xlabel('x1 (SiLU input)')\n",
    "    ax.set_ylabel('x2 (Gate input)')\n",
    "    ax.set_title('SwiGLU: SiLU(x1) * x2')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Comparison of FFN architectures\n",
    "    ax = axes[1, 1]\n",
    "    architectures = ['Standard\\n(2 layers)', 'GLU\\n(3 matrices)', 'SwiGLU\\n(3 matrices)']\n",
    "    performance = [1.0, 1.08, 1.12]  # Relative performance\n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    \n",
    "    bars = ax.bar(architectures, performance, color=colors, alpha=0.7)\n",
    "    ax.set_ylabel('Relative Performance')\n",
    "    ax.set_title('FFN Architecture Comparison')\n",
    "    ax.set_ylim(0.9, 1.15)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, perf in zip(bars, performance):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{perf:.2f}x', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"SwiGLU advantages:\")\n",
    "    print(\"1. Smoother gradients than ReLU\")\n",
    "    print(\"2. Gating mechanism allows selective information flow\")\n",
    "    print(\"3. Better performance despite using same parameter count\")\n",
    "\n",
    "compare_activations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixture of Experts (MoE)\n",
    "\n",
    "Let's explore how Mixture of Experts enables massive scale while keeping compute manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMoE(nn.Module):\n",
    "    \"\"\"Simplified Mixture of Experts layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_size=256, num_experts=8, num_experts_per_tok=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_experts_per_tok = num_experts_per_tok\n",
    "        \n",
    "        # Router\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        \n",
    "        # Experts (simple FFN)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size * 4),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_size * 4, hidden_size)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, hidden_size = x.shape\n",
    "        x_flat = x.view(-1, hidden_size)\n",
    "        \n",
    "        # Compute router scores\n",
    "        router_logits = self.gate(x_flat)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        topk_probs, topk_indices = torch.topk(router_probs, self.num_experts_per_tok, dim=-1)\n",
    "        topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)  # Renormalize\n",
    "        \n",
    "        # Process tokens through experts\n",
    "        output = torch.zeros_like(x_flat)\n",
    "        \n",
    "        # Track expert usage for visualization\n",
    "        expert_usage = torch.zeros(self.num_experts)\n",
    "        \n",
    "        for i in range(self.num_experts):\n",
    "            # Find tokens assigned to expert i\n",
    "            expert_mask = (topk_indices == i).any(dim=-1)\n",
    "            expert_usage[i] = expert_mask.float().mean()\n",
    "            \n",
    "            if expert_mask.any():\n",
    "                expert_input = x_flat[expert_mask]\n",
    "                expert_output = self.experts[i](expert_input)\n",
    "                \n",
    "                # Get weights for this expert\n",
    "                weights = topk_probs[expert_mask]\n",
    "                weights = weights[topk_indices[expert_mask] == i].unsqueeze(-1)\n",
    "                \n",
    "                output[expert_mask] += weights * expert_output\n",
    "        \n",
    "        output = output.view(batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        return output, router_probs.view(batch_size, seq_len, -1), expert_usage\n",
    "\n",
    "# Demonstrate MoE\n",
    "def visualize_moe():\n",
    "    hidden_size = 256\n",
    "    batch_size = 2\n",
    "    seq_len = 16\n",
    "    \n",
    "    moe = SimpleMoE(hidden_size=hidden_size, num_experts=8, num_experts_per_tok=2)\n",
    "    \n",
    "    # Create input with different patterns\n",
    "    x = torch.randn(batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # Forward pass\n",
    "    output, router_probs, expert_usage = moe(x)\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Router probabilities heatmap\n",
    "    ax = axes[0, 0]\n",
    "    im = ax.imshow(router_probs[0].detach().numpy(), aspect='auto', cmap='YlOrRd')\n",
    "    ax.set_xlabel('Expert ID')\n",
    "    ax.set_ylabel('Token Position')\n",
    "    ax.set_title('Router Probabilities (Batch 0)')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Plot 2: Expert usage\n",
    "    ax = axes[0, 1]\n",
    "    expert_ids = range(moe.num_experts)\n",
    "    ax.bar(expert_ids, expert_usage.detach().numpy(), alpha=0.7)\n",
    "    ax.axhline(y=1/moe.num_experts, color='r', linestyle='--', \n",
    "              label='Ideal uniform usage')\n",
    "    ax.set_xlabel('Expert ID')\n",
    "    ax.set_ylabel('Fraction of Tokens')\n",
    "    ax.set_title('Expert Usage Distribution')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Plot 3: Top-k expert selection\n",
    "    ax = axes[1, 0]\n",
    "    topk_probs, topk_indices = torch.topk(router_probs[0], k=2, dim=-1)\n",
    "    \n",
    "    # Create visualization of top-k selection\n",
    "    selection_matrix = torch.zeros_like(router_probs[0])\n",
    "    for i in range(seq_len):\n",
    "        for j, idx in enumerate(topk_indices[i]):\n",
    "            selection_matrix[i, idx] = topk_probs[i, j]\n",
    "    \n",
    "    im = ax.imshow(selection_matrix.detach().numpy(), aspect='auto', cmap='Blues')\n",
    "    ax.set_xlabel('Expert ID')\n",
    "    ax.set_ylabel('Token Position')\n",
    "    ax.set_title('Top-2 Expert Selection')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    # Plot 4: MoE vs Dense comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Compute FLOPs\n",
    "    dense_flops = hidden_size * hidden_size * 4 * 2  # Two linear layers\n",
    "    moe_flops = dense_flops / moe.num_experts * moe.num_experts_per_tok\n",
    "    \n",
    "    # Parameters\n",
    "    dense_params = hidden_size * hidden_size * 4 * 2\n",
    "    moe_params = dense_params * moe.num_experts + hidden_size * moe.num_experts  # + router\n",
    "    \n",
    "    categories = ['Parameters', 'FLOPs/token']\n",
    "    dense_values = [dense_params / 1e6, dense_flops / 1e6]\n",
    "    moe_values = [moe_params / 1e6, moe_flops / 1e6]\n",
    "    \n",
    "    x_pos = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x_pos - width/2, dense_values, width, label='Dense', alpha=0.7)\n",
    "    ax.bar(x_pos + width/2, moe_values, width, label='MoE (8 experts, top-2)', alpha=0.7)\n",
    "    \n",
    "    ax.set_ylabel('Millions')\n",
    "    ax.set_title('MoE vs Dense FFN Comparison')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (d, m) in enumerate(zip(dense_values, moe_values)):\n",
    "        ax.text(i - width/2, d + 0.1, f'{d:.1f}M', ha='center')\n",
    "        ax.text(i + width/2, m + 0.1, f'{m:.1f}M', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"MoE advantages:\")\n",
    "    print(f\"1. {moe_params/dense_params:.1f}x more parameters\")\n",
    "    print(f\"2. Only {moe_flops/dense_flops:.1f}x compute per token\")\n",
    "    print(f\"3. Specialization: Different experts can learn different patterns\")\n",
    "\n",
    "visualize_moe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Flash Attention Concepts\n",
    "\n",
    "Flash Attention achieves dramatic speedups by being hardware-aware. Let's understand the key concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flash_attention_concepts():\n",
    "    \"\"\"Visualize the key ideas behind Flash Attention.\"\"\"\n",
    "    \n",
    "    # Memory hierarchy visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Memory hierarchy\n",
    "    ax = axes[0, 0]\n",
    "    memory_levels = ['SRAM\\n(~20MB)', 'HBM\\n(~80GB)', 'CPU RAM\\n(~1TB)']\n",
    "    bandwidth = [19000, 1500, 100]  # GB/s\n",
    "    latency = [1, 100, 10000]  # Relative\n",
    "    \n",
    "    x = np.arange(len(memory_levels))\n",
    "    ax.bar(x, bandwidth, alpha=0.7, color='blue')\n",
    "    ax.set_ylabel('Bandwidth (GB/s)', color='blue')\n",
    "    ax.tick_params(axis='y', labelcolor='blue')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(memory_levels)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(x, latency, 'ro-', markersize=10)\n",
    "    ax2.set_ylabel('Relative Latency', color='red')\n",
    "    ax2.tick_params(axis='y', labelcolor='red')\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    ax.set_title('GPU Memory Hierarchy')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Standard vs Flash Attention IO\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "    standard_io = [s**2 * 4 / 1e6 for s in seq_lengths]  # O(nÂ²) MB\n",
    "    flash_io = [s * 4 / 1e3 for s in seq_lengths]  # O(n) MB\n",
    "    \n",
    "    ax.plot(seq_lengths, standard_io, 'o-', label='Standard Attention', linewidth=2)\n",
    "    ax.plot(seq_lengths, flash_io, 's-', label='Flash Attention', linewidth=2)\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Memory IO (MB)')\n",
    "    ax.set_title('Memory IO Comparison')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Tiling visualization\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    # Create a mock attention matrix\n",
    "    n = 16\n",
    "    attention_matrix = np.random.rand(n, n)\n",
    "    \n",
    "    # Show tiling\n",
    "    block_size = 4\n",
    "    for i in range(0, n, block_size):\n",
    "        for j in range(0, n, block_size):\n",
    "            rect = plt.Rectangle((j-0.5, i-0.5), block_size, block_size, \n",
    "                               fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "    \n",
    "    im = ax.imshow(attention_matrix, cmap='Blues', alpha=0.7)\n",
    "    ax.set_title('Flash Attention Tiling (4x4 blocks)')\n",
    "    ax.set_xlabel('Keys')\n",
    "    ax.set_ylabel('Queries')\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.text(2, 2, 'Block fits\\nin SRAM', ha='center', va='center', \n",
    "           bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "    \n",
    "    # Plot 4: Speedup comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    seq_lengths = [512, 1024, 2048, 4096, 8192, 16384]\n",
    "    speedup = [1.5, 2.1, 3.2, 4.8, 7.2, 10.5]  # Approximate speedups\n",
    "    \n",
    "    ax.plot(seq_lengths, speedup, 'go-', markersize=10, linewidth=2)\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Speedup vs Standard Attention')\n",
    "    ax.set_title('Flash Attention Speedup')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (seq, speed) in enumerate(zip(seq_lengths, speedup)):\n",
    "        if i % 2 == 0:\n",
    "            ax.annotate(f'{speed:.1f}x', (seq, speed), \n",
    "                       xytext=(0, 10), textcoords='offset points', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Flash Attention key innovations:\")\n",
    "    print(\"1. Tiling: Process attention in blocks that fit in fast SRAM\")\n",
    "    print(\"2. Recomputation: Trade compute for memory bandwidth\")\n",
    "    print(\"3. Online softmax: Compute softmax without materializing full matrix\")\n",
    "    print(\"4. IO-aware: Minimize slow HBM accesses\")\n",
    "\n",
    "visualize_flash_attention_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Grouped Query Attention (GQA)\n",
    "\n",
    "GQA reduces memory and compute by sharing key/value heads across multiple query heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention_variants():\n",
    "    \"\"\"Compare MHA, MQA, and GQA architectures.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Common parameters\n",
    "    hidden_size = 1024\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Plot 1: Architecture comparison\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    # Visual representation of heads\n",
    "    head_width = 0.8\n",
    "    head_height = 0.15\n",
    "    \n",
    "    # MHA\n",
    "    y_mha = 2.5\n",
    "    for i in range(num_heads):\n",
    "        # Query heads\n",
    "        rect_q = plt.Rectangle((i, y_mha), head_width, head_height, \n",
    "                              facecolor='blue', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect_q)\n",
    "        # Key/Value heads\n",
    "        rect_kv = plt.Rectangle((i, y_mha - 0.2), head_width, head_height, \n",
    "                               facecolor='green', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect_kv)\n",
    "    ax.text(-0.5, y_mha, 'MHA', fontsize=12, va='center')\n",
    "    \n",
    "    # MQA\n",
    "    y_mqa = 1.5\n",
    "    for i in range(num_heads):\n",
    "        # Query heads\n",
    "        rect_q = plt.Rectangle((i, y_mqa), head_width, head_height, \n",
    "                              facecolor='blue', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect_q)\n",
    "    # Single KV head\n",
    "    rect_kv = plt.Rectangle((3.5, y_mqa - 0.2), head_width, head_height, \n",
    "                           facecolor='red', edgecolor='black', alpha=0.7)\n",
    "    ax.add_patch(rect_kv)\n",
    "    ax.text(-0.5, y_mqa, 'MQA', fontsize=12, va='center')\n",
    "    \n",
    "    # GQA\n",
    "    y_gqa = 0.5\n",
    "    num_kv_heads = 4\n",
    "    for i in range(num_heads):\n",
    "        # Query heads\n",
    "        rect_q = plt.Rectangle((i, y_gqa), head_width, head_height, \n",
    "                              facecolor='blue', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect_q)\n",
    "    for i in range(num_kv_heads):\n",
    "        # KV heads\n",
    "        rect_kv = plt.Rectangle((i*2 + 0.5, y_gqa - 0.2), head_width, head_height, \n",
    "                               facecolor='orange', edgecolor='black', alpha=0.7)\n",
    "        ax.add_patch(rect_kv)\n",
    "    ax.text(-0.5, y_gqa, 'GQA', fontsize=12, va='center')\n",
    "    \n",
    "    ax.set_xlim(-1, num_heads)\n",
    "    ax.set_ylim(0, 3)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Attention Head Architectures')\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='blue', alpha=0.7, label='Query heads'),\n",
    "        Patch(facecolor='green', alpha=0.7, label='KV heads (MHA)'),\n",
    "        Patch(facecolor='red', alpha=0.7, label='KV head (MQA)'),\n",
    "        Patch(facecolor='orange', alpha=0.7, label='KV heads (GQA)')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right')\n",
    "    \n",
    "    # Plot 2: Memory usage comparison\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    seq_lengths = [512, 1024, 2048, 4096, 8192]\n",
    "    \n",
    "    # KV cache size (in MB)\n",
    "    head_dim = hidden_size // num_heads\n",
    "    \n",
    "    mha_memory = [2 * s * num_heads * head_dim * 4 / 1e6 for s in seq_lengths]\n",
    "    mqa_memory = [2 * s * 1 * head_dim * 4 / 1e6 for s in seq_lengths]\n",
    "    gqa_memory = [2 * s * num_kv_heads * head_dim * 4 / 1e6 for s in seq_lengths]\n",
    "    \n",
    "    ax.plot(seq_lengths, mha_memory, 'o-', label='MHA (8 heads)', linewidth=2)\n",
    "    ax.plot(seq_lengths, gqa_memory, 's-', label='GQA (4 KV heads)', linewidth=2)\n",
    "    ax.plot(seq_lengths, mqa_memory, '^-', label='MQA (1 KV head)', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('KV Cache Memory (MB)')\n",
    "    ax.set_title('Memory Usage Comparison')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Quality vs efficiency trade-off\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    methods = ['MHA', 'GQA-4', 'GQA-2', 'MQA']\n",
    "    quality = [100, 99.5, 99, 98.5]  # Relative quality\n",
    "    efficiency = [1, 2, 4, 8]  # Relative efficiency\n",
    "    \n",
    "    scatter = ax.scatter(efficiency, quality, s=200, alpha=0.7, c=range(len(methods)))\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        ax.annotate(method, (efficiency[i], quality[i]), \n",
    "                   xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax.set_xlabel('Relative Efficiency')\n",
    "    ax.set_ylabel('Relative Quality (%)')\n",
    "    ax.set_title('Quality vs Efficiency Trade-off')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Scaling behavior\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    model_sizes = ['7B', '13B', '30B', '65B', '175B']\n",
    "    x_pos = np.arange(len(model_sizes))\n",
    "    \n",
    "    # Relative compute requirements\n",
    "    mha_compute = [1, 1, 1, 1, 1]\n",
    "    gqa_compute = [0.5, 0.5, 0.5, 0.5, 0.5]\n",
    "    mqa_compute = [0.125, 0.125, 0.125, 0.125, 0.125]\n",
    "    \n",
    "    width = 0.25\n",
    "    ax.bar(x_pos - width, mha_compute, width, label='MHA', alpha=0.7)\n",
    "    ax.bar(x_pos, gqa_compute, width, label='GQA', alpha=0.7)\n",
    "    ax.bar(x_pos + width, mqa_compute, width, label='MQA', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Model Size')\n",
    "    ax.set_ylabel('Relative Compute (KV operations)')\n",
    "    ax.set_title('Compute Scaling with Model Size')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(model_sizes)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key insights:\")\n",
    "    print(\"1. GQA offers a sweet spot between quality and efficiency\")\n",
    "    print(\"2. Memory savings are crucial for long sequences\")\n",
    "    print(\"3. Quality degradation is minimal with careful tuning\")\n",
    "    print(\"4. Particularly beneficial for inference (KV cache)\")\n",
    "\n",
    "visualize_attention_variants()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis\n",
    "\n",
    "Let's analyze the performance characteristics of modern LLM architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_llm_performance():\n",
    "    \"\"\"Analyze performance metrics of modern LLMs.\"\"\"\n",
    "    \n",
    "    # Model specifications\n",
    "    models = {\n",
    "        'GPT-3': {'params': 175e9, 'flops': 3.14e23, 'tokens': 300e9},\n",
    "        'LLaMA-7B': {'params': 7e9, 'flops': 2e22, 'tokens': 1e12},\n",
    "        'LLaMA-65B': {'params': 65e9, 'flops': 1.4e23, 'tokens': 1.4e12},\n",
    "        'Mistral-7B': {'params': 7e9, 'flops': 2e22, 'tokens': 8e12},\n",
    "        'Mixtral-8x7B': {'params': 47e9, 'flops': 7e22, 'tokens': 1e12},\n",
    "    }\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot 1: Parameters vs Performance\n",
    "    ax = axes[0, 0]\n",
    "    \n",
    "    names = list(models.keys())\n",
    "    params = [models[m]['params'] / 1e9 for m in names]\n",
    "    # Hypothetical performance scores\n",
    "    performance = [85, 82, 88, 84, 86]\n",
    "    \n",
    "    scatter = ax.scatter(params, performance, s=200, alpha=0.7, c=range(len(names)))\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        ax.annotate(name, (params[i], performance[i]), \n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlabel('Parameters (Billions)')\n",
    "    ax.set_ylabel('Performance Score')\n",
    "    ax.set_title('Model Size vs Performance')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Training efficiency\n",
    "    ax = axes[0, 1]\n",
    "    \n",
    "    flops_per_token = [models[m]['flops'] / models[m]['tokens'] for m in names]\n",
    "    tokens_per_param = [models[m]['tokens'] / models[m]['params'] for m in names]\n",
    "    \n",
    "    ax.scatter(tokens_per_param, flops_per_token, s=200, alpha=0.7, c=range(len(names)))\n",
    "    \n",
    "    for i, name in enumerate(names):\n",
    "        ax.annotate(name, (tokens_per_param[i], flops_per_token[i]), \n",
    "                   xytext=(5, 5), textcoords='offset points', fontsize=9)\n",
    "    \n",
    "    ax.set_xlabel('Tokens per Parameter')\n",
    "    ax.set_ylabel('FLOPs per Token')\n",
    "    ax.set_title('Training Efficiency')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Inference speed comparison\n",
    "    ax = axes[1, 0]\n",
    "    \n",
    "    batch_sizes = [1, 8, 32, 128]\n",
    "    # Hypothetical throughput (tokens/second)\n",
    "    throughput = {\n",
    "        'LLaMA-7B': [50, 350, 1200, 4000],\n",
    "        'Mistral-7B': [55, 380, 1350, 4500],\n",
    "        'Mixtral-8x7B': [30, 200, 700, 2400],\n",
    "    }\n",
    "    \n",
    "    for model, values in throughput.items():\n",
    "        ax.plot(batch_sizes, values, 'o-', label=model, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Batch Size')\n",
    "    ax.set_ylabel('Throughput (tokens/second)')\n",
    "    ax.set_title('Inference Speed Comparison')\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_yscale('log')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Memory requirements\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    # Memory breakdown (GB)\n",
    "    categories = ['Weights', 'Activations\\n(BS=1)', 'KV Cache\\n(2K ctx)', 'Total']\n",
    "    \n",
    "    # For 7B model\n",
    "    weights = 7 * 2  # 7B params * 2 bytes (fp16)\n",
    "    activations = 1  # Rough estimate\n",
    "    kv_cache = 2 * 2048 * 32 * 128 * 2 / 1e9  # 2 * seq * layers * dim * bytes / 1e9\n",
    "    total = weights + activations + kv_cache\n",
    "    \n",
    "    values_7b = [weights, activations, kv_cache, total]\n",
    "    \n",
    "    # For 65B model  \n",
    "    weights_65b = 65 * 2\n",
    "    activations_65b = 8\n",
    "    kv_cache_65b = 2 * 2048 * 80 * 128 * 2 / 1e9\n",
    "    total_65b = weights_65b + activations_65b + kv_cache_65b\n",
    "    \n",
    "    values_65b = [weights_65b, activations_65b, kv_cache_65b, total_65b]\n",
    "    \n",
    "    x = np.arange(len(categories))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar(x - width/2, values_7b, width, label='7B Model', alpha=0.7)\n",
    "    ax.bar(x + width/2, values_65b, width, label='65B Model', alpha=0.7)\n",
    "    \n",
    "    ax.set_ylabel('Memory (GB)')\n",
    "    ax.set_title('Memory Requirements')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(categories)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (v7, v65) in enumerate(zip(values_7b, values_65b)):\n",
    "        ax.text(i - width/2, v7 + 1, f'{v7:.1f}', ha='center')\n",
    "        ax.text(i + width/2, v65 + 1, f'{v65:.1f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Performance insights:\")\n",
    "    print(\"1. Mistral-7B achieves similar performance to larger models through better training\")\n",
    "    print(\"2. MoE models like Mixtral offer more parameters with manageable compute\")\n",
    "    print(\"3. Memory bandwidth is often the bottleneck, not compute\")\n",
    "    print(\"4. Efficient architectures are crucial for deployment\")\n",
    "\n",
    "analyze_llm_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The Future of LLM Architectures\n",
    "\n",
    "### Key Innovations We've Explored:\n",
    "\n",
    "1. **LLaMA Family**:\n",
    "   - RMSNorm for faster normalization\n",
    "   - RoPE for better position encoding\n",
    "   - SwiGLU activation for improved performance\n",
    "   - GQA for efficient inference\n",
    "\n",
    "2. **Mixture of Experts**:\n",
    "   - Sparse computation for massive scale\n",
    "   - Specialization through routing\n",
    "   - Load balancing challenges\n",
    "\n",
    "3. **Flash Attention**:\n",
    "   - Hardware-aware algorithm design\n",
    "   - IO optimization over compute optimization\n",
    "   - Enables longer context lengths\n",
    "\n",
    "4. **Efficiency Techniques**:\n",
    "   - GQA/MQA for memory reduction\n",
    "   - Sliding window attention\n",
    "   - Better training recipes\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "1. **Even Longer Context**: 1M+ token context windows\n",
    "2. **More Efficient MoE**: Better routing, dynamic experts\n",
    "3. **Multimodal Native**: Built-in vision/audio understanding\n",
    "4. **Continual Learning**: Models that can update without retraining\n",
    "5. **Edge Deployment**: Efficient models for local inference\n",
    "\n",
    "The race is on to build models that are not just bigger, but fundamentally better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}