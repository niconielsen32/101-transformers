{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Embeddings\n",
    "\n",
    "Welcome to Topic 6! In this notebook, we'll explore how text is converted into numerical representations that transformers can process. We'll implement various tokenization strategies and understand embeddings in depth.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand different tokenization strategies\n",
    "- Implement BPE (Byte-Pair Encoding) from scratch\n",
    "- Explore word, subword, and character-level tokenization\n",
    "- Master embedding techniques\n",
    "- Learn about positional and segment embeddings\n",
    "- Build a complete tokenization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization Fundamentals\n",
    "\n",
    "Let's start by understanding different levels of tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text\n",
    "text = \"The quick brown fox jumps over the lazy dog. Tokenization is fascinating!\"\n",
    "\n",
    "# 1. Character-level tokenization\n",
    "char_tokens = list(text)\n",
    "print(\"Character-level tokenization:\")\n",
    "print(f\"Number of tokens: {len(char_tokens)}\")\n",
    "print(f\"First 20 tokens: {char_tokens[:20]}\")\n",
    "print()\n",
    "\n",
    "# 2. Word-level tokenization (simple split)\n",
    "word_tokens = text.split()\n",
    "print(\"Word-level tokenization (simple):\")\n",
    "print(f\"Number of tokens: {len(word_tokens)}\")\n",
    "print(f\"Tokens: {word_tokens}\")\n",
    "print()\n",
    "\n",
    "# 3. Word-level tokenization (with punctuation handling)\n",
    "import re\n",
    "word_tokens_punct = re.findall(r'\\b\\w+\\b|[.!?]', text)\n",
    "print(\"Word-level tokenization (with punctuation):\")\n",
    "print(f\"Number of tokens: {len(word_tokens_punct)}\")\n",
    "print(f\"Tokens: {word_tokens_punct}\")\n",
    "print()\n",
    "\n",
    "# Visualization of tokenization levels\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Character-level histogram\n",
    "char_counts = Counter(char_tokens)\n",
    "axes[0].bar(range(len(char_counts)), list(char_counts.values()))\n",
    "axes[0].set_title('Character Token Frequencies')\n",
    "axes[0].set_xlabel('Character Index')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Word-level histogram\n",
    "word_counts = Counter(word_tokens_punct)\n",
    "axes[1].bar(range(len(word_counts)), list(word_counts.values()))\n",
    "axes[1].set_title('Word Token Frequencies')\n",
    "axes[1].set_xlabel('Word Index')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "\n",
    "# Vocabulary sizes comparison\n",
    "vocab_sizes = [\n",
    "    len(set(char_tokens)),\n",
    "    len(set(word_tokens)),\n",
    "    len(set(word_tokens_punct))\n",
    "]\n",
    "axes[2].bar(['Character', 'Word (simple)', 'Word (punct)'], vocab_sizes)\n",
    "axes[2].set_title('Vocabulary Sizes')\n",
    "axes[2].set_ylabel('Unique Tokens')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Simple Tokenizer\n",
    "\n",
    "Let's build a basic word-level tokenizer with vocabulary management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab_size=None):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        \n",
    "        # Special tokens\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.unk_token = '<UNK>'\n",
    "        self.bos_token = '<BOS>'\n",
    "        self.eos_token = '<EOS>'\n",
    "        \n",
    "        # Initialize special tokens\n",
    "        special_tokens = [self.pad_token, self.unk_token, self.bos_token, self.eos_token]\n",
    "        for i, token in enumerate(special_tokens):\n",
    "            self.token_to_id[token] = i\n",
    "            self.id_to_token[i] = token\n",
    "            \n",
    "    def build_vocab(self, texts: List[str]):\n",
    "        \"\"\"Build vocabulary from texts.\"\"\"\n",
    "        # Count token frequencies\n",
    "        token_freq = Counter()\n",
    "        for text in texts:\n",
    "            tokens = self._tokenize(text)\n",
    "            token_freq.update(tokens)\n",
    "            \n",
    "        # Sort by frequency\n",
    "        sorted_tokens = sorted(token_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Add to vocabulary\n",
    "        current_id = len(self.token_to_id)\n",
    "        for token, freq in sorted_tokens:\n",
    "            if self.vocab_size and current_id >= self.vocab_size:\n",
    "                break\n",
    "            if token not in self.token_to_id:\n",
    "                self.token_to_id[token] = current_id\n",
    "                self.id_to_token[current_id] = token\n",
    "                current_id += 1\n",
    "                \n",
    "        print(f\"Vocabulary size: {len(self.token_to_id)}\")\n",
    "        \n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Simple word tokenization.\"\"\"\n",
    "        # Convert to lowercase and split\n",
    "        tokens = re.findall(r'\\b\\w+\\b|[.!?]', text.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
    "        \"\"\"Convert text to token ids.\"\"\"\n",
    "        tokens = self._tokenize(text)\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "            \n",
    "        # Convert to ids\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_id:\n",
    "                ids.append(self.token_to_id[token])\n",
    "            else:\n",
    "                ids.append(self.token_to_id[self.unk_token])\n",
    "                \n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
    "        \"\"\"Convert token ids back to text.\"\"\"\n",
    "        tokens = []\n",
    "        special_tokens = {self.pad_token, self.unk_token, self.bos_token, self.eos_token}\n",
    "        \n",
    "        for id in ids:\n",
    "            if id in self.id_to_token:\n",
    "                token = self.id_to_token[id]\n",
    "                if skip_special_tokens and token in special_tokens:\n",
    "                    continue\n",
    "                tokens.append(token)\n",
    "                \n",
    "        return ' '.join(tokens)\n",
    "\n",
    "# Test the tokenizer\n",
    "sample_texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning is amazing!\",\n",
    "    \"Transformers revolutionized NLP.\",\n",
    "    \"Tokenization is a crucial preprocessing step.\"\n",
    "]\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab_size=50)\n",
    "tokenizer.build_vocab(sample_texts)\n",
    "\n",
    "# Test encoding and decoding\n",
    "test_text = \"Machine learning is fascinating!\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"\\nOriginal text: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "\n",
    "# Show vocabulary\n",
    "print(\"\\nFirst 20 tokens in vocabulary:\")\n",
    "for i in range(min(20, len(tokenizer.id_to_token))):\n",
    "    print(f\"{i}: {tokenizer.id_to_token[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Byte-Pair Encoding (BPE) Implementation\n",
    "\n",
    "Now let's implement BPE, a popular subword tokenization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.word_tokenizer = re.compile(r'\\b\\w+\\b|[.!?]')\n",
    "        self.vocab = {}\n",
    "        self.merges = []\n",
    "        \n",
    "    def _get_word_frequencies(self, texts: List[str]) -> Dict[str, int]:\n",
    "        \"\"\"Get frequency of each word in the corpus.\"\"\"\n",
    "        word_freq = Counter()\n",
    "        for text in texts:\n",
    "            words = self.word_tokenizer.findall(text.lower())\n",
    "            word_freq.update(words)\n",
    "        return dict(word_freq)\n",
    "    \n",
    "    def _get_pair_frequencies(self, word_splits: Dict[Tuple, int]) -> Counter:\n",
    "        \"\"\"Count frequency of adjacent pairs.\"\"\"\n",
    "        pair_freq = Counter()\n",
    "        for word_tuple, freq in word_splits.items():\n",
    "            for i in range(len(word_tuple) - 1):\n",
    "                pair = (word_tuple[i], word_tuple[i + 1])\n",
    "                pair_freq[pair] += freq\n",
    "        return pair_freq\n",
    "    \n",
    "    def _merge_pair(self, word_splits: Dict[Tuple, int], pair: Tuple[str, str]) -> Dict[Tuple, int]:\n",
    "        \"\"\"Merge the most frequent pair.\"\"\"\n",
    "        new_word_splits = {}\n",
    "        for word_tuple, freq in word_splits.items():\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word_tuple):\n",
    "                if i < len(word_tuple) - 1 and word_tuple[i] == pair[0] and word_tuple[i + 1] == pair[1]:\n",
    "                    new_word.append(pair[0] + pair[1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word_tuple[i])\n",
    "                    i += 1\n",
    "            new_word_splits[tuple(new_word)] = freq\n",
    "        return new_word_splits\n",
    "    \n",
    "    def train(self, texts: List[str]):\n",
    "        \"\"\"Train BPE on texts.\"\"\"\n",
    "        # Get word frequencies\n",
    "        word_freq = self._get_word_frequencies(texts)\n",
    "        \n",
    "        # Initialize with character-level splits\n",
    "        word_splits = {}\n",
    "        for word, freq in word_freq.items():\n",
    "            word_tuple = tuple(word) + ('</w>',)  # Add end-of-word token\n",
    "            word_splits[word_tuple] = freq\n",
    "            \n",
    "        # Initialize vocabulary with characters\n",
    "        vocab_id = 0\n",
    "        for word_tuple in word_splits:\n",
    "            for char in word_tuple:\n",
    "                if char not in self.vocab:\n",
    "                    self.vocab[char] = vocab_id\n",
    "                    vocab_id += 1\n",
    "                    \n",
    "        print(f\"Initial vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "        # Perform merges\n",
    "        progress_bar = tqdm(total=self.vocab_size - len(self.vocab), desc=\"BPE Training\")\n",
    "        \n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            # Get pair frequencies\n",
    "            pair_freq = self._get_pair_frequencies(word_splits)\n",
    "            \n",
    "            if not pair_freq:\n",
    "                break\n",
    "                \n",
    "            # Find most frequent pair\n",
    "            most_frequent_pair = max(pair_freq, key=pair_freq.get)\n",
    "            \n",
    "            # Merge the pair\n",
    "            word_splits = self._merge_pair(word_splits, most_frequent_pair)\n",
    "            \n",
    "            # Add to vocabulary\n",
    "            merged = most_frequent_pair[0] + most_frequent_pair[1]\n",
    "            if merged not in self.vocab:\n",
    "                self.vocab[merged] = vocab_id\n",
    "                vocab_id += 1\n",
    "                self.merges.append(most_frequent_pair)\n",
    "                progress_bar.update(1)\n",
    "                \n",
    "        progress_bar.close()\n",
    "        print(f\"Final vocabulary size: {len(self.vocab)}\")\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text using learned BPE.\"\"\"\n",
    "        words = self.word_tokenizer.findall(text.lower())\n",
    "        tokens = []\n",
    "        \n",
    "        for word in words:\n",
    "            word_tokens = list(word) + ['</w>']\n",
    "            \n",
    "            # Apply merges\n",
    "            for merge in self.merges:\n",
    "                i = 0\n",
    "                while i < len(word_tokens) - 1:\n",
    "                    if word_tokens[i] == merge[0] and word_tokens[i + 1] == merge[1]:\n",
    "                        word_tokens = word_tokens[:i] + [merge[0] + merge[1]] + word_tokens[i + 2:]\n",
    "                    else:\n",
    "                        i += 1\n",
    "                        \n",
    "            tokens.extend(word_tokens)\n",
    "            \n",
    "        return tokens\n",
    "\n",
    "# Train BPE tokenizer\n",
    "corpus = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Machine learning algorithms are powerful.\",\n",
    "    \"Natural language processing is fascinating.\",\n",
    "    \"Transformers have revolutionized NLP.\",\n",
    "    \"Tokenization is important for text processing.\",\n",
    "    \"Subword tokenization helps with rare words.\",\n",
    "    \"BPE is a popular tokenization method.\",\n",
    "    \"Neural networks learn representations.\"\n",
    "]\n",
    "\n",
    "bpe_tokenizer = BPETokenizer(vocab_size=100)\n",
    "bpe_tokenizer.train(corpus)\n",
    "\n",
    "# Test BPE tokenization\n",
    "test_sentences = [\n",
    "    \"Machine learning is amazing.\",\n",
    "    \"Tokenization helps process text.\",\n",
    "    \"Unknown words are handled well.\"\n",
    "]\n",
    "\n",
    "print(\"\\nBPE Tokenization Examples:\")\n",
    "for sentence in test_sentences:\n",
    "    tokens = bpe_tokenizer.tokenize(sentence)\n",
    "    print(f\"\\nText: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "    \n",
    "# Show some learned merges\n",
    "print(\"\\nFirst 20 BPE merges:\")\n",
    "for i, merge in enumerate(bpe_tokenizer.merges[:20]):\n",
    "    print(f\"{i+1}: {merge[0]} + {merge[1]} = {merge[0] + merge[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Embeddings\n",
    "\n",
    "Now let's explore how tokens are converted to dense vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, padding_idx: int = 0):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize embedding weights.\"\"\"\n",
    "        # Xavier uniform initialization\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        \n",
    "        # Set padding embedding to zero\n",
    "        if self.embedding.padding_idx is not None:\n",
    "            with torch.no_grad():\n",
    "                self.embedding.weight[self.embedding.padding_idx].fill_(0)\n",
    "                \n",
    "    def forward(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        return self.embedding(input_ids)\n",
    "\n",
    "# Create embedding layer\n",
    "vocab_size = 1000\n",
    "embedding_dim = 128\n",
    "embedding_layer = EmbeddingLayer(vocab_size, embedding_dim)\n",
    "\n",
    "# Test embeddings\n",
    "token_ids = torch.tensor([[1, 5, 10, 15, 0, 0],  # 0 is padding\n",
    "                         [2, 7, 12, 0, 0, 0]])\n",
    "embeddings = embedding_layer(token_ids)\n",
    "\n",
    "print(f\"Input shape: {token_ids.shape}\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {embedding_dim}\")\n",
    "\n",
    "# Visualize embedding space (2D projection using PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Get embeddings for first 100 tokens\n",
    "token_indices = torch.arange(100)\n",
    "token_embeddings = embedding_layer(token_indices).detach().numpy()\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(token_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6)\n",
    "\n",
    "# Annotate some points\n",
    "for i in range(0, 100, 10):\n",
    "    plt.annotate(str(i), (embeddings_2d[i, 0], embeddings_2d[i, 1]))\n",
    "    \n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title('Token Embeddings Visualization (PCA)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Show embedding statistics\n",
    "print(\"\\nEmbedding Statistics:\")\n",
    "print(f\"Mean: {token_embeddings.mean():.4f}\")\n",
    "print(f\"Std: {token_embeddings.std():.4f}\")\n",
    "print(f\"Min: {token_embeddings.min():.4f}\")\n",
    "print(f\"Max: {token_embeddings.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Positional and Segment Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbeddings(nn.Module):\n",
    "    \"\"\"Complete embedding layer for transformers including token, position, and segment embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int, max_seq_length: int, \n",
    "                 num_segments: int = 2, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        \n",
    "        # Positional embeddings (learnable)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_length, d_model)\n",
    "        \n",
    "        # Segment embeddings (for BERT-style models)\n",
    "        self.segment_embeddings = nn.Embedding(num_segments, d_model)\n",
    "        \n",
    "        # Layer normalization and dropout\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, input_ids: torch.Tensor, \n",
    "                segment_ids: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        seq_length = input_ids.size(1)\n",
    "        \n",
    "        # Token embeddings\n",
    "        token_embeds = self.token_embeddings(input_ids)\n",
    "        \n",
    "        # Position embeddings\n",
    "        position_ids = torch.arange(seq_length, device=input_ids.device).unsqueeze(0)\n",
    "        position_embeds = self.position_embeddings(position_ids)\n",
    "        \n",
    "        # Segment embeddings\n",
    "        if segment_ids is None:\n",
    "            segment_ids = torch.zeros_like(input_ids)\n",
    "        segment_embeds = self.segment_embeddings(segment_ids)\n",
    "        \n",
    "        # Combine embeddings\n",
    "        embeddings = token_embeds + position_embeds + segment_embeds\n",
    "        \n",
    "        # Apply layer norm and dropout\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test transformer embeddings\n",
    "vocab_size = 1000\n",
    "d_model = 256\n",
    "max_seq_length = 128\n",
    "\n",
    "transformer_embeddings = TransformerEmbeddings(vocab_size, d_model, max_seq_length)\n",
    "\n",
    "# Create sample input\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "input_ids = torch.randint(1, vocab_size, (batch_size, seq_length))\n",
    "segment_ids = torch.cat([torch.zeros(batch_size, 5, dtype=torch.long),\n",
    "                        torch.ones(batch_size, 5, dtype=torch.long)], dim=1)\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = transformer_embeddings(input_ids, segment_ids)\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Output embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Visualize different embedding components\n",
    "with torch.no_grad():\n",
    "    # Get individual components\n",
    "    token_only = transformer_embeddings.token_embeddings(input_ids[0])\n",
    "    position_ids = torch.arange(seq_length)\n",
    "    position_only = transformer_embeddings.position_embeddings(position_ids)\n",
    "    segment_only = transformer_embeddings.segment_embeddings(segment_ids[0])\n",
    "\n",
    "# Plot embedding magnitudes\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Token embeddings\n",
    "axes[0].imshow(token_only.numpy().T, aspect='auto', cmap='coolwarm')\n",
    "axes[0].set_title('Token Embeddings')\n",
    "axes[0].set_xlabel('Position')\n",
    "axes[0].set_ylabel('Embedding Dimension')\n",
    "\n",
    "# Position embeddings\n",
    "axes[1].imshow(position_only.numpy().T, aspect='auto', cmap='coolwarm')\n",
    "axes[1].set_title('Position Embeddings')\n",
    "axes[1].set_xlabel('Position')\n",
    "axes[1].set_ylabel('Embedding Dimension')\n",
    "\n",
    "# Segment embeddings\n",
    "axes[2].imshow(segment_only.numpy().T, aspect='auto', cmap='coolwarm')\n",
    "axes[2].set_title('Segment Embeddings')\n",
    "axes[2].set_xlabel('Position')\n",
    "axes[2].set_ylabel('Embedding Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Tokenization: WordPiece\n",
    "\n",
    "Let's implement a simplified version of WordPiece tokenization used in BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordPieceTokenizer:\n",
    "    def __init__(self, vocab: List[str], unk_token: str = '[UNK]', max_word_length: int = 100):\n",
    "        self.vocab = set(vocab)\n",
    "        self.unk_token = unk_token\n",
    "        self.max_word_length = max_word_length\n",
    "        \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        \"\"\"Tokenize text using WordPiece algorithm.\"\"\"\n",
    "        output_tokens = []\n",
    "        \n",
    "        # First, do basic word tokenization\n",
    "        words = text.lower().split()\n",
    "        \n",
    "        for word in words:\n",
    "            if len(word) > self.max_word_length:\n",
    "                output_tokens.append(self.unk_token)\n",
    "                continue\n",
    "                \n",
    "            is_bad = False\n",
    "            sub_tokens = []\n",
    "            start = 0\n",
    "            \n",
    "            while start < len(word):\n",
    "                end = len(word)\n",
    "                cur_substr = None\n",
    "                \n",
    "                while start < end:\n",
    "                    substr = word[start:end]\n",
    "                    if start > 0:\n",
    "                        substr = '##' + substr\n",
    "                        \n",
    "                    if substr in self.vocab:\n",
    "                        cur_substr = substr\n",
    "                        break\n",
    "                        \n",
    "                    end -= 1\n",
    "                    \n",
    "                if cur_substr is None:\n",
    "                    is_bad = True\n",
    "                    break\n",
    "                    \n",
    "                sub_tokens.append(cur_substr)\n",
    "                start = end\n",
    "                \n",
    "            if is_bad:\n",
    "                output_tokens.append(self.unk_token)\n",
    "            else:\n",
    "                output_tokens.extend(sub_tokens)\n",
    "                \n",
    "        return output_tokens\n",
    "\n",
    "# Create a simple vocabulary\n",
    "vocab = [\n",
    "    '[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]',\n",
    "    'the', 'quick', 'brown', 'fox', 'jumps', 'over', 'lazy', 'dog',\n",
    "    'machine', 'learning', 'is', 'amazing', 'transform', '##er', '##s',\n",
    "    'token', '##ization', 'neural', 'network', '##ing',\n",
    "    'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
    "    'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z',\n",
    "    '##a', '##e', '##i', '##o', '##u', '##n', '##t', '##s', '##d', '##r'\n",
    "]\n",
    "\n",
    "wordpiece_tokenizer = WordPieceTokenizer(vocab)\n",
    "\n",
    "# Test WordPiece tokenization\n",
    "test_sentences = [\n",
    "    \"the quick brown fox\",\n",
    "    \"machine learning is amazing\",\n",
    "    \"transformers tokenization\",\n",
    "    \"unknown words like cryptocurrency\"\n",
    "]\n",
    "\n",
    "print(\"WordPiece Tokenization Examples:\")\n",
    "for sentence in test_sentences:\n",
    "    tokens = wordpiece_tokenizer.tokenize(sentence)\n",
    "    print(f\"\\nText: {sentence}\")\n",
    "    print(f\"Tokens: {tokens}\")\n",
    "\n",
    "# Visualize subword splits\n",
    "word = \"tokenization\"\n",
    "tokens = wordpiece_tokenizer.tokenize(word)\n",
    "print(f\"\\nSubword tokenization of '{word}': {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Embedding Analysis and Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embed1: torch.Tensor, embed2: torch.Tensor) -> float:\n",
    "    \"\"\"Calculate cosine similarity between two embeddings.\"\"\"\n",
    "    dot_product = torch.sum(embed1 * embed2)\n",
    "    norm1 = torch.norm(embed1)\n",
    "    norm2 = torch.norm(embed2)\n",
    "    return (dot_product / (norm1 * norm2)).item()\n",
    "\n",
    "# Create embeddings for similarity analysis\n",
    "vocab_size = 100\n",
    "embedding_dim = 64\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Get embeddings for some tokens\n",
    "token_ids = torch.arange(20)\n",
    "embeddings = embedding_layer(token_ids)\n",
    "\n",
    "# Calculate similarity matrix\n",
    "similarity_matrix = torch.zeros(20, 20)\n",
    "for i in range(20):\n",
    "    for j in range(20):\n",
    "        similarity_matrix[i, j] = cosine_similarity(embeddings[i], embeddings[j])\n",
    "\n",
    "# Visualize similarity matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix.numpy(), cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
    "plt.title('Token Embedding Similarity Matrix')\n",
    "plt.xlabel('Token ID')\n",
    "plt.ylabel('Token ID')\n",
    "plt.show()\n",
    "\n",
    "# Find most similar tokens\n",
    "print(\"Most similar token pairs (excluding self-similarity):\")\n",
    "similarity_matrix_no_diag = similarity_matrix.clone()\n",
    "similarity_matrix_no_diag.fill_diagonal_(-1)  # Exclude self-similarity\n",
    "\n",
    "for _ in range(5):\n",
    "    max_sim = similarity_matrix_no_diag.max()\n",
    "    max_idx = similarity_matrix_no_diag.argmax()\n",
    "    i = max_idx // 20\n",
    "    j = max_idx % 20\n",
    "    print(f\"Token {i} and Token {j}: similarity = {max_sim:.4f}\")\n",
    "    similarity_matrix_no_diag[i, j] = -1\n",
    "    similarity_matrix_no_diag[j, i] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Tokenization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizationPipeline:\n",
    "    \"\"\"Complete tokenization pipeline for transformers.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_file: Optional[str] = None, vocab_size: int = 1000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.special_tokens = {\n",
    "            'pad_token': '[PAD]',\n",
    "            'unk_token': '[UNK]',\n",
    "            'cls_token': '[CLS]',\n",
    "            'sep_token': '[SEP]',\n",
    "            'mask_token': '[MASK]'\n",
    "        }\n",
    "        \n",
    "        # Build vocabulary\n",
    "        if vocab_file:\n",
    "            self.load_vocab(vocab_file)\n",
    "        else:\n",
    "            self.build_default_vocab()\n",
    "            \n",
    "    def build_default_vocab(self):\n",
    "        \"\"\"Build a default vocabulary.\"\"\"\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "        \n",
    "        # Add special tokens\n",
    "        for i, (_, token) in enumerate(self.special_tokens.items()):\n",
    "            self.token_to_id[token] = i\n",
    "            self.id_to_token[i] = token\n",
    "            \n",
    "        print(f\"Vocabulary initialized with {len(self.token_to_id)} special tokens\")\n",
    "        \n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"Preprocess text before tokenization.\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def encode(self, text: str, max_length: Optional[int] = None,\n",
    "               truncation: bool = True, padding: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Encode text to model inputs.\"\"\"\n",
    "        # Preprocess\n",
    "        text = self.preprocess_text(text)\n",
    "        \n",
    "        # Simple word tokenization for demo\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Add special tokens\n",
    "        tokens = [self.special_tokens['cls_token']] + tokens + [self.special_tokens['sep_token']]\n",
    "        \n",
    "        # Convert to ids\n",
    "        input_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.token_to_id:\n",
    "                input_ids.append(self.token_to_id[token])\n",
    "            else:\n",
    "                input_ids.append(self.token_to_id[self.special_tokens['unk_token']])\n",
    "                \n",
    "        # Truncation\n",
    "        if max_length and truncation and len(input_ids) > max_length:\n",
    "            input_ids = input_ids[:max_length]\n",
    "            \n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # Padding\n",
    "        if max_length and padding:\n",
    "            padding_length = max_length - len(input_ids)\n",
    "            input_ids = input_ids + [self.token_to_id[self.special_tokens['pad_token']]] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "            \n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask)\n",
    "        }\n",
    "    \n",
    "    def batch_encode(self, texts: List[str], max_length: Optional[int] = None,\n",
    "                    truncation: bool = True, padding: bool = True) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Encode multiple texts.\"\"\"\n",
    "        encoded_batch = [self.encode(text, max_length, truncation, False) for text in texts]\n",
    "        \n",
    "        # Find max length in batch\n",
    "        if padding and not max_length:\n",
    "            max_length = max(len(enc['input_ids']) for enc in encoded_batch)\n",
    "            \n",
    "        # Pad to max length\n",
    "        if padding:\n",
    "            for enc in encoded_batch:\n",
    "                padding_length = max_length - len(enc['input_ids'])\n",
    "                if padding_length > 0:\n",
    "                    enc['input_ids'] = torch.cat([\n",
    "                        enc['input_ids'],\n",
    "                        torch.full((padding_length,), self.token_to_id[self.special_tokens['pad_token']])\n",
    "                    ])\n",
    "                    enc['attention_mask'] = torch.cat([\n",
    "                        enc['attention_mask'],\n",
    "                        torch.zeros(padding_length, dtype=torch.long)\n",
    "                    ])\n",
    "                    \n",
    "        # Stack into batch\n",
    "        return {\n",
    "            'input_ids': torch.stack([enc['input_ids'] for enc in encoded_batch]),\n",
    "            'attention_mask': torch.stack([enc['attention_mask'] for enc in encoded_batch])\n",
    "        }\n",
    "\n",
    "# Test the complete pipeline\n",
    "pipeline = TokenizationPipeline()\n",
    "\n",
    "# Single text encoding\n",
    "text = \"Hello, this is a test sentence!\"\n",
    "encoded = pipeline.encode(text, max_length=15)\n",
    "print(\"Single text encoding:\")\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Input IDs: {encoded['input_ids']}\")\n",
    "print(f\"Attention Mask: {encoded['attention_mask']}\")\n",
    "\n",
    "# Batch encoding\n",
    "texts = [\n",
    "    \"This is the first sentence.\",\n",
    "    \"This is a much longer second sentence that might need truncation.\",\n",
    "    \"Short one.\"\n",
    "]\n",
    "\n",
    "batch_encoded = pipeline.batch_encode(texts, max_length=20)\n",
    "print(\"\\nBatch encoding:\")\n",
    "print(f\"Batch Input IDs shape: {batch_encoded['input_ids'].shape}\")\n",
    "print(f\"Batch Attention Mask shape: {batch_encoded['attention_mask'].shape}\")\n",
    "\n",
    "# Visualize batch\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Input IDs\n",
    "im1 = ax1.imshow(batch_encoded['input_ids'].numpy(), aspect='auto', cmap='viridis')\n",
    "ax1.set_title('Batch Input IDs')\n",
    "ax1.set_xlabel('Token Position')\n",
    "ax1.set_ylabel('Batch Index')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# Attention Mask\n",
    "im2 = ax2.imshow(batch_encoded['attention_mask'].numpy(), aspect='auto', cmap='RdBu')\n",
    "ax2.set_title('Batch Attention Mask')\n",
    "ax2.set_xlabel('Token Position')\n",
    "ax2.set_ylabel('Batch Index')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored tokenization and embeddings in depth:\n",
    "\n",
    "1. **Tokenization Levels**: Character, word, and subword tokenization\n",
    "2. **Simple Tokenizer**: Built a basic word-level tokenizer\n",
    "3. **BPE Implementation**: Implemented Byte-Pair Encoding from scratch\n",
    "4. **WordPiece**: Implemented simplified WordPiece tokenization\n",
    "5. **Embeddings**: Explored token, positional, and segment embeddings\n",
    "6. **Similarity Analysis**: Analyzed embedding relationships\n",
    "7. **Complete Pipeline**: Built a full tokenization pipeline\n",
    "\n",
    "Key takeaways:\n",
    "- Tokenization is crucial for converting text to numerical form\n",
    "- Subword tokenization balances vocabulary size and coverage\n",
    "- Embeddings provide dense representations of discrete tokens\n",
    "- Position and segment embeddings add structural information\n",
    "- A good tokenization pipeline handles various edge cases\n",
    "\n",
    "Next, we'll explore how to train transformers effectively!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}