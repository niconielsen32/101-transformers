{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Transformers\n",
    "\n",
    "This notebook provides an interactive guide to training transformer models effectively, covering optimization strategies, learning rate schedules, and debugging techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from typing import Dict, List, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Training Process\n",
    "\n",
    "Let's start by building a simple transformer model to demonstrate training concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTransformer(nn.Module):\n",
    "    \"\"\"A simplified transformer for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size=1000, d_model=128, nhead=8, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 512, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_feedforward=512, dropout=0.1\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.output_projection = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Embed and add positional encoding\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Transformer blocks\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch, features)\n",
    "        x = self.transformer(x, src_key_padding_mask=mask)\n",
    "        x = x.transpose(0, 1)  # (batch, seq_len, features)\n",
    "        \n",
    "        # Output projection\n",
    "        return self.output_projection(x)\n",
    "\n",
    "# Create model\n",
    "model = SimpleTransformer().to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Loading\n",
    "\n",
    "Efficient data loading is crucial for training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"Simple dataset for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=1000, seq_length=50, vocab_size=1000):\n",
    "        self.data = torch.randint(1, vocab_size, (num_samples, seq_length))\n",
    "        self.targets = torch.randint(1, vocab_size, (num_samples, seq_length))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.data[idx],\n",
    "            'labels': self.targets[idx]\n",
    "        }\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SimpleDataset(num_samples=1000)\n",
    "val_dataset = SimpleDataset(num_samples=200)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Learning Rate Schedules\n",
    "\n",
    "Let's visualize different learning rate schedules commonly used for transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Linear warmup and linear decay.\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return max(0.0, float(num_training_steps - current_step) / \n",
    "                   float(max(1, num_training_steps - num_warmup_steps)))\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "    \"\"\"Cosine learning rate schedule with warmup.\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        progress = float(current_step - num_warmup_steps) / float(\n",
    "            max(1, num_training_steps - num_warmup_steps))\n",
    "        return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "def get_inverse_sqrt_schedule(optimizer, num_warmup_steps):\n",
    "    \"\"\"Inverse square root schedule (original Transformer).\"\"\"\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        return float(num_warmup_steps) ** 0.5 / float(max(current_step, 1)) ** 0.5\n",
    "    return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Visualize schedules\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Create dummy optimizer\n",
    "dummy_model = nn.Linear(10, 10)\n",
    "lr = 1e-3\n",
    "warmup_steps = 100\n",
    "total_steps = 1000\n",
    "\n",
    "schedules = [\n",
    "    ('Linear', get_linear_schedule_with_warmup),\n",
    "    ('Cosine', get_cosine_schedule_with_warmup),\n",
    "    ('Inverse Sqrt', lambda opt, warm, total: get_inverse_sqrt_schedule(opt, warm))\n",
    "]\n",
    "\n",
    "for ax, (name, schedule_fn) in zip(axes, schedules):\n",
    "    optimizer = torch.optim.Adam(dummy_model.parameters(), lr=lr)\n",
    "    \n",
    "    if name == 'Inverse Sqrt':\n",
    "        scheduler = schedule_fn(optimizer, warmup_steps, total_steps)\n",
    "    else:\n",
    "        scheduler = schedule_fn(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    lrs = []\n",
    "    for step in range(total_steps):\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "    \n",
    "    ax.plot(lrs)\n",
    "    ax.set_title(f'{name} Schedule')\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Learning Rate')\n",
    "    ax.axvline(x=warmup_steps, color='r', linestyle='--', alpha=0.5, label='Warmup End')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizer Configuration\n",
    "\n",
    "AdamW is the standard optimizer for transformers. Let's explore its configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_optimizer(model, learning_rate=3e-4, weight_decay=0.01):\n",
    "    \"\"\"Configure AdamW optimizer with proper weight decay.\"\"\"\n",
    "    \n",
    "    # Separate parameters that should and shouldn't have weight decay\n",
    "    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                      if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            'weight_decay': weight_decay,\n",
    "        },\n",
    "        {\n",
    "            'params': [p for n, p in model.named_parameters() \n",
    "                      if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "            'weight_decay': 0.0,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    return optimizer\n",
    "\n",
    "# Configure optimizer\n",
    "optimizer = configure_optimizer(model)\n",
    "\n",
    "# Print parameter groups\n",
    "for i, group in enumerate(optimizer.param_groups):\n",
    "    print(f\"Group {i}: {len(group['params'])} parameters, weight_decay={group['weight_decay']}\")\n",
    "\n",
    "# Count parameters by type\n",
    "param_counts = defaultdict(int)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        param_counts['bias'] += param.numel()\n",
    "    elif 'LayerNorm' in name or 'layer_norm' in name:\n",
    "        param_counts['layer_norm'] += param.numel()\n",
    "    else:\n",
    "        param_counts['other'] += param.numel()\n",
    "\n",
    "print(\"\\nParameter distribution:\")\n",
    "for ptype, count in param_counts.items():\n",
    "    print(f\"  {ptype}: {count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop with Gradient Clipping\n",
    "\n",
    "Let's implement a training loop with proper gradient clipping and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, batch, optimizer, clip_value=1.0):\n",
    "    \"\"\"Single training step with gradient clipping.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    outputs = model(input_ids)\n",
    "    loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Gradient clipping\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), grad_norm.item()\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    \"\"\"Evaluate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids)\n",
    "            loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            \n",
    "            total_loss += loss.item() * input_ids.size(0) * input_ids.size(1)\n",
    "            total_tokens += input_ids.size(0) * input_ids.size(1)\n",
    "    \n",
    "    return total_loss / total_tokens\n",
    "\n",
    "# Training loop with monitoring\n",
    "num_epochs = 3\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "grad_norms = []\n",
    "learning_rates = []\n",
    "\n",
    "# Setup scheduler\n",
    "total_steps = len(train_loader) * num_epochs\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, \n",
    "                                           num_warmup_steps=100, \n",
    "                                           num_training_steps=total_steps)\n",
    "\n",
    "# Training\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    # Training\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    epoch_losses = []\n",
    "    epoch_grad_norms = []\n",
    "    \n",
    "    for batch in pbar:\n",
    "        loss, grad_norm = train_step(model, batch, optimizer)\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_losses.append(loss)\n",
    "        epoch_grad_norms.append(grad_norm)\n",
    "        learning_rates.append(optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss:.4f}',\n",
    "            'grad_norm': f'{grad_norm:.2f}',\n",
    "            'lr': f'{optimizer.param_groups[0][\"lr\"]:.2e}'\n",
    "        })\n",
    "        \n",
    "        step += 1\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.extend(epoch_losses)\n",
    "    grad_norms.extend(epoch_grad_norms)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss = evaluate(model, val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1} - Train Loss: {np.mean(epoch_losses):.4f}, \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Avg Grad Norm: {np.mean(epoch_grad_norms):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualizing Training Metrics\n",
    "\n",
    "Let's visualize the training progress to identify any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot training loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(train_losses, alpha=0.7)\n",
    "ax.set_title('Training Loss')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot validation loss\n",
    "ax = axes[0, 1]\n",
    "val_steps = np.linspace(0, len(train_losses), len(val_losses))\n",
    "ax.plot(val_steps, val_losses, 'o-', color='red', label='Validation')\n",
    "ax.set_title('Validation Loss')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot gradient norms\n",
    "ax = axes[1, 0]\n",
    "ax.plot(grad_norms, alpha=0.7, color='green')\n",
    "ax.axhline(y=1.0, color='r', linestyle='--', label='Clip threshold')\n",
    "ax.set_title('Gradient Norms')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Gradient Norm')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot learning rate\n",
    "ax = axes[1, 1]\n",
    "ax.plot(learning_rates, color='orange')\n",
    "ax.set_title('Learning Rate Schedule')\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Learning Rate')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Average gradient norm: {np.mean(grad_norms):.2f}\")\n",
    "print(f\"Max gradient norm: {np.max(grad_norms):.2f}\")\n",
    "print(f\"Gradient clipping activated: {sum(g > 1.0 for g in grad_norms)} times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Mixed Precision Training\n",
    "\n",
    "Mixed precision training can significantly speed up training and reduce memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_step_mixed_precision(model, batch, optimizer, scaler, clip_value=1.0):\n",
    "    \"\"\"Training step with mixed precision.\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Mixed precision forward pass\n",
    "    with autocast():\n",
    "        outputs = model(input_ids)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "    \n",
    "    # Scaled backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    \n",
    "    # Unscale gradients for clipping\n",
    "    scaler.unscale_(optimizer)\n",
    "    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "    \n",
    "    # Optimizer step with scaling\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    \n",
    "    return loss.item(), grad_norm.item()\n",
    "\n",
    "# Demonstrate mixed precision\n",
    "if device.type == 'cuda':\n",
    "    print(\"Mixed precision training available!\")\n",
    "    \n",
    "    # Create scaler\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    # Train for a few steps\n",
    "    model_mp = SimpleTransformer().to(device)\n",
    "    optimizer_mp = configure_optimizer(model_mp)\n",
    "    \n",
    "    mp_losses = []\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        if i >= 10:  # Just a few steps for demo\n",
    "            break\n",
    "        loss, grad_norm = train_step_mixed_precision(model_mp, batch, optimizer_mp, scaler)\n",
    "        mp_losses.append(loss)\n",
    "    \n",
    "    print(f\"\\nMixed precision training losses: {mp_losses[:5]}\")\n",
    "    print(f\"Average loss: {np.mean(mp_losses):.4f}\")\n",
    "else:\n",
    "    print(\"Mixed precision requires CUDA device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Gradient Accumulation\n",
    "\n",
    "Gradient accumulation allows training with larger effective batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_gradient_accumulation(model, dataloader, optimizer, accumulation_steps=4):\n",
    "    \"\"\"Training with gradient accumulation.\"\"\"\n",
    "    model.train()\n",
    "    accumulated_loss = 0\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # Scale loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        accumulated_loss += loss.item()\n",
    "        \n",
    "        # Update weights every accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            print(f\"Step {(i + 1) // accumulation_steps}: \"\n",
    "                  f\"Loss = {accumulated_loss:.4f} \"\n",
    "                  f\"(effective batch size = {accumulation_steps * dataloader.batch_size})\")\n",
    "            \n",
    "            accumulated_loss = 0\n",
    "            \n",
    "        if i >= 20:  # Demo only\n",
    "            break\n",
    "\n",
    "# Demonstrate gradient accumulation\n",
    "print(\"Training with gradient accumulation:\")\n",
    "model_ga = SimpleTransformer().to(device)\n",
    "optimizer_ga = configure_optimizer(model_ga)\n",
    "\n",
    "train_with_gradient_accumulation(model_ga, train_loader, optimizer_ga, accumulation_steps=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Monitoring Training Health\n",
    "\n",
    "Let's implement tools to monitor and diagnose training issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gradients(model):\n",
    "    \"\"\"Analyze gradient statistics across layers.\"\"\"\n",
    "    gradient_stats = {}\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            grad = param.grad.data\n",
    "            gradient_stats[name] = {\n",
    "                'mean': grad.mean().item(),\n",
    "                'std': grad.std().item(),\n",
    "                'max': grad.abs().max().item(),\n",
    "                'norm': grad.norm().item(),\n",
    "            }\n",
    "    \n",
    "    return gradient_stats\n",
    "\n",
    "def check_gradient_flow(model):\n",
    "    \"\"\"Check if gradients are flowing properly.\"\"\"\n",
    "    # Forward and backward pass\n",
    "    dummy_input = torch.randint(0, 1000, (2, 10)).to(device)\n",
    "    dummy_target = torch.randint(0, 1000, (2, 10)).to(device)\n",
    "    \n",
    "    output = model(dummy_input)\n",
    "    loss = F.cross_entropy(output.view(-1, output.size(-1)), dummy_target.view(-1))\n",
    "    loss.backward()\n",
    "    \n",
    "    # Analyze gradients\n",
    "    grad_stats = analyze_gradients(model)\n",
    "    \n",
    "    # Visualize gradient norms by layer\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Extract layer names and gradient norms\n",
    "    layer_names = []\n",
    "    grad_norms = []\n",
    "    grad_means = []\n",
    "    \n",
    "    for name, stats in grad_stats.items():\n",
    "        # Simplify layer names\n",
    "        if 'weight' in name:\n",
    "            layer_names.append(name.replace('.weight', ''))\n",
    "            grad_norms.append(stats['norm'])\n",
    "            grad_means.append(abs(stats['mean']))\n",
    "    \n",
    "    # Plot gradient norms\n",
    "    ax1.bar(range(len(grad_norms)), grad_norms)\n",
    "    ax1.set_xlabel('Layer')\n",
    "    ax1.set_ylabel('Gradient Norm')\n",
    "    ax1.set_title('Gradient Norms by Layer')\n",
    "    ax1.set_xticks(range(len(layer_names)))\n",
    "    ax1.set_xticklabels(layer_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Plot gradient means (log scale)\n",
    "    ax2.bar(range(len(grad_means)), grad_means)\n",
    "    ax2.set_xlabel('Layer')\n",
    "    ax2.set_ylabel('|Gradient Mean|')\n",
    "    ax2.set_title('Gradient Means by Layer (abs value)')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xticks(range(len(layer_names)))\n",
    "    ax2.set_xticklabels(layer_names, rotation=45, ha='right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for potential issues\n",
    "    issues = []\n",
    "    for name, stats in grad_stats.items():\n",
    "        if stats['norm'] < 1e-7:\n",
    "            issues.append(f\"Very small gradients in {name}\")\n",
    "        elif stats['norm'] > 100:\n",
    "            issues.append(f\"Very large gradients in {name}\")\n",
    "            \n",
    "    if issues:\n",
    "        print(\"\\nPotential issues detected:\")\n",
    "        for issue in issues:\n",
    "            print(f\"  - {issue}\")\n",
    "    else:\n",
    "        print(\"\\nGradient flow looks healthy!\")\n",
    "    \n",
    "    return grad_stats\n",
    "\n",
    "# Check gradient flow\n",
    "print(\"Analyzing gradient flow...\")\n",
    "grad_stats = check_gradient_flow(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Learning Rate Finder\n",
    "\n",
    "Finding the optimal learning rate is crucial. Let's implement a learning rate finder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_learning_rate(model, dataloader, start_lr=1e-7, end_lr=1, num_iter=100):\n",
    "    \"\"\"Find optimal learning rate using the LR range test.\"\"\"\n",
    "    model_copy = SimpleTransformer().to(device)  # Fresh model\n",
    "    model_copy.load_state_dict(model.state_dict())\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model_copy.parameters(), lr=start_lr)\n",
    "    \n",
    "    lrs = []\n",
    "    losses = []\n",
    "    \n",
    "    # Exponential learning rate schedule\n",
    "    lr_lambda = lambda x: math.exp(x * math.log(end_lr / start_lr) / num_iter)\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    data_iter = iter(dataloader)\n",
    "    smooth_loss = None\n",
    "    \n",
    "    for iteration in range(num_iter):\n",
    "        # Get batch\n",
    "        try:\n",
    "            batch = next(data_iter)\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)\n",
    "            batch = next(data_iter)\n",
    "        \n",
    "        # Training step\n",
    "        model_copy.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model_copy(input_ids)\n",
    "        loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # Smooth the loss\n",
    "        if smooth_loss is None:\n",
    "            smooth_loss = loss.item()\n",
    "        else:\n",
    "            smooth_loss = 0.98 * smooth_loss + 0.02 * loss.item()\n",
    "        \n",
    "        # Record\n",
    "        lrs.append(optimizer.param_groups[0]['lr'])\n",
    "        losses.append(smooth_loss)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Stop if loss explodes\n",
    "        if smooth_loss > 4 * losses[0] or math.isnan(smooth_loss):\n",
    "            break\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Learning Rate')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Learning Rate Finder')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Find suggested LR (steepest descent)\n",
    "    min_grad_idx = np.argmin(np.gradient(losses))\n",
    "    suggested_lr = lrs[min_grad_idx]\n",
    "    plt.axvline(x=suggested_lr, color='r', linestyle='--', \n",
    "                label=f'Suggested LR: {suggested_lr:.2e}')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return lrs, losses, suggested_lr\n",
    "\n",
    "# Find optimal learning rate\n",
    "print(\"Running learning rate finder...\")\n",
    "lrs, losses, suggested_lr = find_learning_rate(model, train_loader)\n",
    "print(f\"\\nSuggested learning rate: {suggested_lr:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Debugging Training Issues\n",
    "\n",
    "Let's create tools to diagnose common training problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDebugger:\n",
    "    \"\"\"Tools for debugging transformer training.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.activation_stats = {}\n",
    "        self.gradient_stats = {}\n",
    "        self.hooks = []\n",
    "        \n",
    "    def register_hooks(self):\n",
    "        \"\"\"Register forward and backward hooks.\"\"\"\n",
    "        def forward_hook(module, input, output):\n",
    "            if isinstance(output, torch.Tensor):\n",
    "                self.activation_stats[module] = {\n",
    "                    'mean': output.mean().item(),\n",
    "                    'std': output.std().item(),\n",
    "                    'max': output.abs().max().item(),\n",
    "                }\n",
    "        \n",
    "        def backward_hook(module, grad_input, grad_output):\n",
    "            if isinstance(grad_output[0], torch.Tensor):\n",
    "                self.gradient_stats[module] = {\n",
    "                    'mean': grad_output[0].mean().item(),\n",
    "                    'std': grad_output[0].std().item(),\n",
    "                    'max': grad_output[0].abs().max().item(),\n",
    "                }\n",
    "        \n",
    "        for module in self.model.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.LayerNorm)):\n",
    "                self.hooks.append(module.register_forward_hook(forward_hook))\n",
    "                self.hooks.append(module.register_backward_hook(backward_hook))\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        \"\"\"Remove all hooks.\"\"\"\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "        self.hooks = []\n",
    "    \n",
    "    def diagnose(self, dataloader, num_batches=5):\n",
    "        \"\"\"Run diagnostic forward and backward passes.\"\"\"\n",
    "        self.register_hooks()\n",
    "        \n",
    "        issues = []\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_batches:\n",
    "                break\n",
    "                \n",
    "            # Forward pass\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = self.model(input_ids)\n",
    "            loss = F.cross_entropy(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            # Check for issues\n",
    "            for module, stats in self.activation_stats.items():\n",
    "                if math.isnan(stats['mean']) or math.isinf(stats['mean']):\n",
    "                    issues.append(f\"NaN/Inf activations in {module.__class__.__name__}\")\n",
    "                elif stats['std'] < 1e-6:\n",
    "                    issues.append(f\"Dead neurons in {module.__class__.__name__} (std={stats['std']:.2e})\")\n",
    "            \n",
    "            for module, stats in self.gradient_stats.items():\n",
    "                if math.isnan(stats['mean']) or math.isinf(stats['mean']):\n",
    "                    issues.append(f\"NaN/Inf gradients in {module.__class__.__name__}\")\n",
    "                elif stats['max'] > 100:\n",
    "                    issues.append(f\"Exploding gradients in {module.__class__.__name__} (max={stats['max']:.2f})\")\n",
    "                elif stats['max'] < 1e-6:\n",
    "                    issues.append(f\"Vanishing gradients in {module.__class__.__name__} (max={stats['max']:.2e})\")\n",
    "            \n",
    "            # Clear gradients\n",
    "            self.model.zero_grad()\n",
    "        \n",
    "        self.remove_hooks()\n",
    "        \n",
    "        return list(set(issues))  # Remove duplicates\n",
    "    \n",
    "    def plot_activation_distribution(self):\n",
    "        \"\"\"Plot activation statistics.\"\"\"\n",
    "        if not self.activation_stats:\n",
    "            print(\"No activation statistics available. Run diagnose() first.\")\n",
    "            return\n",
    "        \n",
    "        layers = []\n",
    "        means = []\n",
    "        stds = []\n",
    "        \n",
    "        for module, stats in self.activation_stats.items():\n",
    "            layers.append(module.__class__.__name__)\n",
    "            means.append(stats['mean'])\n",
    "            stds.append(stats['std'])\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot means\n",
    "        ax1.bar(range(len(means)), means)\n",
    "        ax1.set_xlabel('Layer')\n",
    "        ax1.set_ylabel('Activation Mean')\n",
    "        ax1.set_title('Activation Means by Layer')\n",
    "        ax1.set_xticks(range(len(layers)))\n",
    "        ax1.set_xticklabels(layers, rotation=45, ha='right')\n",
    "        \n",
    "        # Plot stds\n",
    "        ax2.bar(range(len(stds)), stds)\n",
    "        ax2.set_xlabel('Layer')\n",
    "        ax2.set_ylabel('Activation Std')\n",
    "        ax2.set_title('Activation Stds by Layer')\n",
    "        ax2.set_xticks(range(len(layers)))\n",
    "        ax2.set_xticklabels(layers, rotation=45, ha='right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Run diagnostics\n",
    "print(\"Running training diagnostics...\")\n",
    "debugger = TrainingDebugger(model)\n",
    "issues = debugger.diagnose(train_loader)\n",
    "\n",
    "if issues:\n",
    "    print(\"\\nIssues detected:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "else:\n",
    "    print(\"\\nNo issues detected!\")\n",
    "\n",
    "# Plot activation distribution\n",
    "debugger.plot_activation_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Learning Rate Schedule**: Use warmup + cosine/linear decay\n",
    "2. **Optimizer**: AdamW with proper weight decay settings\n",
    "3. **Gradient Clipping**: Essential for stable training (typically 1.0)\n",
    "4. **Mixed Precision**: 2x speedup with minimal accuracy loss\n",
    "5. **Gradient Accumulation**: Simulate larger batch sizes\n",
    "6. **Monitoring**: Track loss, gradients, and learning rate\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "| Issue | Symptoms | Solutions |\n",
    "|-------|----------|----------|\n",
    "| Loss explosion | NaN loss, huge gradients | Lower LR, gradient clipping |\n",
    "| Slow convergence | Loss plateaus | Increase LR, check data |\n",
    "| Overfitting | Train << Val loss | Dropout, weight decay, more data |\n",
    "| Unstable training | Loss spikes | LR warmup, better initialization |\n",
    "\n",
    "### Training Recipe:\n",
    "\n",
    "1. Start with small model/data to debug\n",
    "2. Use learning rate finder\n",
    "3. Monitor gradients and activations\n",
    "4. Scale up gradually\n",
    "5. Save checkpoints frequently"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}