{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms - Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on experience with attention mechanisms - the key innovation behind transformers.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import HTML, display\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Attention Intuitively\n",
    "\n",
    "Let's start with a simple example to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple attention example: Finding relevant words\n",
    "def simple_attention_demo():\n",
    "    # Sentence and word embeddings (simplified)\n",
    "    words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    \n",
    "    # Simple embeddings (2D for visualization)\n",
    "    embeddings = {\n",
    "        \"The\": np.array([0.1, 0.9]),\n",
    "        \"cat\": np.array([0.9, 0.2]),\n",
    "        \"sat\": np.array([0.5, 0.5]),\n",
    "        \"on\": np.array([0.3, 0.7]),\n",
    "        \"mat\": np.array([0.8, 0.3])\n",
    "    }\n",
    "    \n",
    "    # Query: \"What did the cat do?\"\n",
    "    query = np.array([0.7, 0.4])  # Similar to \"cat\" and \"sat\"\n",
    "    \n",
    "    # Compute attention scores (dot product)\n",
    "    scores = []\n",
    "    for word in words:\n",
    "        score = np.dot(query, embeddings[word])\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Convert to probabilities with softmax\n",
    "    scores = np.array(scores)\n",
    "    attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "    \n",
    "    # Visualize\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Plot embeddings\n",
    "    for word in set(words):\n",
    "        emb = embeddings[word]\n",
    "        ax1.scatter(emb[0], emb[1], s=100)\n",
    "        ax1.annotate(word, (emb[0], emb[1]), xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    ax1.scatter(query[0], query[1], s=200, c='red', marker='*')\n",
    "    ax1.annotate('Query', (query[0], query[1]), xytext=(5, 5), textcoords='offset points', color='red')\n",
    "    ax1.set_xlabel('Dimension 1')\n",
    "    ax1.set_ylabel('Dimension 2')\n",
    "    ax1.set_title('Word Embeddings and Query')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot attention weights\n",
    "    bars = ax2.bar(range(len(words)), attention_weights)\n",
    "    ax2.set_xticks(range(len(words)))\n",
    "    ax2.set_xticklabels(words)\n",
    "    ax2.set_ylabel('Attention Weight')\n",
    "    ax2.set_title('Attention Distribution')\n",
    "    \n",
    "    # Color bars by weight\n",
    "    for bar, weight in zip(bars, attention_weights):\n",
    "        bar.set_color(plt.cm.Blues(weight * 2))\n",
    "    \n",
    "    # Add values on bars\n",
    "    for i, (word, weight) in enumerate(zip(words, attention_weights)):\n",
    "        ax2.text(i, weight + 0.01, f'{weight:.3f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(\"Attention weights show which words are most relevant to the query:\")\n",
    "    sorted_idx = np.argsort(attention_weights)[::-1]\n",
    "    for idx in sorted_idx[:3]:\n",
    "        print(f\"  '{words[idx]}': {attention_weights[idx]:.3f}\")\n",
    "\n",
    "simple_attention_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Implementing Attention Step by Step\n",
    "\n",
    "Let's build attention from scratch to understand each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_step_by_step():\n",
    "    print(\"=== Scaled Dot-Product Attention ===\\n\")\n",
    "    \n",
    "    # Example dimensions\n",
    "    seq_len = 4\n",
    "    d_k = 3\n",
    "    \n",
    "    # Random queries, keys, values\n",
    "    Q = np.random.randn(seq_len, d_k)\n",
    "    K = np.random.randn(seq_len, d_k)\n",
    "    V = np.random.randn(seq_len, d_k)\n",
    "    \n",
    "    print(f\"Q shape: {Q.shape} (seq_len × d_k)\")\n",
    "    print(f\"K shape: {K.shape} (seq_len × d_k)\")\n",
    "    print(f\"V shape: {V.shape} (seq_len × d_k)\\n\")\n",
    "    \n",
    "    # Step 1: Compute scores\n",
    "    scores = Q @ K.T\n",
    "    print(\"Step 1: Compute scores = Q @ K^T\")\n",
    "    print(f\"Scores shape: {scores.shape}\")\n",
    "    print(\"Scores matrix:\")\n",
    "    print(scores)\n",
    "    print()\n",
    "    \n",
    "    # Step 2: Scale\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "    print(f\"Step 2: Scale by √d_k = √{d_k} = {np.sqrt(d_k):.3f}\")\n",
    "    print(\"Scaled scores:\")\n",
    "    print(scaled_scores)\n",
    "    print()\n",
    "    \n",
    "    # Step 3: Apply softmax\n",
    "    attention_weights = np.exp(scaled_scores) / np.sum(np.exp(scaled_scores), axis=-1, keepdims=True)\n",
    "    print(\"Step 3: Apply softmax (row-wise)\")\n",
    "    print(\"Attention weights:\")\n",
    "    print(attention_weights)\n",
    "    print(f\"\\nRow sums: {attention_weights.sum(axis=1)} (should all be 1.0)\")\n",
    "    print()\n",
    "    \n",
    "    # Step 4: Apply to values\n",
    "    output = attention_weights @ V\n",
    "    print(\"Step 4: Multiply by values\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(\"Output:\")\n",
    "    print(output)\n",
    "    \n",
    "    # Visualize the process\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    \n",
    "    # Q, K, V matrices\n",
    "    im1 = axes[0, 0].imshow(Q, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 0].set_title('Q (Queries)')\n",
    "    axes[0, 0].set_ylabel('Position')\n",
    "    axes[0, 0].set_xlabel('Dimension')\n",
    "    plt.colorbar(im1, ax=axes[0, 0])\n",
    "    \n",
    "    im2 = axes[0, 1].imshow(K, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 1].set_title('K (Keys)')\n",
    "    axes[0, 1].set_xlabel('Dimension')\n",
    "    plt.colorbar(im2, ax=axes[0, 1])\n",
    "    \n",
    "    im3 = axes[0, 2].imshow(V, cmap='RdBu', aspect='auto')\n",
    "    axes[0, 2].set_title('V (Values)')\n",
    "    axes[0, 2].set_xlabel('Dimension')\n",
    "    plt.colorbar(im3, ax=axes[0, 2])\n",
    "    \n",
    "    # Scores and attention\n",
    "    im4 = axes[1, 0].imshow(scores, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 0].set_title('Scores (Q @ K^T)')\n",
    "    axes[1, 0].set_ylabel('Query position')\n",
    "    axes[1, 0].set_xlabel('Key position')\n",
    "    plt.colorbar(im4, ax=axes[1, 0])\n",
    "    \n",
    "    im5 = axes[1, 1].imshow(attention_weights, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[1, 1].set_title('Attention Weights')\n",
    "    axes[1, 1].set_ylabel('Query position')\n",
    "    axes[1, 1].set_xlabel('Key position')\n",
    "    plt.colorbar(im5, ax=axes[1, 1])\n",
    "    \n",
    "    im6 = axes[1, 2].imshow(output, cmap='RdBu', aspect='auto')\n",
    "    axes[1, 2].set_title('Output')\n",
    "    axes[1, 2].set_ylabel('Position')\n",
    "    axes[1, 2].set_xlabel('Dimension')\n",
    "    plt.colorbar(im6, ax=axes[1, 2])\n",
    "    \n",
    "    plt.suptitle('Attention Computation Visualization', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "attention_step_by_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Self-Attention in Action\n",
    "\n",
    "Let's see how self-attention helps with understanding context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSelfAttention:\n",
    "    def __init__(self, d_model, d_k=None):\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k or d_model\n",
    "        \n",
    "        # Initialize projection matrices\n",
    "        self.W_q = np.random.randn(d_model, self.d_k) * 0.1\n",
    "        self.W_k = np.random.randn(d_model, self.d_k) * 0.1\n",
    "        self.W_v = np.random.randn(d_model, self.d_k) * 0.1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project to Q, K, V\n",
    "        Q = x @ self.W_q\n",
    "        K = x @ self.W_k\n",
    "        V = x @ self.W_v\n",
    "        \n",
    "        # Compute attention\n",
    "        scores = Q @ K.T / np.sqrt(self.d_k)\n",
    "        weights = self.softmax(scores)\n",
    "        output = weights @ V\n",
    "        \n",
    "        return output, weights\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Demonstrate with a sentence\n",
    "def self_attention_sentence_demo():\n",
    "    # Simple sentence\n",
    "    words = [\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n",
    "    \n",
    "    # Create simple embeddings\n",
    "    vocab = list(set(words))\n",
    "    word_to_idx = {w: i for i, w in enumerate(vocab)}\n",
    "    \n",
    "    # One-hot encode and project to embeddings\n",
    "    d_model = 8\n",
    "    embedding_matrix = np.random.randn(len(vocab), d_model)\n",
    "    \n",
    "    # Get embeddings for our sentence\n",
    "    embeddings = np.array([embedding_matrix[word_to_idx[w]] for w in words])\n",
    "    \n",
    "    # Apply self-attention\n",
    "    attention = SimpleSelfAttention(d_model, d_k=4)\n",
    "    output, weights = attention.forward(embeddings)\n",
    "    \n",
    "    # Visualize attention patterns\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(weights, \n",
    "                xticklabels=words,\n",
    "                yticklabels=words,\n",
    "                cmap='Blues',\n",
    "                cbar_kws={'label': 'Attention Weight'},\n",
    "                square=True,\n",
    "                vmin=0,\n",
    "                vmax=1)\n",
    "    plt.title('Self-Attention Pattern')\n",
    "    plt.xlabel('Attending to (Keys)')\n",
    "    plt.ylabel('Position (Queries)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze specific positions\n",
    "    print(\"Attention Analysis:\")\n",
    "    for i, word in enumerate(words):\n",
    "        top_attention_idx = np.argsort(weights[i])[::-1][:3]\n",
    "        print(f\"\\n'{word}' (position {i}) mainly attends to:\")\n",
    "        for idx in top_attention_idx:\n",
    "            print(f\"  - '{words[idx]}' (position {idx}): {weights[i, idx]:.3f}\")\n",
    "\n",
    "self_attention_sentence_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing Attention Types\n",
    "\n",
    "Let's compare different attention mechanisms and see their differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_attention_types():\n",
    "    # Setup\n",
    "    seq_len = 6\n",
    "    d_model = 8\n",
    "    \n",
    "    # Create input\n",
    "    x = np.random.randn(seq_len, d_model)\n",
    "    \n",
    "    # 1. Full Self-Attention\n",
    "    full_attention = SimpleSelfAttention(d_model)\n",
    "    _, weights_full = full_attention.forward(x)\n",
    "    \n",
    "    # 2. Causal (Autoregressive) Attention\n",
    "    weights_causal = weights_full.copy()\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "    weights_causal[mask == 1] = 0\n",
    "    # Renormalize\n",
    "    weights_causal = weights_causal / weights_causal.sum(axis=-1, keepdims=True)\n",
    "    \n",
    "    # 3. Local Attention (window size 3)\n",
    "    weights_local = np.zeros((seq_len, seq_len))\n",
    "    window = 1\n",
    "    for i in range(seq_len):\n",
    "        start = max(0, i - window)\n",
    "        end = min(seq_len, i + window + 1)\n",
    "        weights_local[i, start:end] = weights_full[i, start:end]\n",
    "        weights_local[i] = weights_local[i] / weights_local[i].sum()\n",
    "    \n",
    "    # 4. Strided Attention\n",
    "    weights_strided = np.zeros((seq_len, seq_len))\n",
    "    stride = 2\n",
    "    for i in range(seq_len):\n",
    "        for j in range(0, seq_len, stride):\n",
    "            if j < seq_len:\n",
    "                weights_strided[i, j] = weights_full[i, j]\n",
    "        weights_strided[i] = weights_strided[i] / (weights_strided[i].sum() + 1e-9)\n",
    "    \n",
    "    # Visualize all patterns\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    patterns = [\n",
    "        (weights_full, 'Full Self-Attention', axes[0, 0]),\n",
    "        (weights_causal, 'Causal Attention', axes[0, 1]),\n",
    "        (weights_local, 'Local Attention (window=1)', axes[1, 0]),\n",
    "        (weights_strided, 'Strided Attention (stride=2)', axes[1, 1])\n",
    "    ]\n",
    "    \n",
    "    for weights, title, ax in patterns:\n",
    "        im = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1, aspect='auto')\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        \n",
    "        # Add grid\n",
    "        for i in range(seq_len + 1):\n",
    "            ax.axhline(i - 0.5, color='gray', linewidth=0.5)\n",
    "            ax.axvline(i - 0.5, color='gray', linewidth=0.5)\n",
    "        \n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.suptitle('Different Attention Patterns', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print complexity analysis\n",
    "    print(\"Complexity Analysis:\")\n",
    "    print(f\"Full Attention: O(n²) = O({seq_len}²) = {seq_len**2} operations\")\n",
    "    print(f\"Causal Attention: O(n²/2) ≈ {seq_len**2 // 2} operations\")\n",
    "    print(f\"Local Attention: O(n×w) = O({seq_len}×{2*window+1}) = {seq_len*(2*window+1)} operations\")\n",
    "    print(f\"Strided Attention: O(n²/s) = O({seq_len}²/{stride}) = {seq_len**2 // stride} operations\")\n",
    "\n",
    "compare_attention_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Importance of Scaling\n",
    "\n",
    "Let's see why we scale by √d_k in attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_scaling_importance():\n",
    "    print(\"=== Why Scale by √d_k? ===\\n\")\n",
    "    \n",
    "    # Different dimensions\n",
    "    dimensions = [4, 16, 64, 256]\n",
    "    seq_len = 8\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    \n",
    "    for idx, d_k in enumerate(dimensions):\n",
    "        # Generate random Q and K\n",
    "        Q = np.random.randn(seq_len, d_k)\n",
    "        K = np.random.randn(seq_len, d_k)\n",
    "        \n",
    "        # Compute scores without scaling\n",
    "        scores_unscaled = Q @ K.T\n",
    "        \n",
    "        # Compute scores with scaling\n",
    "        scores_scaled = scores_unscaled / np.sqrt(d_k)\n",
    "        \n",
    "        # Apply softmax\n",
    "        def softmax(x):\n",
    "            exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "            return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "        \n",
    "        weights_unscaled = softmax(scores_unscaled)\n",
    "        weights_scaled = softmax(scores_scaled)\n",
    "        \n",
    "        # Visualize\n",
    "        im1 = axes[0, idx].imshow(weights_unscaled, cmap='Blues', vmin=0, vmax=1)\n",
    "        axes[0, idx].set_title(f'd_k={d_k}\\nUnscaled')\n",
    "        \n",
    "        im2 = axes[1, idx].imshow(weights_scaled, cmap='Blues', vmin=0, vmax=1)\n",
    "        axes[1, idx].set_title(f'Scaled by √{d_k}')\n",
    "        \n",
    "        # Calculate entropy (measure of concentration)\n",
    "        entropy_unscaled = -np.sum(weights_unscaled * np.log(weights_unscaled + 1e-9)) / seq_len\n",
    "        entropy_scaled = -np.sum(weights_scaled * np.log(weights_scaled + 1e-9)) / seq_len\n",
    "        \n",
    "        axes[0, idx].set_xlabel(f'Entropy: {entropy_unscaled:.2f}')\n",
    "        axes[1, idx].set_xlabel(f'Entropy: {entropy_scaled:.2f}')\n",
    "    \n",
    "    plt.suptitle('Effect of Scaling on Attention Distribution', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the mathematical reason\n",
    "    print(\"Mathematical Explanation:\")\n",
    "    print(\"- Dot product variance: Var(q·k) = d_k × Var(q_i) × Var(k_i)\")\n",
    "    print(\"- As d_k increases, dot products grow larger\")\n",
    "    print(\"- Large values → softmax becomes peaked (near one-hot)\")\n",
    "    print(\"- Peaked softmax → vanishing gradients\")\n",
    "    print(\"- Scaling by √d_k keeps variance constant\")\n",
    "    \n",
    "    # Demonstrate gradient issue\n",
    "    print(\"\\nGradient magnitude through softmax:\")\n",
    "    x = np.linspace(-10, 10, 100)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for scale in [0.5, 1.0, 2.0, 5.0]:\n",
    "        y = np.exp(x * scale) / np.sum(np.exp(x * scale))\n",
    "        gradient = y * (1 - y)  # Gradient of softmax\n",
    "        plt.plot(x, gradient, label=f'Scale={scale}')\n",
    "    \n",
    "    plt.xlabel('Input value')\n",
    "    plt.ylabel('Gradient magnitude')\n",
    "    plt.title('Softmax Gradient for Different Scales')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "demonstrate_scaling_importance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Attention as Information Retrieval\n",
    "\n",
    "Let's build an intuitive example showing attention as a soft database lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_as_retrieval():\n",
    "    print(\"=== Attention as Information Retrieval ===\\n\")\n",
    "    \n",
    "    # Create a \"database\" of facts\n",
    "    facts = [\n",
    "        \"Paris is the capital of France\",\n",
    "        \"London is the capital of England\",\n",
    "        \"Cats are animals\",\n",
    "        \"Dogs are animals\",\n",
    "        \"The sun is hot\",\n",
    "        \"Ice is cold\"\n",
    "    ]\n",
    "    \n",
    "    # Simple encoding: average word embeddings\n",
    "    word_embeddings = {\n",
    "        \"Paris\": np.array([0.9, 0.1, 0.0, 0.0]),\n",
    "        \"London\": np.array([0.8, 0.2, 0.0, 0.0]),\n",
    "        \"capital\": np.array([0.7, 0.7, 0.0, 0.0]),\n",
    "        \"France\": np.array([0.9, 0.0, 0.0, 0.0]),\n",
    "        \"England\": np.array([0.8, 0.0, 0.0, 0.0]),\n",
    "        \"cats\": np.array([0.0, 0.0, 0.9, 0.1]),\n",
    "        \"dogs\": np.array([0.0, 0.0, 0.8, 0.2]),\n",
    "        \"animals\": np.array([0.0, 0.0, 0.7, 0.7]),\n",
    "        \"sun\": np.array([0.0, 0.9, 0.0, 0.9]),\n",
    "        \"hot\": np.array([0.0, 0.7, 0.0, 0.9]),\n",
    "        \"ice\": np.array([0.0, 0.1, 0.0, 0.1]),\n",
    "        \"cold\": np.array([0.0, 0.3, 0.0, 0.1]),\n",
    "        # Default for other words\n",
    "        \"default\": np.array([0.1, 0.1, 0.1, 0.1])\n",
    "    }\n",
    "    \n",
    "    # Encode facts (keys and values)\n",
    "    fact_embeddings = []\n",
    "    for fact in facts:\n",
    "        words = fact.lower().split()\n",
    "        embeddings = [word_embeddings.get(w, word_embeddings[\"default\"]) for w in words]\n",
    "        fact_embedding = np.mean(embeddings, axis=0)\n",
    "        fact_embeddings.append(fact_embedding)\n",
    "    \n",
    "    fact_embeddings = np.array(fact_embeddings)\n",
    "    \n",
    "    # Queries\n",
    "    queries = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"Tell me about animals\",\n",
    "        \"Temperature information\"\n",
    "    ]\n",
    "    \n",
    "    # Process each query\n",
    "    fig, axes = plt.subplots(1, len(queries), figsize=(15, 5))\n",
    "    \n",
    "    for idx, (query, ax) in enumerate(zip(queries, axes)):\n",
    "        # Encode query\n",
    "        query_words = query.lower().replace(\"?\", \"\").split()\n",
    "        query_embeddings = [word_embeddings.get(w, word_embeddings[\"default\"]) for w in query_words]\n",
    "        query_vector = np.mean(query_embeddings, axis=0)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = fact_embeddings @ query_vector\n",
    "        attention_weights = np.exp(scores) / np.sum(np.exp(scores))\n",
    "        \n",
    "        # Visualize\n",
    "        bars = ax.barh(range(len(facts)), attention_weights)\n",
    "        ax.set_yticks(range(len(facts)))\n",
    "        ax.set_yticklabels([f[:20] + \"...\" if len(f) > 20 else f for f in facts])\n",
    "        ax.set_xlabel('Attention Weight')\n",
    "        ax.set_title(f'Query: \"{query}\"')\n",
    "        \n",
    "        # Color by weight\n",
    "        for bar, weight in zip(bars, attention_weights):\n",
    "            bar.set_color(plt.cm.Blues(weight * 2))\n",
    "        \n",
    "        # Add values\n",
    "        for i, weight in enumerate(attention_weights):\n",
    "            ax.text(weight + 0.01, i, f'{weight:.3f}', va='center')\n",
    "    \n",
    "    plt.suptitle('Attention as Soft Database Lookup', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Key Insights:\")\n",
    "    print(\"- Attention finds relevant information based on similarity\")\n",
    "    print(\"- Unlike hard lookup, it can combine multiple sources\")\n",
    "    print(\"- Weights show the relevance of each piece of information\")\n",
    "\n",
    "attention_as_retrieval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multi-Head Attention Preview\n",
    "\n",
    "Let's get a preview of why we use multiple attention heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_preview():\n",
    "    print(\"=== Why Multiple Attention Heads? ===\\n\")\n",
    "    \n",
    "    # Sentence for analysis\n",
    "    words = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "    seq_len = len(words)\n",
    "    \n",
    "    # Simulate different attention patterns that different heads might learn\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Head 1: Attending to previous word\n",
    "    head1 = np.zeros((seq_len, seq_len))\n",
    "    for i in range(1, seq_len):\n",
    "        head1[i, i-1] = 0.8\n",
    "        head1[i, i] = 0.2\n",
    "    head1[0, 0] = 1.0\n",
    "    \n",
    "    # Head 2: Attending to next word\n",
    "    head2 = np.zeros((seq_len, seq_len))\n",
    "    for i in range(seq_len-1):\n",
    "        head2[i, i+1] = 0.8\n",
    "        head2[i, i] = 0.2\n",
    "    head2[-1, -1] = 1.0\n",
    "    \n",
    "    # Head 3: Attending to determiners and their nouns\n",
    "    head3 = np.eye(seq_len) * 0.3\n",
    "    # \"The\" -> \"fox\", \"the\" -> \"dog\"\n",
    "    head3[0, 3] = 0.7  # The -> fox\n",
    "    head3[6, 8] = 0.7  # the -> dog\n",
    "    # Adjectives to nouns\n",
    "    head3[1, 3] = 0.5  # quick -> fox\n",
    "    head3[2, 3] = 0.5  # brown -> fox\n",
    "    head3[7, 8] = 0.5  # lazy -> dog\n",
    "    \n",
    "    # Head 4: Global attention (attending to all positions)\n",
    "    head4 = np.ones((seq_len, seq_len)) / seq_len\n",
    "    \n",
    "    heads = [\n",
    "        (head1, \"Head 1: Previous Word\", axes[0, 0]),\n",
    "        (head2, \"Head 2: Next Word\", axes[0, 1]),\n",
    "        (head3, \"Head 3: Syntactic Relations\", axes[1, 0]),\n",
    "        (head4, \"Head 4: Global Context\", axes[1, 1])\n",
    "    ]\n",
    "    \n",
    "    for weights, title, ax in heads:\n",
    "        im = ax.imshow(weights, cmap='Blues', vmin=0, vmax=1)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(words)\n",
    "        ax.set_xlabel('Attending to')\n",
    "        ax.set_ylabel('From position')\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.suptitle('Different Attention Heads Learn Different Patterns', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Benefits of Multiple Heads:\")\n",
    "    print(\"1. Different heads can capture different types of relationships\")\n",
    "    print(\"2. Parallel attention to multiple aspects of the input\")\n",
    "    print(\"3. More expressive power than single attention\")\n",
    "    print(\"4. Robustness - if one head fails, others compensate\")\n",
    "\n",
    "multi_head_preview()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Implementing Efficient Attention\n",
    "\n",
    "Let's implement attention efficiently using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k=None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k or d_model\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, self.d_k, bias=False)\n",
    "        \n",
    "    def forward(self, x, mask=None, return_attention=True):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.W_q(x)\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        if return_attention:\n",
    "            return output, attention_weights\n",
    "        return output\n",
    "\n",
    "# Benchmark and visualize\n",
    "def benchmark_attention():\n",
    "    print(\"=== Efficient Attention Implementation ===\\n\")\n",
    "    \n",
    "    # Test different sequence lengths\n",
    "    seq_lengths = [10, 50, 100, 200, 500]\n",
    "    d_model = 64\n",
    "    batch_size = 32\n",
    "    \n",
    "    times = []\n",
    "    memory_usage = []\n",
    "    \n",
    "    for seq_len in seq_lengths:\n",
    "        # Create model and input\n",
    "        model = EfficientAttention(d_model)\n",
    "        x = torch.randn(batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Warm up\n",
    "        for _ in range(5):\n",
    "            _ = model(x, return_attention=False)\n",
    "        \n",
    "        # Time\n",
    "        import time\n",
    "        start = time.time()\n",
    "        for _ in range(20):\n",
    "            _ = model(x, return_attention=False)\n",
    "        elapsed = (time.time() - start) / 20\n",
    "        times.append(elapsed * 1000)  # Convert to ms\n",
    "        \n",
    "        # Memory (attention matrix size)\n",
    "        memory = batch_size * seq_len * seq_len * 4 / (1024 * 1024)  # MB\n",
    "        memory_usage.append(memory)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    ax1.plot(seq_lengths, times, 'b-o', linewidth=2, markersize=8)\n",
    "    ax1.set_xlabel('Sequence Length')\n",
    "    ax1.set_ylabel('Time (ms)')\n",
    "    ax1.set_title('Attention Computation Time')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2.plot(seq_lengths, memory_usage, 'r-o', linewidth=2, markersize=8)\n",
    "    ax2.set_xlabel('Sequence Length')\n",
    "    ax2.set_ylabel('Memory (MB)')\n",
    "    ax2.set_title('Attention Matrix Memory Usage')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Attention Scalability Analysis', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Complexity: O(n²d) where n=sequence length, d=dimension\")\n",
    "    print(f\"Memory: O(n²) for storing attention weights\")\n",
    "    print(f\"\\nFor seq_len=1000: {1000**2:,} attention values to compute!\")\n",
    "\n",
    "benchmark_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Attention Patterns in Practice\n",
    "\n",
    "Let's visualize some real attention patterns that emerge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_learned_patterns():\n",
    "    print(\"=== Attention Patterns in Practice ===\\n\")\n",
    "    \n",
    "    # Create a more complex example\n",
    "    sentences = [\n",
    "        \"The cat sat on the mat .\",\n",
    "        \"She opened the door carefully .\",\n",
    "        \"Time flies like an arrow .\"\n",
    "    ]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for idx, (sentence, ax) in enumerate(zip(sentences, axes)):\n",
    "        words = sentence.split()\n",
    "        seq_len = len(words)\n",
    "        \n",
    "        # Create mock attention pattern based on linguistic principles\n",
    "        attention = np.random.rand(seq_len, seq_len) * 0.1\n",
    "        \n",
    "        # Add some realistic patterns\n",
    "        for i in range(seq_len):\n",
    "            # Self-attention\n",
    "            attention[i, i] += 0.3\n",
    "            \n",
    "            # Adjacent words\n",
    "            if i > 0:\n",
    "                attention[i, i-1] += 0.2\n",
    "            if i < seq_len - 1:\n",
    "                attention[i, i+1] += 0.2\n",
    "        \n",
    "        # Specific patterns for this sentence\n",
    "        if idx == 0:  # \"The cat sat on the mat\"\n",
    "            attention[2, 1] += 0.3  # sat -> cat\n",
    "            attention[5, 1] += 0.2  # mat -> cat\n",
    "        elif idx == 1:  # \"She opened the door carefully\"\n",
    "            attention[1, 0] += 0.3  # opened -> She\n",
    "            attention[4, 1] += 0.3  # carefully -> opened\n",
    "        elif idx == 2:  # \"Time flies like an arrow\"\n",
    "            attention[1, 0] += 0.4  # flies -> Time\n",
    "            attention[4, 1] += 0.2  # arrow -> flies\n",
    "        \n",
    "        # Normalize\n",
    "        attention = attention / attention.sum(axis=-1, keepdims=True)\n",
    "        \n",
    "        # Visualize\n",
    "        im = ax.imshow(attention, cmap='Blues', vmin=0, vmax=0.5)\n",
    "        ax.set_xticks(range(seq_len))\n",
    "        ax.set_yticks(range(seq_len))\n",
    "        ax.set_xticklabels(words, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(words)\n",
    "        ax.set_title(f'Sentence {idx+1}')\n",
    "        ax.set_xlabel('Attending to')\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel('From position')\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax)\n",
    "    \n",
    "    plt.suptitle('Typical Attention Patterns in Language', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Common patterns observed:\")\n",
    "    print(\"1. Strong self-attention (diagonal)\")\n",
    "    print(\"2. Local attention to nearby words\")\n",
    "    print(\"3. Syntactic dependencies (subject-verb, verb-object)\")\n",
    "    print(\"4. Long-range semantic connections\")\n",
    "\n",
    "analyze_learned_patterns()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Key Takeaways\n",
    "\n",
    "Let's summarize what we've learned about attention mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary():\n",
    "    print(\"=== Attention Mechanisms: Summary ===\\n\")\n",
    "    \n",
    "    # Create a visual summary\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Key concepts\n",
    "    concepts = [\n",
    "        \"Query-Key-Value\\nFramework\",\n",
    "        \"Scaled Dot-Product\\nAttention\",\n",
    "        \"Parallel\\nComputation\",\n",
    "        \"No Information\\nBottleneck\",\n",
    "        \"Content-Based\\nAddressing\",\n",
    "        \"Interpretable\\nWeights\"\n",
    "    ]\n",
    "    \n",
    "    # Position concepts in a circle\n",
    "    n_concepts = len(concepts)\n",
    "    angles = np.linspace(0, 2*np.pi, n_concepts, endpoint=False)\n",
    "    radius = 3\n",
    "    \n",
    "    # Draw concepts\n",
    "    for i, (concept, angle) in enumerate(zip(concepts, angles)):\n",
    "        x = radius * np.cos(angle)\n",
    "        y = radius * np.sin(angle)\n",
    "        \n",
    "        # Draw box\n",
    "        box = plt.Rectangle((x-0.8, y-0.3), 1.6, 0.6, \n",
    "                           facecolor='lightblue', \n",
    "                           edgecolor='darkblue',\n",
    "                           linewidth=2)\n",
    "        ax.add_patch(box)\n",
    "        \n",
    "        # Add text\n",
    "        ax.text(x, y, concept, ha='center', va='center', \n",
    "               fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw center\n",
    "    center_circle = plt.Circle((0, 0), 1.2, facecolor='gold', \n",
    "                              edgecolor='darkorange', linewidth=3)\n",
    "    ax.add_patch(center_circle)\n",
    "    ax.text(0, 0, 'ATTENTION\\nMECHANISM', ha='center', va='center',\n",
    "           fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Draw connections\n",
    "    for angle in angles:\n",
    "        x1 = 1.2 * np.cos(angle)\n",
    "        y1 = 1.2 * np.sin(angle)\n",
    "        x2 = (radius - 0.8) * np.cos(angle)\n",
    "        y2 = (radius - 0.8) * np.sin(angle)\n",
    "        ax.plot([x1, x2], [y1, y2], 'k-', alpha=0.3, linewidth=2)\n",
    "    \n",
    "    ax.set_xlim(-5, 5)\n",
    "    ax.set_ylim(-5, 5)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.title('Attention Mechanisms: Core Concepts', fontsize=16, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print key formulas\n",
    "    print(\"Key Formulas:\\n\")\n",
    "    print(\"1. Attention(Q,K,V) = softmax(QK^T/√d_k)V\")\n",
    "    print(\"2. Q = XW_Q, K = XW_K, V = XW_V\")\n",
    "    print(\"3. MultiHead = Concat(head_1, ..., head_h)W_O\")\n",
    "    print(\"\\nComplexity: O(n²d) time, O(n²) space\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"🎯 You now understand attention mechanisms!\")\n",
    "    print(\"🚀 Next: See how transformers use attention as their core building block\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "create_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Exercises ===\\n\")\n",
    "\n",
    "print(\"1. Implement Masked Attention\")\n",
    "print(\"   Create a function that applies different mask types (causal, padding)\\n\")\n",
    "\n",
    "print(\"2. Temperature Scaling\")\n",
    "print(\"   Modify attention to include a temperature parameter\")\n",
    "print(\"   Observe how it affects the attention distribution\\n\")\n",
    "\n",
    "print(\"3. Relative Position Encoding\")\n",
    "print(\"   Instead of absolute positions, implement attention with relative positions\\n\")\n",
    "\n",
    "print(\"4. Sparse Attention\")\n",
    "print(\"   Implement a pattern where each position only attends to a subset\\n\")\n",
    "\n",
    "print(\"5. Cross-Attention\")\n",
    "print(\"   Implement attention between two different sequences\")\n",
    "print(\"   (e.g., for translation or question-answering)\\n\")\n",
    "\n",
    "# Exercise starter code\n",
    "def exercise_masked_attention():\n",
    "    \"\"\"Exercise 1: Implement different mask types\"\"\"\n",
    "    seq_len = 8\n",
    "    \n",
    "    # TODO: Create causal mask (lower triangular)\n",
    "    causal_mask = None\n",
    "    \n",
    "    # TODO: Create padding mask (mask out positions 6,7)\n",
    "    padding_mask = None\n",
    "    \n",
    "    # TODO: Apply masks to attention computation\n",
    "    pass\n",
    "\n",
    "print(\"\\n💡 These exercises will prepare you for understanding transformers!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}