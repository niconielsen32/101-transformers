{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer Architecture\n",
    "\n",
    "Welcome to Topic 4! In this notebook, we'll explore the complete transformer architecture in detail. We'll understand how all the components work together to create this revolutionary model.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the complete transformer architecture\n",
    "- Learn about encoder and decoder stacks\n",
    "- Master positional encoding\n",
    "- Understand layer normalization and residual connections\n",
    "- See how transformers process sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple\n",
    "import math\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Transformer Architecture Overview\n",
    "\n",
    "Let's start by visualizing the complete transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the transformer architecture\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "# Encoder side\n",
    "ax1.set_title('Transformer Encoder', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 10)\n",
    "\n",
    "# Draw encoder components\n",
    "encoder_components = [\n",
    "    (5, 1, 'Input Embeddings', 'lightblue'),\n",
    "    (5, 2, 'Positional Encoding', 'lightgreen'),\n",
    "    (5, 3.5, 'Multi-Head Attention', 'lightyellow'),\n",
    "    (5, 4.5, 'Add & Norm', 'lightgray'),\n",
    "    (5, 6, 'Feed Forward', 'lightcoral'),\n",
    "    (5, 7, 'Add & Norm', 'lightgray'),\n",
    "    (5, 8.5, 'Output', 'lightblue')\n",
    "]\n",
    "\n",
    "for x, y, label, color in encoder_components:\n",
    "    rect = plt.Rectangle((x-2, y-0.4), 4, 0.8, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.text(x, y, label, ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Draw connections\n",
    "for i in range(len(encoder_components)-1):\n",
    "    y1 = encoder_components[i][1] + 0.4\n",
    "    y2 = encoder_components[i+1][1] - 0.4\n",
    "    ax1.arrow(5, y1, 0, y2-y1, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Add \"Nx\" notation\n",
    "ax1.text(8.5, 5.5, 'Nx', fontsize=14, fontweight='bold', bbox=dict(boxstyle=\"round\", facecolor='white'))\n",
    "\n",
    "ax1.axis('off')\n",
    "\n",
    "# Decoder side\n",
    "ax2.set_title('Transformer Decoder', fontsize=16, fontweight='bold')\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 10)\n",
    "\n",
    "# Draw decoder components\n",
    "decoder_components = [\n",
    "    (5, 0.5, 'Output Embeddings', 'lightblue'),\n",
    "    (5, 1.5, 'Positional Encoding', 'lightgreen'),\n",
    "    (5, 2.5, 'Masked Multi-Head Attention', 'lightyellow'),\n",
    "    (5, 3.3, 'Add & Norm', 'lightgray'),\n",
    "    (5, 4.5, 'Multi-Head Attention', 'lightyellow'),\n",
    "    (5, 5.3, 'Add & Norm', 'lightgray'),\n",
    "    (5, 6.5, 'Feed Forward', 'lightcoral'),\n",
    "    (5, 7.3, 'Add & Norm', 'lightgray'),\n",
    "    (5, 8.5, 'Linear & Softmax', 'lightblue')\n",
    "]\n",
    "\n",
    "for x, y, label, color in decoder_components:\n",
    "    rect = plt.Rectangle((x-2.5, y-0.35), 5, 0.7, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax2.add_patch(rect)\n",
    "    ax2.text(x, y, label, ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Draw connections\n",
    "for i in range(len(decoder_components)-1):\n",
    "    y1 = decoder_components[i][1] + 0.35\n",
    "    y2 = decoder_components[i+1][1] - 0.35\n",
    "    ax2.arrow(5, y1, 0, y2-y1, head_width=0.2, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Add encoder-decoder connection\n",
    "ax2.arrow(1, 4.5, 1.5, 0, head_width=0.2, head_length=0.1, fc='red', ec='red', linestyle='--')\n",
    "ax2.text(1, 4.8, 'From\\nEncoder', ha='center', fontsize=10, color='red')\n",
    "\n",
    "# Add \"Nx\" notation\n",
    "ax2.text(8.5, 5, 'Nx', fontsize=14, fontweight='bold', bbox=dict(boxstyle=\"round\", facecolor='white'))\n",
    "\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Architecture Components:\")\n",
    "print(\"1. Input/Output Embeddings: Convert tokens to vectors\")\n",
    "print(\"2. Positional Encoding: Add position information\")\n",
    "print(\"3. Multi-Head Attention: Core attention mechanism\")\n",
    "print(\"4. Feed Forward: Position-wise transformations\")\n",
    "print(\"5. Add & Norm: Residual connections and layer normalization\")\n",
    "print(\"6. Nx: Stack of N identical layers (typically 6-24)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Positional Encoding\n",
    "\n",
    "Since transformers don't have inherent position awareness, we need to add positional information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding using sine and cosine functions.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_seq_length: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for the sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension and register as buffer\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
    "        # x shape: [batch_size, seq_length, d_model]\n",
    "        seq_length = x.size(1)\n",
    "        x = x + self.pe[:, :seq_length, :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 512\n",
    "max_length = 100\n",
    "pos_encoder = PositionalEncoding(d_model, max_length)\n",
    "\n",
    "# Get positional encoding values\n",
    "pe_values = pos_encoder.pe[0, :max_length, :].numpy()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(pe_values.T, aspect='auto', cmap='RdBu', interpolation='nearest')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding Pattern')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot specific dimensions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "dimensions = [0, 1, 10, 100]\n",
    "\n",
    "for ax, dim in zip(axes.flat, dimensions):\n",
    "    ax.plot(pe_values[:, dim])\n",
    "    ax.set_title(f'Dimension {dim}')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Encoding Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Positional Encoding Properties:\")\n",
    "print(f\"- Each position has a unique encoding vector of dimension {d_model}\")\n",
    "print(\"- Uses sinusoidal functions with different frequencies\")\n",
    "print(\"- Allows the model to learn relative positions\")\n",
    "print(\"- Can extrapolate to longer sequences than seen during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Head Attention Layer\n",
    "\n",
    "Let's implement the complete multi-head attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-Head Attention mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = math.sqrt(self.d_k)\n",
    "        \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Forward pass of multi-head attention.\"\"\"\n",
    "        batch_size = query.size(0)\n",
    "        seq_length = query.size(1)\n",
    "        \n",
    "        # 1. Linear projections in batch from d_model => h x d_k\n",
    "        Q = self.W_q(query).view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. Apply attention on all the projected vectors in batch\n",
    "        attention_output, attention_weights = self.scaled_dot_product_attention(\n",
    "            Q, K, V, mask\n",
    "        )\n",
    "        \n",
    "        # 3. Concatenate heads and put through final linear layer\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_length, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, \n",
    "                                   V: torch.Tensor, mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Scaled dot-product attention.\"\"\"\n",
    "        # Calculate attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
    "        \n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Apply softmax\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "seq_length = 10\n",
    "batch_size = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(batch_size, seq_length, d_model)\n",
    "\n",
    "# Forward pass\n",
    "output, attention_weights = mha(x, x, x)  # Self-attention\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Visualize attention weights for one head\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_weights[0, 0].detach().numpy(), \n",
    "            cmap='Blues', cbar=True, square=True,\n",
    "            xticklabels=range(seq_length),\n",
    "            yticklabels=range(seq_length))\n",
    "plt.xlabel('Keys')\n",
    "plt.ylabel('Queries')\n",
    "plt.title('Attention Weights (Head 0, Batch 0)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feed-Forward Network\n",
    "\n",
    "The position-wise feed-forward network applies the same transformation to each position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"Position-wise feed-forward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass: Linear -> ReLU -> Dropout -> Linear.\"\"\"\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Visualize feed-forward network\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "ffn = FeedForwardNetwork(d_model, d_ff)\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(1, 10, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"FFN Architecture:\")\n",
    "print(f\"Input: {x.shape} -> Linear({d_model}, {d_ff}) -> ReLU -> Dropout -> Linear({d_ff}, {d_model}) -> Output: {output.shape}\")\n",
    "\n",
    "# Visualize the transformation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Input distribution\n",
    "ax1.hist(x.numpy().flatten(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax1.set_title('Input Distribution')\n",
    "ax1.set_xlabel('Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# Output distribution\n",
    "ax2.hist(output.detach().numpy().flatten(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "ax2.set_title('Output Distribution (after FFN)')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFeed-Forward Network Properties:\")\n",
    "print(\"- Applied independently to each position\")\n",
    "print(\"- Two linear transformations with ReLU activation\")\n",
    "print(f\"- Hidden dimension ({d_ff}) is typically 4x the model dimension ({d_model})\")\n",
    "print(\"- Adds non-linearity to the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Layer Normalization and Residual Connections\n",
    "\n",
    "These components are crucial for training deep transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"Layer normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Normalize across the feature dimension.\"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        normalized = (x - mean) / (std + self.eps)\n",
    "        return self.gamma * normalized + self.beta\n",
    "\n",
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"Residual connection with layer normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, sublayer: nn.Module) -> torch.Tensor:\n",
    "        \"\"\"Apply residual connection to any sublayer.\"\"\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "# Demonstrate layer normalization\n",
    "batch_size = 2\n",
    "seq_length = 10\n",
    "d_model = 512\n",
    "\n",
    "# Create sample data with different scales\n",
    "x = torch.randn(batch_size, seq_length, d_model) * 10 + 5\n",
    "\n",
    "# Apply layer normalization\n",
    "layer_norm = LayerNorm(d_model)\n",
    "x_normalized = layer_norm(x)\n",
    "\n",
    "# Visualize the effect\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Before normalization\n",
    "axes[0, 0].hist(x[0, 0].numpy(), bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0, 0].set_title('Before LayerNorm (Position 0)')\n",
    "axes[0, 0].set_xlabel('Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# After normalization\n",
    "axes[0, 1].hist(x_normalized[0, 0].detach().numpy(), bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[0, 1].set_title('After LayerNorm (Position 0)')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# Statistics before\n",
    "axes[1, 0].text(0.1, 0.8, f\"Mean: {x[0, 0].mean():.4f}\", transform=axes[1, 0].transAxes, fontsize=14)\n",
    "axes[1, 0].text(0.1, 0.6, f\"Std: {x[0, 0].std():.4f}\", transform=axes[1, 0].transAxes, fontsize=14)\n",
    "axes[1, 0].text(0.1, 0.4, f\"Min: {x[0, 0].min():.4f}\", transform=axes[1, 0].transAxes, fontsize=14)\n",
    "axes[1, 0].text(0.1, 0.2, f\"Max: {x[0, 0].max():.4f}\", transform=axes[1, 0].transAxes, fontsize=14)\n",
    "axes[1, 0].set_title('Statistics Before')\n",
    "axes[1, 0].axis('off')\n",
    "\n",
    "# Statistics after\n",
    "axes[1, 1].text(0.1, 0.8, f\"Mean: {x_normalized[0, 0].mean():.4f}\", transform=axes[1, 1].transAxes, fontsize=14)\n",
    "axes[1, 1].text(0.1, 0.6, f\"Std: {x_normalized[0, 0].std():.4f}\", transform=axes[1, 1].transAxes, fontsize=14)\n",
    "axes[1, 1].text(0.1, 0.4, f\"Min: {x_normalized[0, 0].min():.4f}\", transform=axes[1, 1].transAxes, fontsize=14)\n",
    "axes[1, 1].text(0.1, 0.2, f\"Max: {x_normalized[0, 0].max():.4f}\", transform=axes[1, 1].transAxes, fontsize=14)\n",
    "axes[1, 1].set_title('Statistics After')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Layer Normalization Benefits:\")\n",
    "print(\"- Stabilizes training of deep networks\")\n",
    "print(\"- Reduces internal covariate shift\")\n",
    "print(\"- Allows higher learning rates\")\n",
    "print(\"- Normalizes across features (not batch)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encoder Layer\n",
    "\n",
    "Now let's combine all components into a complete encoder layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"Single encoder layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through encoder layer.\"\"\"\n",
    "        # Self-attention with residual connection and layer norm\n",
    "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward with residual connection and layer norm\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test encoder layer\n",
    "encoder_layer = EncoderLayer(d_model=512, n_heads=8, d_ff=2048)\n",
    "x = torch.randn(2, 10, 512)  # [batch_size, seq_length, d_model]\n",
    "output = encoder_layer(x)\n",
    "\n",
    "print(f\"Encoder Layer:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in encoder_layer.parameters()):,}\")\n",
    "\n",
    "# Visualize information flow\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Draw encoder layer components\n",
    "components = [\n",
    "    (5, 8, 'Input', 'lightblue'),\n",
    "    (5, 7, 'Multi-Head\\nSelf-Attention', 'lightyellow'),\n",
    "    (5, 6, 'Dropout', 'lightgray'),\n",
    "    (5, 5, 'Add & Norm', 'lightgreen'),\n",
    "    (5, 4, 'Feed Forward', 'lightcoral'),\n",
    "    (5, 3, 'Dropout', 'lightgray'),\n",
    "    (5, 2, 'Add & Norm', 'lightgreen'),\n",
    "    (5, 1, 'Output', 'lightblue')\n",
    "]\n",
    "\n",
    "for x, y, label, color in components:\n",
    "    if label in ['Add & Norm']:\n",
    "        rect = plt.Rectangle((x-1.5, y-0.3), 3, 0.6, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    else:\n",
    "        rect = plt.Rectangle((x-2, y-0.3), 4, 0.6, facecolor=color, edgecolor='black', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y, label, ha='center', va='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Draw connections\n",
    "# Main flow\n",
    "for i in range(len(components)-1):\n",
    "    y1 = components[i][1] - 0.3\n",
    "    y2 = components[i+1][1] + 0.3\n",
    "    ax.arrow(5, y1, 0, y2-y1, head_width=0.15, head_length=0.1, fc='black', ec='black')\n",
    "\n",
    "# Residual connections\n",
    "# First residual\n",
    "ax.arrow(7.5, 7.7, 0, -2.4, head_width=0.15, head_length=0.1, fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "ax.arrow(7.5, 5.3, -2, 0, head_width=0.15, head_length=0.1, fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "\n",
    "# Second residual\n",
    "ax.arrow(7.5, 4.7, 0, -2.4, head_width=0.15, head_length=0.1, fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "ax.arrow(7.5, 2.3, -2, 0, head_width=0.15, head_length=0.1, fc='red', ec='red', linestyle='--', linewidth=2)\n",
    "\n",
    "ax.text(8, 6.5, 'Residual', color='red', fontsize=10, rotation=-90)\n",
    "ax.text(8, 3.5, 'Residual', color='red', fontsize=10, rotation=-90)\n",
    "\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 9)\n",
    "ax.set_title('Encoder Layer Information Flow', fontsize=16, fontweight='bold')\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Decoder Layer\n",
    "\n",
    "The decoder layer is similar but includes masked self-attention and encoder-decoder attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"Single decoder layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = FeedForwardNetwork(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through decoder layer.\"\"\"\n",
    "        # Masked self-attention\n",
    "        attn_output, _ = self.masked_self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Encoder-decoder attention\n",
    "        attn_output, _ = self.encoder_decoder_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create causal mask for decoder\n",
    "def create_causal_mask(seq_length: int) -> torch.Tensor:\n",
    "    \"\"\"Create causal mask to prevent attending to future positions.\"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1)\n",
    "    return mask == 0\n",
    "\n",
    "# Test decoder layer\n",
    "decoder_layer = DecoderLayer(d_model=512, n_heads=8, d_ff=2048)\n",
    "\n",
    "# Sample inputs\n",
    "tgt = torch.randn(2, 10, 512)  # Decoder input\n",
    "memory = torch.randn(2, 15, 512)  # Encoder output\n",
    "tgt_mask = create_causal_mask(10)\n",
    "\n",
    "output = decoder_layer(tgt, memory, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Decoder Layer:\")\n",
    "print(f\"Target input shape: {tgt.shape}\")\n",
    "print(f\"Encoder output shape: {memory.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nNumber of parameters: {sum(p.numel() for p in decoder_layer.parameters()):,}\")\n",
    "\n",
    "# Visualize causal mask\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(tgt_mask.numpy(), cmap='Blues', interpolation='nearest')\n",
    "plt.colorbar(label='Can Attend')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Causal Mask (Prevents Looking at Future Tokens)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCausal Mask Properties:\")\n",
    "print(\"- Each position can only attend to previous positions\")\n",
    "print(\"- Ensures autoregressive property during generation\")\n",
    "print(\"- Upper triangle is masked (set to -infinity before softmax)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Complete Transformer Model\n",
    "\n",
    "Now let's put everything together into a complete transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"Complete Transformer model.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int = 512,\n",
    "                 n_heads: int = 8,\n",
    "                 n_encoder_layers: int = 6,\n",
    "                 n_decoder_layers: int = 6,\n",
    "                 d_ff: int = 2048,\n",
    "                 max_seq_length: int = 5000,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        \"\"\"Initialize parameters with Xavier uniform.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def encode(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Encode source sequence.\"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        x = self.src_embedding(src) * math.sqrt(self.src_embedding.embedding_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Pass through encoder layers\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, src_mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def decode(self, tgt: torch.Tensor, memory: torch.Tensor,\n",
    "               src_mask: Optional[torch.Tensor] = None,\n",
    "               tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Decode target sequence.\"\"\"\n",
    "        # Embed and add positional encoding\n",
    "        x = self.tgt_embedding(tgt) * math.sqrt(self.tgt_embedding.embedding_dim)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        # Pass through decoder layers\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "            \n",
    "        return x\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor,\n",
    "                src_mask: Optional[torch.Tensor] = None,\n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"Forward pass through the transformer.\"\"\"\n",
    "        # Encode source\n",
    "        memory = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode target\n",
    "        output = self.decode(tgt, memory, src_mask, tgt_mask)\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        output = self.output_projection(output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create a small transformer model\n",
    "model = Transformer(\n",
    "    src_vocab_size=1000,\n",
    "    tgt_vocab_size=1000,\n",
    "    d_model=256,\n",
    "    n_heads=4,\n",
    "    n_encoder_layers=2,\n",
    "    n_decoder_layers=2,\n",
    "    d_ff=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "src = torch.randint(0, 1000, (2, 10))  # [batch_size, src_seq_length]\n",
    "tgt = torch.randint(0, 1000, (2, 8))   # [batch_size, tgt_seq_length]\n",
    "tgt_mask = create_causal_mask(8)\n",
    "\n",
    "output = model(src, tgt, tgt_mask=tgt_mask)\n",
    "\n",
    "print(f\"Complete Transformer Model:\")\n",
    "print(f\"Source shape: {src.shape}\")\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Parameter breakdown\n",
    "print(\"\\nParameter Breakdown:\")\n",
    "for name, module in model.named_children():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    print(f\"{name}: {params:,} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference Example\n",
    "\n",
    "Let's see how the transformer generates sequences during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model: Transformer, src: torch.Tensor, max_length: int = 50,\n",
    "                  start_token: int = 1, end_token: int = 2) -> torch.Tensor:\n",
    "    \"\"\"Greedy decoding for sequence generation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode source\n",
    "    memory = model.encode(src)\n",
    "    \n",
    "    # Start with start token\n",
    "    ys = torch.ones(1, 1).fill_(start_token).type_as(src)\n",
    "    \n",
    "    for i in range(max_length - 1):\n",
    "        # Create mask\n",
    "        tgt_mask = create_causal_mask(ys.size(1)).type_as(src)\n",
    "        \n",
    "        # Decode\n",
    "        out = model.decode(ys, memory, tgt_mask=tgt_mask)\n",
    "        \n",
    "        # Project and get next token\n",
    "        prob = model.output_projection(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.item()\n",
    "        \n",
    "        # Append to sequence\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).fill_(next_word).type_as(src)], dim=1)\n",
    "        \n",
    "        # Stop if end token is generated\n",
    "        if next_word == end_token:\n",
    "            break\n",
    "            \n",
    "    return ys\n",
    "\n",
    "# Simulate translation task\n",
    "print(\"Simulating Translation with Greedy Decoding:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create source sequence\n",
    "src_sequence = torch.randint(3, 100, (1, 8))  # Random source tokens\n",
    "print(f\"Source tokens: {src_sequence.tolist()[0]}\")\n",
    "\n",
    "# Generate translation\n",
    "translation = greedy_decode(model, src_sequence, max_length=15)\n",
    "print(f\"Generated tokens: {translation.tolist()[0]}\")\n",
    "print(f\"Generated length: {translation.size(1)}\")\n",
    "\n",
    "# Visualize attention patterns during generation\n",
    "def visualize_generation_attention(model, src, tgt):\n",
    "    \"\"\"Visualize attention during generation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get encoder output\n",
    "    memory = model.encode(src)\n",
    "    \n",
    "    # Get decoder attention weights (simplified - would need to modify model to return these)\n",
    "    tgt_mask = create_causal_mask(tgt.size(1))\n",
    "    \n",
    "    # For visualization, we'll create synthetic attention patterns\n",
    "    seq_len = tgt.size(1)\n",
    "    src_len = src.size(1)\n",
    "    \n",
    "    # Self-attention pattern (causal)\n",
    "    self_attn = torch.zeros(seq_len, seq_len)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(i+1):\n",
    "            self_attn[i, j] = 1.0 / (i + 1)  # Uniform over visible positions\n",
    "            \n",
    "    # Cross-attention pattern (attending to source)\n",
    "    cross_attn = torch.rand(seq_len, src_len)\n",
    "    cross_attn = F.softmax(cross_attn, dim=-1)\n",
    "    \n",
    "    return self_attn, cross_attn\n",
    "\n",
    "# Visualize attention\n",
    "self_attn, cross_attn = visualize_generation_attention(model, src_sequence, translation)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Self-attention\n",
    "sns.heatmap(self_attn.numpy(), cmap='Blues', cbar=True, square=True, ax=ax1)\n",
    "ax1.set_xlabel('Position')\n",
    "ax1.set_ylabel('Position')\n",
    "ax1.set_title('Decoder Self-Attention (Causal)')\n",
    "\n",
    "# Cross-attention\n",
    "sns.heatmap(cross_attn.numpy(), cmap='Reds', cbar=True, ax=ax2)\n",
    "ax2.set_xlabel('Source Position')\n",
    "ax2.set_ylabel('Target Position')\n",
    "ax2.set_title('Encoder-Decoder Cross-Attention')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored the complete transformer architecture:\n",
    "\n",
    "1. **Architecture Overview**: Encoder-decoder structure with self-attention\n",
    "2. **Positional Encoding**: Adding position information with sinusoidal functions\n",
    "3. **Multi-Head Attention**: Parallel attention mechanisms\n",
    "4. **Feed-Forward Networks**: Position-wise transformations\n",
    "5. **Layer Normalization**: Stabilizing deep network training\n",
    "6. **Residual Connections**: Enabling gradient flow\n",
    "7. **Complete Model**: Putting all components together\n",
    "8. **Inference**: Generating sequences with the model\n",
    "\n",
    "The transformer's power comes from:\n",
    "- **Parallelization**: All positions processed simultaneously\n",
    "- **Long-range Dependencies**: Direct connections between all positions\n",
    "- **Scalability**: Can be made deeper and wider\n",
    "- **Flexibility**: Works for many sequence tasks\n",
    "\n",
    "Next, we'll implement a transformer from scratch to deepen our understanding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}