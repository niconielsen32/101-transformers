{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Sequence Modeling - Interactive Notebook\n",
    "\n",
    "This notebook provides hands-on experience with RNNs and demonstrates why we need attention mechanisms.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML, display\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Helper for pretty printing\n",
    "def print_section(title):\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"  {title}\")\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding Sequential Data\n",
    "\n",
    "Let's start by seeing why order matters in sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Why Order Matters\")\n",
    "\n",
    "# Example 1: Word order changes meaning\n",
    "sentences = [\n",
    "    \"The cat chased the mouse\",\n",
    "    \"The mouse chased the cat\",\n",
    "    \"Chased the mouse the cat\"  # Scrambled\n",
    "]\n",
    "\n",
    "print(\"Sentences with different word orders:\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}. {sent}\")\n",
    "\n",
    "# Example 2: Bag of Words loses order\n",
    "print(\"\\nBag of Words representation:\")\n",
    "for sent in sentences[:2]:\n",
    "    words = sent.lower().split()\n",
    "    bow = {word: words.count(word) for word in set(words)}\n",
    "    print(f\"'{sent}' → {bow}\")\n",
    "\n",
    "print(\"\\n⚠️  Both sentences have the same BoW representation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Simple RNN from Scratch\n",
    "\n",
    "Let's implement a basic RNN to understand how it processes sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"A minimal RNN implementation for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # Initialize weights with small random values\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Why = np.random.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def step(self, x, h_prev):\n",
    "        \"\"\"Single RNN step.\"\"\"\n",
    "        # Update hidden state\n",
    "        h = np.tanh(self.Wxh @ x + self.Whh @ h_prev + self.bh)\n",
    "        # Compute output\n",
    "        y = self.Why @ h + self.by\n",
    "        return h, y\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Process entire sequence.\"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hiddens = [h]\n",
    "        \n",
    "        for x in inputs:\n",
    "            h, y = self.step(x, h)\n",
    "            outputs.append(y)\n",
    "            hiddens.append(h)\n",
    "            \n",
    "        return outputs, hiddens\n",
    "\n",
    "# Create a small RNN\n",
    "rnn = SimpleRNN(input_size=3, hidden_size=4, output_size=2)\n",
    "\n",
    "# Process a sequence\n",
    "sequence = [np.random.randn(3, 1) for _ in range(5)]\n",
    "outputs, hiddens = rnn.forward(sequence)\n",
    "\n",
    "print_section(\"RNN Processing\")\n",
    "print(f\"Sequence length: {len(sequence)}\")\n",
    "print(f\"Hidden state shape: {hiddens[0].shape}\")\n",
    "print(f\"Output shape: {outputs[0].shape}\")\n",
    "print(f\"\\nNumber of parameters: {rnn.Wxh.size + rnn.Whh.size + rnn.Why.size + rnn.bh.size + rnn.by.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing RNN Processing\n",
    "\n",
    "Let's visualize how information flows through an RNN step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rnn_processing():\n",
    "    \"\"\"Visualize how RNN processes a sequence step by step.\"\"\"\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "    fig.suptitle('RNN Processing Steps', fontsize=16)\n",
    "    \n",
    "    # Sample sequence\n",
    "    words = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "    hidden_size = 4\n",
    "    \n",
    "    # Process each word\n",
    "    hidden_states = []\n",
    "    h = np.zeros(hidden_size)\n",
    "    \n",
    "    for t, word in enumerate(words):\n",
    "        # Simulate processing (random for visualization)\n",
    "        h = np.tanh(np.random.randn(hidden_size) + 0.5 * h)\n",
    "        hidden_states.append(h)\n",
    "        \n",
    "        if t < 6:  # We have 6 subplot slots\n",
    "            ax = axes[t // 3, t % 3]\n",
    "            \n",
    "            # Visualize hidden state\n",
    "            ax.bar(range(hidden_size), h, color='blue', alpha=0.7)\n",
    "            ax.set_ylim(-1, 1)\n",
    "            ax.set_title(f\"Step {t+1}: '{word}'\")\n",
    "            ax.set_xlabel('Hidden units')\n",
    "            ax.set_ylabel('Activation')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add text showing what's encoded\n",
    "            encoded = \" \".join(words[:t+1])\n",
    "            ax.text(0.5, -1.3, f\"Encoding: '{encoded}'\", \n",
    "                   transform=ax.transAxes, ha='center', fontsize=10)\n",
    "    \n",
    "    # Hide unused subplot\n",
    "    axes[1, 2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show hidden state evolution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    hidden_matrix = np.array(hidden_states).T\n",
    "    \n",
    "    plt.imshow(hidden_matrix, aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Activation')\n",
    "    plt.xlabel('Time step')\n",
    "    plt.ylabel('Hidden unit')\n",
    "    plt.title('Hidden State Evolution Over Time')\n",
    "    plt.xticks(range(len(words)), words)\n",
    "    plt.show()\n",
    "\n",
    "visualize_rnn_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Vanishing Gradient Problem\n",
    "\n",
    "Let's demonstrate why RNNs struggle with long sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_gradient_flow():\n",
    "    \"\"\"Show how gradients vanish or explode in RNNs.\"\"\"\n",
    "    \n",
    "    print_section(\"Gradient Flow in RNNs\")\n",
    "    \n",
    "    # Simulate gradient backpropagation\n",
    "    sequence_lengths = [5, 10, 20, 50]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    for idx, seq_len in enumerate(sequence_lengths):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        # Case 1: Vanishing gradients (factor < 1)\n",
    "        gradient_vanish = []\n",
    "        grad = 1.0\n",
    "        for t in range(seq_len):\n",
    "            grad *= 0.8  # Gradient shrinks\n",
    "            gradient_vanish.append(grad)\n",
    "        \n",
    "        # Case 2: Exploding gradients (factor > 1)\n",
    "        gradient_explode = []\n",
    "        grad = 1.0\n",
    "        for t in range(seq_len):\n",
    "            grad *= 1.2  # Gradient grows\n",
    "            gradient_explode.append(grad)\n",
    "        \n",
    "        # Plot\n",
    "        ax.semilogy(gradient_vanish, 'b-', linewidth=2, label='Vanishing (×0.8)')\n",
    "        ax.semilogy(gradient_explode, 'r-', linewidth=2, label='Exploding (×1.2)')\n",
    "        ax.axhline(y=0.01, color='gray', linestyle='--', alpha=0.5)\n",
    "        ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        ax.set_xlabel('Backprop steps')\n",
    "        ax.set_ylabel('Gradient magnitude')\n",
    "        ax.set_title(f'Sequence length: {seq_len}')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add annotations\n",
    "        ax.fill_between(range(seq_len), 0.01, 0.0001, alpha=0.2, color='blue', \n",
    "                       label='Too small to learn')\n",
    "        ax.fill_between(range(seq_len), 100, 1000, alpha=0.2, color='red',\n",
    "                       label='Numerical instability')\n",
    "    \n",
    "    plt.suptitle('Gradient Flow Through Time', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the mathematical reason\n",
    "    print(\"\\nMathematical explanation:\")\n",
    "    print(\"Gradient after t steps: ∂L/∂h₀ = ∂L/∂hₜ × ∏(∂hᵢ/∂hᵢ₋₁)\")\n",
    "    print(\"If |∂hᵢ/∂hᵢ₋₁| < 1: gradient → 0 (vanishing)\")\n",
    "    print(\"If |∂hᵢ/∂hᵢ₋₁| > 1: gradient → ∞ (exploding)\")\n",
    "\n",
    "demonstrate_gradient_flow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LSTM: Attempting to Solve the Problem\n",
    "\n",
    "Let's see how LSTMs try to address gradient issues with gating mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell:\n",
    "    \"\"\"Simple LSTM cell implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        # Combined weights for all gates\n",
    "        self.W = np.random.randn(4 * hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b = np.zeros((4 * hidden_size, 1))\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        # Concatenate input and previous hidden\n",
    "        combined = np.vstack([x, h_prev])\n",
    "        \n",
    "        # Compute all gates at once\n",
    "        gates = self.W @ combined + self.b\n",
    "        \n",
    "        # Split into individual gates\n",
    "        i_gate = self._sigmoid(gates[:self.hidden_size])  # Input gate\n",
    "        f_gate = self._sigmoid(gates[self.hidden_size:2*self.hidden_size])  # Forget gate\n",
    "        g_gate = np.tanh(gates[2*self.hidden_size:3*self.hidden_size])  # Candidate\n",
    "        o_gate = self._sigmoid(gates[3*self.hidden_size:])  # Output gate\n",
    "        \n",
    "        # Update cell state (highway for gradients)\n",
    "        c = f_gate * c_prev + i_gate * g_gate\n",
    "        \n",
    "        # Update hidden state\n",
    "        h = o_gate * np.tanh(c)\n",
    "        \n",
    "        return h, c, (i_gate, f_gate, g_gate, o_gate)\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "# Visualize LSTM gates\n",
    "def visualize_lstm_gates():\n",
    "    print_section(\"LSTM Gate Visualization\")\n",
    "    \n",
    "    lstm = LSTMCell(input_size=5, hidden_size=4)\n",
    "    \n",
    "    # Sample input\n",
    "    x = np.random.randn(5, 1)\n",
    "    h_prev = np.zeros((4, 1))\n",
    "    c_prev = np.zeros((4, 1))\n",
    "    \n",
    "    # Forward pass\n",
    "    h, c, gates = lstm.forward(x, h_prev, c_prev)\n",
    "    i_gate, f_gate, g_gate, o_gate = gates\n",
    "    \n",
    "    # Plot gates\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    gates_data = [\n",
    "        (i_gate, 'Input Gate', 'Controls new information'),\n",
    "        (f_gate, 'Forget Gate', 'Controls what to forget'),\n",
    "        (g_gate, 'Candidate Values', 'New information to add'),\n",
    "        (o_gate, 'Output Gate', 'Controls what to output')\n",
    "    ]\n",
    "    \n",
    "    for idx, (gate, title, desc) in enumerate(gates_data):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        \n",
    "        bars = ax.bar(range(len(gate)), gate.flatten())\n",
    "        \n",
    "        # Color bars based on value\n",
    "        for bar, val in zip(bars, gate.flatten()):\n",
    "            if title == 'Candidate Values':\n",
    "                bar.set_color('green' if val > 0 else 'red')\n",
    "            else:\n",
    "                bar.set_color(plt.cm.Blues(val))\n",
    "        \n",
    "        ax.set_ylim(-1.5 if title == 'Candidate Values' else 0, 1.5 if title == 'Candidate Values' else 1)\n",
    "        ax.set_xlabel('Hidden units')\n",
    "        ax.set_ylabel('Gate value')\n",
    "        ax.set_title(f'{title}\\n{desc}')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('LSTM Gates in Action', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nKey insights:\")\n",
    "    print(\"- Forget gate ≈ 0.5: Keeping about half of previous information\")\n",
    "    print(\"- Input gate controls how much new information to add\")\n",
    "    print(\"- Cell state provides a highway for gradient flow\")\n",
    "    print(\"- But still processes sequentially!\")\n",
    "\n",
    "visualize_lstm_gates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Information Bottleneck Problem\n",
    "\n",
    "Let's visualize how RNNs compress all information into a fixed-size vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_bottleneck():\n",
    "    print_section(\"Information Bottleneck in Seq2Seq\")\n",
    "    \n",
    "    # Simulate encoding sentences of different lengths\n",
    "    sentences = [\n",
    "        \"Hello\",\n",
    "        \"Hello world\",\n",
    "        \"The quick brown fox jumps\",\n",
    "        \"The quick brown fox jumps over the lazy dog near the river\"\n",
    "    ]\n",
    "    \n",
    "    hidden_size = 8  # Fixed size context vector\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    for idx, sentence in enumerate(sentences):\n",
    "        ax = axes[idx // 2, idx % 2]\n",
    "        words = sentence.split()\n",
    "        \n",
    "        # Simulate encoding process\n",
    "        context_vector = np.random.randn(hidden_size)\n",
    "        \n",
    "        # Visualize compression\n",
    "        word_info = len(words) * 50  # Arbitrary \"information units\" per word\n",
    "        context_capacity = hidden_size * 10  # Capacity of context vector\n",
    "        \n",
    "        # Bar chart showing information\n",
    "        bars = ax.bar(['Input Information', 'Context Capacity'], \n",
    "                      [word_info, context_capacity])\n",
    "        \n",
    "        # Color based on whether information fits\n",
    "        if word_info > context_capacity:\n",
    "            bars[0].set_color('red')\n",
    "            bars[1].set_color('orange')\n",
    "            ax.text(0.5, word_info + 10, 'Information Lost!', \n",
    "                   ha='center', color='red', fontweight='bold')\n",
    "        else:\n",
    "            bars[0].set_color('green')\n",
    "            bars[1].set_color('green')\n",
    "        \n",
    "        ax.set_ylabel('Information (arbitrary units)')\n",
    "        ax.set_title(f'Sentence: \"{sentence}\"\\n({len(words)} words)')\n",
    "        \n",
    "        # Add text showing the compression ratio\n",
    "        compression_ratio = word_info / context_capacity\n",
    "        ax.text(0.5, -0.15, f'Compression ratio: {compression_ratio:.1f}:1', \n",
    "               transform=ax.transAxes, ha='center')\n",
    "    \n",
    "    plt.suptitle('Information Bottleneck: Longer Sequences Lose More Information', \n",
    "                fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🔴 Problem: All sentences compressed to same size vector!\")\n",
    "    print(\"🟢 Solution: Attention lets decoder look at all encoder states\")\n",
    "\n",
    "demonstrate_bottleneck()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sequential vs Parallel Processing\n",
    "\n",
    "Let's compare how RNNs and Attention process sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_processing_methods():\n",
    "    print_section(\"Sequential (RNN) vs Parallel (Attention) Processing\")\n",
    "    \n",
    "    sequence_length = 8\n",
    "    \n",
    "    # Simulate RNN processing\n",
    "    print(\"RNN Processing (Sequential):\")\n",
    "    print(\"Each step must wait for the previous one...\\n\")\n",
    "    \n",
    "    rnn_times = []\n",
    "    for t in range(sequence_length):\n",
    "        print(f\"Step {t+1}: \", end='')\n",
    "        for _ in range(t+1):\n",
    "            print(\"→\", end='')\n",
    "            time.sleep(0.1)\n",
    "        print(f\" Done (depends on steps 1-{t})\")\n",
    "        rnn_times.append((t+1) * 0.1)\n",
    "    \n",
    "    total_rnn_time = sum(rnn_times)\n",
    "    print(f\"\\nTotal RNN time: {total_rnn_time:.1f}s\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Simulate Attention processing  \n",
    "    print(\"Attention Processing (Parallel):\")\n",
    "    print(\"All positions processed simultaneously!\\n\")\n",
    "    \n",
    "    print(\"All steps: \", end='')\n",
    "    for t in range(sequence_length):\n",
    "        print(f\"[{t+1}]\", end=' ')\n",
    "    \n",
    "    time.sleep(0.2)  # Simulate parallel processing\n",
    "    print(\"→ Done!\")\n",
    "    print(f\"\\nTotal Attention time: 0.2s (with {sequence_length} parallel processors)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # RNN timeline\n",
    "    for t in range(sequence_length):\n",
    "        ax1.barh(t, 1, left=t, height=0.8, alpha=0.7, \n",
    "                color=plt.cm.Blues(0.5 + t * 0.05))\n",
    "        ax1.text(t + 0.5, t, f'Step {t+1}', ha='center', va='center')\n",
    "    \n",
    "    ax1.set_xlim(0, sequence_length)\n",
    "    ax1.set_ylim(-0.5, sequence_length - 0.5)\n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Processing Step')\n",
    "    ax1.set_title('RNN: Sequential Processing')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Attention timeline\n",
    "    ax2.barh(range(sequence_length), [1] * sequence_length, \n",
    "            height=0.8, alpha=0.7, color='green')\n",
    "    for t in range(sequence_length):\n",
    "        ax2.text(0.5, t, f'Pos {t+1}', ha='center', va='center')\n",
    "    \n",
    "    ax2.set_xlim(0, 2)\n",
    "    ax2.set_ylim(-0.5, sequence_length - 0.5)\n",
    "    ax2.set_xlabel('Time')\n",
    "    ax2.set_ylabel('Position')\n",
    "    ax2.set_title('Attention: Parallel Processing')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Processing Time Comparison', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n⚡ Speedup with parallelization: {total_rnn_time/0.2:.0f}x faster!\")\n",
    "\n",
    "compare_processing_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training a Simple RNN Language Model\n",
    "\n",
    "Let's train a character-level RNN to see its limitations in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size=128):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden=None):\n",
    "        embed = self.embedding(x)\n",
    "        out, hidden = self.rnn(embed, hidden)\n",
    "        out = self.output(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def generate(self, start_char, char_to_idx, idx_to_char, length=100):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            # Start with the given character\n",
    "            current = torch.tensor([[char_to_idx[start_char]]])\n",
    "            hidden = None\n",
    "            result = start_char\n",
    "            \n",
    "            for _ in range(length - 1):\n",
    "                output, hidden = self.forward(current, hidden)\n",
    "                probs = torch.softmax(output[0, -1], dim=0)\n",
    "                next_char_idx = torch.multinomial(probs, 1).item()\n",
    "                result += idx_to_char[next_char_idx]\n",
    "                current = torch.tensor([[next_char_idx]])\n",
    "                \n",
    "        return result\n",
    "\n",
    "# Train on a simple pattern\n",
    "def train_char_rnn():\n",
    "    print_section(\"Training Character-Level RNN\")\n",
    "    \n",
    "    # Simple repetitive text to learn\n",
    "    text = \"hello world! \" * 20\n",
    "    chars = list(set(text))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "    \n",
    "    print(f\"Training text: '{text[:50]}...'\")\n",
    "    print(f\"Vocabulary size: {len(chars)}\")\n",
    "    print(f\"Characters: {chars}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    data = torch.tensor([char_to_idx[ch] for ch in text])\n",
    "    \n",
    "    # Create model\n",
    "    model = CharRNN(len(chars), hidden_size=32)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training\n",
    "    losses = []\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        hidden = None\n",
    "        total_loss = 0\n",
    "        \n",
    "        # Process sequence in chunks\n",
    "        chunk_size = 10\n",
    "        for i in range(0, len(data) - chunk_size, chunk_size):\n",
    "            # Get input and target\n",
    "            inputs = data[i:i+chunk_size].unsqueeze(0)\n",
    "            targets = data[i+1:i+chunk_size+1].unsqueeze(0)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            loss = criterion(outputs.view(-1, len(chars)), targets.view(-1))\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Detach hidden state\n",
    "            hidden = hidden.detach()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        losses.append(total_loss)\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss:.4f}\")\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\nGenerated text samples:\")\n",
    "    for start in ['h', 'w', ' ']:\n",
    "        generated = model.generate(start, char_to_idx, idx_to_char, length=50)\n",
    "        print(f\"Starting with '{start}': {generated}\")\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(losses)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('RNN Training Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n📝 Note: RNN learned the pattern but would struggle with longer dependencies!\")\n",
    "\n",
    "train_char_rnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demonstrating Long-Range Dependencies\n",
    "\n",
    "Let's see why RNNs fail at long-range dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_range_dependency_test():\n",
    "    print_section(\"Long-Range Dependency Challenge\")\n",
    "    \n",
    "    # Create sentences with dependencies at different distances\n",
    "    test_sentences = [\n",
    "        (\"The cat [MASK].\", \"sat\", 1),\n",
    "        (\"The cat that was black [MASK].\", \"sat\", 4),\n",
    "        (\"The cat that chased the mouse that ate the cheese [MASK].\", \"sat\", 10),\n",
    "        (\"The cat that lived in the house that Jack built which stood on the hill [MASK].\", \"sat\", 15)\n",
    "    ]\n",
    "    \n",
    "    # Simulate RNN performance degradation\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    distances = []\n",
    "    rnn_accuracy = []\n",
    "    attention_accuracy = []\n",
    "    \n",
    "    for sent, answer, distance in test_sentences:\n",
    "        distances.append(distance)\n",
    "        \n",
    "        # Simulate accuracy (RNN degrades, Attention maintains)\n",
    "        rnn_acc = 0.95 * (0.8 ** (distance / 5))  # Exponential decay\n",
    "        att_acc = 0.95 - 0.01 * (distance / 10)   # Slight linear decay\n",
    "        \n",
    "        rnn_accuracy.append(rnn_acc)\n",
    "        attention_accuracy.append(att_acc)\n",
    "        \n",
    "        print(f\"\\nSentence: {sent}\")\n",
    "        print(f\"Correct answer: '{answer}'\")\n",
    "        print(f\"Distance to dependency: {distance} words\")\n",
    "        print(f\"RNN accuracy: {rnn_acc:.2%}\")\n",
    "        print(f\"Attention accuracy: {att_acc:.2%}\")\n",
    "    \n",
    "    # Plot comparison\n",
    "    x = np.arange(len(distances))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, rnn_accuracy, width, label='RNN', color='red', alpha=0.7)\n",
    "    plt.bar(x + width/2, attention_accuracy, width, label='Attention', color='green', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Test Case')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Performance on Long-Range Dependencies')\n",
    "    plt.xticks(x, [f'Distance: {d}' for d in distances])\n",
    "    plt.legend()\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, (rnn, att) in enumerate(zip(rnn_accuracy, attention_accuracy)):\n",
    "        plt.text(i - width/2, rnn + 0.02, f'{rnn:.0%}', ha='center')\n",
    "        plt.text(i + width/2, att + 0.02, f'{att:.0%}', ha='center')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n🔴 RNN: Performance drops significantly with distance\")\n",
    "    print(\"🟢 Attention: Maintains performance regardless of distance\")\n",
    "\n",
    "long_range_dependency_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary: Why We Need Attention\n",
    "\n",
    "Let's summarize all the problems with RNNs and preview the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_visualization():\n",
    "    print_section(\"RNN Limitations vs Attention Solutions\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Sequential vs Parallel\n",
    "    ax = axes[0, 0]\n",
    "    ax.text(0.5, 0.8, 'Processing', ha='center', fontsize=14, fontweight='bold')\n",
    "    ax.text(0.25, 0.6, 'RNN', ha='center', fontsize=12, color='red')\n",
    "    ax.text(0.25, 0.4, '→→→→→', ha='center', fontsize=10)\n",
    "    ax.text(0.25, 0.2, 'Sequential\\n(Slow)', ha='center', fontsize=10)\n",
    "    \n",
    "    ax.text(0.75, 0.6, 'Attention', ha='center', fontsize=12, color='green')\n",
    "    ax.text(0.75, 0.4, '⇉⇉⇉⇉⇉', ha='center', fontsize=10)\n",
    "    ax.text(0.75, 0.2, 'Parallel\\n(Fast)', ha='center', fontsize=10)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 2. Information flow\n",
    "    ax = axes[0, 1]\n",
    "    ax.text(0.5, 0.8, 'Information Flow', ha='center', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # RNN bottleneck\n",
    "    ax.arrow(0.1, 0.5, 0.15, 0, head_width=0.05, color='red')\n",
    "    ax.text(0.3, 0.5, '🍾', fontsize=20)\n",
    "    ax.arrow(0.35, 0.5, 0.15, 0, head_width=0.05, color='red')\n",
    "    ax.text(0.25, 0.3, 'Bottleneck', ha='center', color='red')\n",
    "    \n",
    "    # Attention direct connections\n",
    "    for i in range(5):\n",
    "        y = 0.5 + (i - 2) * 0.05\n",
    "        ax.arrow(0.6, y, 0.3, 0, head_width=0.02, color='green', alpha=0.7)\n",
    "    ax.text(0.75, 0.3, 'Direct paths', ha='center', color='green')\n",
    "    \n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 3. Gradient flow\n",
    "    ax = axes[1, 0]\n",
    "    steps = np.arange(10)\n",
    "    rnn_gradient = 0.8 ** steps\n",
    "    attention_gradient = np.ones_like(steps) * 0.9\n",
    "    \n",
    "    ax.plot(steps, rnn_gradient, 'r-', linewidth=2, label='RNN')\n",
    "    ax.plot(steps, attention_gradient, 'g-', linewidth=2, label='Attention')\n",
    "    ax.set_xlabel('Backprop Steps')\n",
    "    ax.set_ylabel('Gradient Magnitude')\n",
    "    ax.set_title('Gradient Flow')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Memory access pattern\n",
    "    ax = axes[1, 1]\n",
    "    positions = np.arange(8)\n",
    "    \n",
    "    # RNN: can only access through chain\n",
    "    ax.text(0.5, 0.9, 'Memory Access', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw RNN chain\n",
    "    for i in range(7):\n",
    "        ax.plot([i, i+1], [0.7, 0.7], 'r-', linewidth=2)\n",
    "        ax.plot(i, 0.7, 'ro', markersize=8)\n",
    "    ax.plot(7, 0.7, 'ro', markersize=8)\n",
    "    ax.text(3.5, 0.6, 'RNN: Sequential access only', ha='center', color='red')\n",
    "    \n",
    "    # Draw Attention full connectivity\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if i != j:\n",
    "                ax.plot([i, j], [0.3, 0.3], 'g-', alpha=0.1, linewidth=1)\n",
    "        ax.plot(i, 0.3, 'go', markersize=8)\n",
    "    ax.text(3.5, 0.2, 'Attention: Random access to any position', ha='center', color='green')\n",
    "    \n",
    "    ax.set_xlim(-0.5, 7.5)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.suptitle('Why Attention Solves RNN Problems', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_summary_visualization()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎯 KEY INSIGHTS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1. RNNs must process sequences step by step (slow)\")\n",
    "print(\"   → Attention processes all positions in parallel (fast)\")\n",
    "print(\"\\n2. RNNs compress everything into fixed-size vectors\")\n",
    "print(\"   → Attention maintains all information\")\n",
    "print(\"\\n3. RNNs suffer from vanishing/exploding gradients\")\n",
    "print(\"   → Attention has direct gradient paths\")\n",
    "print(\"\\n4. RNNs can only access memory sequentially\")\n",
    "print(\"   → Attention can access any position directly\")\n",
    "print(\"\\n🚀 Next: Learn how attention mechanisms work!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Try these exercises to deepen your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section(\"Exercises\")\n",
    "\n",
    "print(\"1. Gradient Clipping\")\n",
    "print(\"   Implement gradient clipping in the RNN backward pass.\")\n",
    "print(\"   Why is this necessary?\\n\")\n",
    "\n",
    "print(\"2. Bidirectional RNN\")\n",
    "print(\"   Modify the SimpleRNN to process sequences in both directions.\")\n",
    "print(\"   What are the benefits?\\n\")\n",
    "\n",
    "print(\"3. GRU Implementation\")\n",
    "print(\"   Implement a GRU cell (simpler than LSTM).\")\n",
    "print(\"   Compare its gates with LSTM.\\n\")\n",
    "\n",
    "print(\"4. Attention Preview\")\n",
    "print(\"   Given a sequence, compute pairwise similarity scores.\")\n",
    "print(\"   This is the foundation of attention!\\n\")\n",
    "\n",
    "# Skeleton for Exercise 4\n",
    "def attention_preview():\n",
    "    # Word embeddings (random for now)\n",
    "    words = [\"The\", \"cat\", \"sat\", \"on\", \"mat\"]\n",
    "    embeddings = np.random.randn(5, 4)  # 5 words, 4 dimensions\n",
    "    \n",
    "    # TODO: Compute similarity matrix using dot products\n",
    "    # similarity[i,j] = embedding[i] · embedding[j]\n",
    "    \n",
    "    # TODO: Apply softmax to get attention weights\n",
    "    # For each word, what other words does it \"attend to\"?\n",
    "    \n",
    "    pass\n",
    "\n",
    "print(\"💡 Hint for Exercise 4: This is exactly what self-attention does!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}