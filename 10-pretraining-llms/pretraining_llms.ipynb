{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining Large Language Models\n",
    "\n",
    "This notebook provides an interactive guide to pretraining LLMs from scratch, covering dataset preparation, distributed training strategies, and practical considerations for training at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to LLM Pretraining\n",
    "\n",
    "Pretraining is the foundation of modern LLMs. During this phase, models learn general language understanding from massive text corpora before being fine-tuned for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Scale of Pretraining\n",
    "\n",
    "Let's visualize the scale of modern LLM pretraining to understand the computational requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model sizes and training data\n",
    "models_data = {\n",
    "    'Model': ['GPT-2', 'GPT-3', 'LLaMA-7B', 'LLaMA-65B', 'GPT-4*'],\n",
    "    'Parameters': [1.5e9, 175e9, 7e9, 65e9, 1.7e12],\n",
    "    'Training Tokens': [10e9, 300e9, 1e12, 1.4e12, 13e12],\n",
    "    'Training FLOPs': [1.5e21, 3.14e23, 2e23, 1e24, 2e25],\n",
    "    'GPU Hours (A100)': [100, 30000, 82000, 1e6, 1e7]\n",
    "}\n",
    "\n",
    "df_models = pd.DataFrame(models_data)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Parameters\n",
    "axes[0, 0].bar(df_models['Model'], df_models['Parameters'] / 1e9)\n",
    "axes[0, 0].set_ylabel('Parameters (Billions)')\n",
    "axes[0, 0].set_title('Model Size Comparison')\n",
    "axes[0, 0].set_yscale('log')\n",
    "\n",
    "# Training tokens\n",
    "axes[0, 1].bar(df_models['Model'], df_models['Training Tokens'] / 1e12)\n",
    "axes[0, 1].set_ylabel('Training Tokens (Trillions)')\n",
    "axes[0, 1].set_title('Training Data Scale')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# Compute required\n",
    "axes[1, 0].bar(df_models['Model'], df_models['Training FLOPs'] / 1e23)\n",
    "axes[1, 0].set_ylabel('FLOPs (×10²³)')\n",
    "axes[1, 0].set_title('Computational Requirements')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# GPU hours\n",
    "axes[1, 1].bar(df_models['Model'], df_models['GPU Hours (A100)'])\n",
    "axes[1, 1].set_ylabel('GPU Hours (A100)')\n",
    "axes[1, 1].set_title('Training Time')\n",
    "axes[1, 1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"* GPT-4 numbers are estimates based on reports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Preparation Pipeline\n",
    "\n",
    "High-quality data is crucial for successful pretraining. Let's explore the data preparation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQualityAnalyzer:\n",
    "    \"\"\"Analyze text quality for pretraining.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.quality_scores = []\n",
    "        \n",
    "    def analyze_text(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze various quality metrics of text.\"\"\"\n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # Basic metrics\n",
    "        word_count = len(words)\n",
    "        avg_word_length = np.mean([len(w) for w in words]) if words else 0\n",
    "        \n",
    "        # Vocabulary diversity\n",
    "        unique_words = len(set(words))\n",
    "        vocab_diversity = unique_words / word_count if word_count > 0 else 0\n",
    "        \n",
    "        # Repetition score\n",
    "        bigrams = [' '.join(words[i:i+2]) for i in range(len(words)-1)]\n",
    "        unique_bigrams = len(set(bigrams))\n",
    "        repetition_score = 1 - (unique_bigrams / len(bigrams) if bigrams else 1)\n",
    "        \n",
    "        # Sentence length variation\n",
    "        sent_lengths = [len(s.split()) for s in sentences if s.strip()]\n",
    "        sent_length_std = np.std(sent_lengths) if len(sent_lengths) > 1 else 0\n",
    "        \n",
    "        return {\n",
    "            'word_count': word_count,\n",
    "            'avg_word_length': avg_word_length,\n",
    "            'vocab_diversity': vocab_diversity,\n",
    "            'repetition_score': repetition_score,\n",
    "            'sentence_variation': sent_length_std,\n",
    "            'quality_score': self._compute_quality_score(\n",
    "                vocab_diversity, repetition_score, sent_length_std\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def _compute_quality_score(self, diversity: float, repetition: float, \n",
    "                             variation: float) -> float:\n",
    "        \"\"\"Compute overall quality score.\"\"\"\n",
    "        # Higher diversity is better\n",
    "        # Lower repetition is better\n",
    "        # Higher sentence variation is better (to a point)\n",
    "        score = (diversity * 0.4 + \n",
    "                (1 - repetition) * 0.4 + \n",
    "                min(variation / 10, 1) * 0.2)\n",
    "        return score\n",
    "\n",
    "# Example texts of varying quality\n",
    "texts = [\n",
    "    \"\"\"The transformer architecture has revolutionized natural language processing\n",
    "    by introducing self-attention mechanisms that allow models to process sequences\n",
    "    in parallel. This breakthrough has led to significant improvements in various\n",
    "    NLP tasks including translation, summarization, and question answering.\"\"\",\n",
    "    \n",
    "    \"\"\"The cat sat on the mat. The cat sat on the mat. The cat sat on the mat.\n",
    "    The cat sat on the mat. The cat sat on the mat. The cat sat on the mat.\"\"\",\n",
    "    \n",
    "    \"\"\"Lorem ipsum dolor sit amet consectetur adipiscing elit sed do eiusmod\n",
    "    tempor incididunt ut labore et dolore magna aliqua ut enim ad minim veniam\n",
    "    quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo.\"\"\"\n",
    "]\n",
    "\n",
    "analyzer = DataQualityAnalyzer()\n",
    "quality_results = []\n",
    "\n",
    "for i, text in enumerate(texts):\n",
    "    metrics = analyzer.analyze_text(text)\n",
    "    quality_results.append({\n",
    "        'Text': f'Sample {i+1}',\n",
    "        **metrics\n",
    "    })\n",
    "\n",
    "df_quality = pd.DataFrame(quality_results)\n",
    "print(\"Text Quality Analysis:\")\n",
    "display(df_quality.round(3))\n",
    "\n",
    "# Visualize quality scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "metrics_to_plot = ['vocab_diversity', 'repetition_score', 'quality_score']\n",
    "x = np.arange(len(texts))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar(x + i * width, df_quality[metric], width, label=metric)\n",
    "\n",
    "plt.xlabel('Text Sample')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Text Quality Metrics Comparison')\n",
    "plt.xticks(x + width, [f'Sample {i+1}' for i in range(len(texts))])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Deduplication Strategies\n",
    "\n",
    "Deduplication is crucial to prevent memorization and improve generalization. Let's explore different deduplication methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from collections import defaultdict\n",
    "\n",
    "class DeduplicationDemo:\n",
    "    \"\"\"Demonstrate different deduplication strategies.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def exact_dedup(documents: List[str]) -> Tuple[List[str], int]:\n",
    "        \"\"\"Exact deduplication using hashing.\"\"\"\n",
    "        seen_hashes = set()\n",
    "        unique_docs = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_hash = hashlib.sha256(doc.encode()).hexdigest()\n",
    "            if doc_hash not in seen_hashes:\n",
    "                seen_hashes.add(doc_hash)\n",
    "                unique_docs.append(doc)\n",
    "                \n",
    "        return unique_docs, len(documents) - len(unique_docs)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ngram_dedup(documents: List[str], n: int = 5, \n",
    "                   threshold: float = 0.8) -> Tuple[List[str], int]:\n",
    "        \"\"\"N-gram based deduplication.\"\"\"\n",
    "        def get_ngrams(text: str, n: int) -> set:\n",
    "            words = text.split()\n",
    "            return set(' '.join(words[i:i+n]) for i in range(len(words)-n+1))\n",
    "        \n",
    "        unique_docs = []\n",
    "        all_ngrams = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            doc_ngrams = get_ngrams(doc, n)\n",
    "            \n",
    "            # Check overlap with existing documents\n",
    "            is_duplicate = False\n",
    "            for existing_ngrams in all_ngrams:\n",
    "                overlap = len(doc_ngrams & existing_ngrams)\n",
    "                if overlap / len(doc_ngrams) > threshold:\n",
    "                    is_duplicate = True\n",
    "                    break\n",
    "                    \n",
    "            if not is_duplicate:\n",
    "                unique_docs.append(doc)\n",
    "                all_ngrams.append(doc_ngrams)\n",
    "                \n",
    "        return unique_docs, len(documents) - len(unique_docs)\n",
    "\n",
    "# Create sample documents with duplicates\n",
    "sample_docs = [\n",
    "    \"The transformer model uses self-attention to process sequences.\",\n",
    "    \"The transformer model uses self-attention to process sequences.\",  # Exact duplicate\n",
    "    \"The transformer architecture uses self-attention to process sequences.\",  # Near duplicate\n",
    "    \"BERT is a bidirectional transformer model for NLP tasks.\",\n",
    "    \"GPT is an autoregressive transformer model for text generation.\",\n",
    "    \"BERT is a bidirectional transformer model for NLP tasks.\",  # Exact duplicate\n",
    "    \"Attention mechanisms allow models to focus on relevant parts of input.\"\n",
    "]\n",
    "\n",
    "dedup = DeduplicationDemo()\n",
    "\n",
    "# Test exact deduplication\n",
    "exact_unique, exact_removed = dedup.exact_dedup(sample_docs)\n",
    "print(f\"Exact Deduplication:\")\n",
    "print(f\"  Original documents: {len(sample_docs)}\")\n",
    "print(f\"  Unique documents: {len(exact_unique)}\")\n",
    "print(f\"  Removed: {exact_removed}\\n\")\n",
    "\n",
    "# Test n-gram deduplication\n",
    "ngram_unique, ngram_removed = dedup.ngram_dedup(sample_docs, n=3, threshold=0.6)\n",
    "print(f\"N-gram Deduplication (3-grams, 60% threshold):\")\n",
    "print(f\"  Original documents: {len(sample_docs)}\")\n",
    "print(f\"  Unique documents: {len(ngram_unique)}\")\n",
    "print(f\"  Removed: {ngram_removed}\\n\")\n",
    "\n",
    "# Visualize deduplication results\n",
    "methods = ['Original', 'Exact Dedup', 'N-gram Dedup']\n",
    "counts = [len(sample_docs), len(exact_unique), len(ngram_unique)]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(methods, counts, color=['gray', 'blue', 'green'])\n",
    "plt.ylabel('Number of Documents')\n",
    "plt.title('Deduplication Results')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
    "             str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.ylim(0, max(counts) + 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Objectives\n",
    "\n",
    "Let's explore different training objectives used in LLM pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training_objectives():\n",
    "    \"\"\"Visualize different LLM training objectives.\"\"\"\n",
    "    \n",
    "    # Sample sequence\n",
    "    tokens = [\"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\"]\n",
    "    n_tokens = len(tokens)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Causal Language Modeling (CLM)\n",
    "    ax = axes[0, 0]\n",
    "    clm_mask = np.tril(np.ones((n_tokens, n_tokens)))\n",
    "    im1 = ax.imshow(clm_mask, cmap='Blues', aspect='equal')\n",
    "    ax.set_title('Causal Language Modeling (GPT-style)', fontsize=14)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Can attend to')\n",
    "    ax.set_xticks(range(n_tokens))\n",
    "    ax.set_yticks(range(n_tokens))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    \n",
    "    # Add arrows to show prediction direction\n",
    "    for i in range(n_tokens - 1):\n",
    "        ax.annotate('', xy=(i+1, i), xytext=(i, i),\n",
    "                   arrowprops=dict(arrowstyle='->', color='red', lw=2))\n",
    "    \n",
    "    # 2. Masked Language Modeling (MLM)\n",
    "    ax = axes[0, 1]\n",
    "    mlm_mask = np.ones((n_tokens, n_tokens))\n",
    "    masked_positions = [2, 5, 7]  # Mask \"brown\", \"over\", \"lazy\"\n",
    "    \n",
    "    # Create visualization\n",
    "    mlm_visual = np.ones((n_tokens, n_tokens)) * 0.3\n",
    "    for pos in masked_positions:\n",
    "        mlm_visual[pos, :] = 1.0\n",
    "        mlm_visual[:, pos] = 1.0\n",
    "    \n",
    "    im2 = ax.imshow(mlm_visual, cmap='Reds', aspect='equal')\n",
    "    ax.set_title('Masked Language Modeling (BERT-style)', fontsize=14)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Position')\n",
    "    ax.set_xticks(range(n_tokens))\n",
    "    ax.set_yticks(range(n_tokens))\n",
    "    \n",
    "    # Mark masked tokens\n",
    "    masked_tokens = tokens.copy()\n",
    "    for pos in masked_positions:\n",
    "        masked_tokens[pos] = f\"[MASK]\"\n",
    "    ax.set_xticklabels(masked_tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    \n",
    "    # 3. Prefix Language Modeling\n",
    "    ax = axes[1, 0]\n",
    "    prefix_len = 4\n",
    "    prefix_mask = np.zeros((n_tokens, n_tokens))\n",
    "    # Prefix can attend bidirectionally\n",
    "    prefix_mask[:prefix_len, :prefix_len] = 1\n",
    "    # Rest is causal\n",
    "    prefix_mask[prefix_len:, :] = np.tril(np.ones((n_tokens-prefix_len, n_tokens)))\n",
    "    \n",
    "    im3 = ax.imshow(prefix_mask, cmap='Greens', aspect='equal')\n",
    "    ax.set_title('Prefix Language Modeling (T5-style)', fontsize=14)\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Can attend to')\n",
    "    ax.set_xticks(range(n_tokens))\n",
    "    ax.set_yticks(range(n_tokens))\n",
    "    ax.set_xticklabels(tokens, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(tokens)\n",
    "    ax.axvline(x=prefix_len-0.5, color='red', linestyle='--', lw=2)\n",
    "    ax.axhline(y=prefix_len-0.5, color='red', linestyle='--', lw=2)\n",
    "    \n",
    "    # 4. Span Corruption (T5)\n",
    "    ax = axes[1, 1]\n",
    "    # Show original and corrupted sequences\n",
    "    original = \"The quick brown fox jumps over the lazy dog\"\n",
    "    corrupted = \"The quick <X> jumps over <Y> dog\"\n",
    "    target = \"<X> brown fox <Y> the lazy\"\n",
    "    \n",
    "    ax.text(0.5, 0.8, \"Original:\", transform=ax.transAxes, fontsize=12, ha='center')\n",
    "    ax.text(0.5, 0.7, original, transform=ax.transAxes, fontsize=10, ha='center')\n",
    "    \n",
    "    ax.text(0.5, 0.5, \"Input:\", transform=ax.transAxes, fontsize=12, ha='center')\n",
    "    ax.text(0.5, 0.4, corrupted, transform=ax.transAxes, fontsize=10, ha='center', color='blue')\n",
    "    \n",
    "    ax.text(0.5, 0.2, \"Target:\", transform=ax.transAxes, fontsize=12, ha='center')\n",
    "    ax.text(0.5, 0.1, target, transform=ax.transAxes, fontsize=10, ha='center', color='red')\n",
    "    \n",
    "    ax.set_title('Span Corruption (T5)', fontsize=14)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_training_objectives()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Learning Rate Scheduling\n",
    "\n",
    "Proper learning rate scheduling is critical for stable training. Let's explore different schedules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lr_schedules(num_steps: int = 10000, warmup_steps: int = 1000):\n",
    "    \"\"\"Plot different learning rate schedules.\"\"\"\n",
    "    \n",
    "    steps = np.arange(num_steps)\n",
    "    \n",
    "    # Linear warmup + Cosine decay\n",
    "    def cosine_schedule(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        progress = (step - warmup_steps) / (num_steps - warmup_steps)\n",
    "        return 0.5 * (1 + np.cos(np.pi * progress))\n",
    "    \n",
    "    # Linear warmup + Linear decay\n",
    "    def linear_schedule(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        return 1 - (step - warmup_steps) / (num_steps - warmup_steps)\n",
    "    \n",
    "    # Linear warmup + Inverse sqrt\n",
    "    def inverse_sqrt_schedule(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        return 1 / np.sqrt((step - warmup_steps + 1) / warmup_steps)\n",
    "    \n",
    "    # Linear warmup + Constant\n",
    "    def constant_schedule(step):\n",
    "        if step < warmup_steps:\n",
    "            return step / warmup_steps\n",
    "        return 1.0\n",
    "    \n",
    "    schedules = {\n",
    "        'Cosine': [cosine_schedule(s) for s in steps],\n",
    "        'Linear': [linear_schedule(s) for s in steps],\n",
    "        'Inverse Sqrt': [inverse_sqrt_schedule(s) for s in steps],\n",
    "        'Constant': [constant_schedule(s) for s in steps]\n",
    "    }\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for name, schedule in schedules.items():\n",
    "        plt.plot(steps, schedule, label=name, linewidth=2)\n",
    "    \n",
    "    plt.axvline(x=warmup_steps, color='red', linestyle='--', alpha=0.5, label='End of warmup')\n",
    "    plt.xlabel('Training Step', fontsize=12)\n",
    "    plt.ylabel('Learning Rate Multiplier', fontsize=12)\n",
    "    plt.title('Learning Rate Schedules Comparison', fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations\n",
    "    plt.annotate('Warmup Phase', xy=(warmup_steps/2, 0.5), xytext=(warmup_steps/2, 0.3),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'),\n",
    "                ha='center', fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Show schedule recommendations\n",
    "    recommendations = pd.DataFrame({\n",
    "        'Schedule': ['Cosine', 'Linear', 'Inverse Sqrt', 'Constant'],\n",
    "        'Best For': [\n",
    "            'Fixed-length training, smooth decay',\n",
    "            'Simple, predictable decay',\n",
    "            'Continued training, no fixed end',\n",
    "            'Fine-tuning, transfer learning'\n",
    "        ],\n",
    "        'Pros': [\n",
    "            'Smooth, proven effective',\n",
    "            'Easy to reason about',\n",
    "            'Good for long training',\n",
    "            'Stable, no decay'\n",
    "        ],\n",
    "        'Cons': [\n",
    "            'Requires known total steps',\n",
    "            'Can be too aggressive',\n",
    "            'Never reaches zero',\n",
    "            'No adaptation'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nLearning Rate Schedule Recommendations:\")\n",
    "    display(recommendations)\n",
    "\n",
    "plot_lr_schedules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distributed Training Strategies\n",
    "\n",
    "Large models require distributed training. Let's visualize different parallelism strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_parallelism_strategies():\n",
    "    \"\"\"Visualize different distributed training strategies.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Data Parallelism\n",
    "    ax = axes[0, 0]\n",
    "    n_gpus = 4\n",
    "    \n",
    "    # Draw GPUs\n",
    "    for i in range(n_gpus):\n",
    "        rect = plt.Rectangle((i*2, 0), 1.5, 3, fill=True, color='lightblue', ec='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i*2 + 0.75, 1.5, f'GPU {i}\\nFull Model', ha='center', va='center')\n",
    "        \n",
    "        # Data shards\n",
    "        data_rect = plt.Rectangle((i*2, 3.5), 1.5, 1, fill=True, color='lightgreen', ec='black')\n",
    "        ax.add_patch(data_rect)\n",
    "        ax.text(i*2 + 0.75, 4, f'Data\\nShard {i}', ha='center', va='center', fontsize=9)\n",
    "    \n",
    "    ax.set_xlim(-0.5, n_gpus*2)\n",
    "    ax.set_ylim(-0.5, 5)\n",
    "    ax.set_title('Data Parallelism (DDP)', fontsize=14)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 2. Model Parallelism\n",
    "    ax = axes[0, 1]\n",
    "    layers_per_gpu = 2\n",
    "    \n",
    "    for i in range(n_gpus):\n",
    "        rect = plt.Rectangle((i*2, 0), 1.5, 3, fill=True, color='lightcoral', ec='black')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(i*2 + 0.75, 1.5, f'GPU {i}\\nLayers\\n{i*layers_per_gpu}-{(i+1)*layers_per_gpu-1}', \n",
    "                ha='center', va='center')\n",
    "        \n",
    "        # Show data flow\n",
    "        if i < n_gpus - 1:\n",
    "            ax.arrow(i*2 + 1.5, 1.5, 0.4, 0, head_width=0.2, head_length=0.1, fc='black')\n",
    "    \n",
    "    ax.set_xlim(-0.5, n_gpus*2)\n",
    "    ax.set_ylim(-0.5, 5)\n",
    "    ax.set_title('Model Parallelism', fontsize=14)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # 3. Pipeline Parallelism\n",
    "    ax = axes[1, 0]\n",
    "    micro_batches = 4\n",
    "    \n",
    "    # Create pipeline schedule\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, micro_batches))\n",
    "    \n",
    "    for gpu in range(n_gpus):\n",
    "        for mb in range(micro_batches):\n",
    "            # Forward pass\n",
    "            start_time = mb + gpu\n",
    "            rect = plt.Rectangle((start_time, gpu), 1, 0.4, \n",
    "                               fill=True, color=colors[mb], ec='black', alpha=0.7)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(start_time + 0.5, gpu + 0.2, f'F{mb}', ha='center', va='center', fontsize=8)\n",
    "            \n",
    "            # Backward pass\n",
    "            back_time = start_time + n_gpus + micro_batches - 2*gpu - 1\n",
    "            rect = plt.Rectangle((back_time, gpu), 1, 0.4, \n",
    "                               fill=True, color=colors[mb], ec='black', alpha=0.4)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(back_time + 0.5, gpu + 0.2, f'B{mb}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    ax.set_xlim(-0.5, 12)\n",
    "    ax.set_ylim(-0.5, n_gpus)\n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel('GPU', fontsize=12)\n",
    "    ax.set_title('Pipeline Parallelism Schedule', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. 3D Parallelism Comparison\n",
    "    ax = axes[1, 1]\n",
    "    \n",
    "    strategies = ['Data\\nParallel', 'Model\\nParallel', 'Pipeline\\nParallel', '3D\\nParallel']\n",
    "    memory_per_gpu = [100, 25, 30, 10]\n",
    "    communication = [50, 100, 75, 85]\n",
    "    \n",
    "    x = np.arange(len(strategies))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, memory_per_gpu, width, label='Memory per GPU (%)')\n",
    "    bars2 = ax.bar(x + width/2, communication, width, label='Communication (%)')\n",
    "    \n",
    "    ax.set_ylabel('Relative Scale', fontsize=12)\n",
    "    ax.set_title('Parallelism Strategy Comparison', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(strategies)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_parallelism_strategies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training Monitoring and Debugging\n",
    "\n",
    "Monitoring is crucial for successful pretraining. Let's simulate a training run and visualize key metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_training_metrics(num_steps: int = 5000):\n",
    "    \"\"\"Simulate training metrics for visualization.\"\"\"\n",
    "    \n",
    "    steps = np.arange(num_steps)\n",
    "    \n",
    "    # Simulate loss with some noise and anomalies\n",
    "    base_loss = 10 * np.exp(-steps / 1000) + 2\n",
    "    noise = np.random.normal(0, 0.1, num_steps)\n",
    "    loss = base_loss + noise\n",
    "    \n",
    "    # Add some anomalies\n",
    "    anomaly_steps = [1000, 2500, 4000]\n",
    "    for step in anomaly_steps:\n",
    "        loss[step:step+50] += np.random.uniform(1, 3)\n",
    "    \n",
    "    # Gradient norm\n",
    "    grad_norm = 2 * np.exp(-steps / 2000) + np.random.normal(0.5, 0.2, num_steps)\n",
    "    grad_norm[grad_norm < 0] = 0.1\n",
    "    \n",
    "    # Learning rate (cosine schedule)\n",
    "    warmup = 500\n",
    "    lr = np.zeros(num_steps)\n",
    "    lr[:warmup] = np.linspace(0, 1, warmup)\n",
    "    lr[warmup:] = 0.5 * (1 + np.cos(np.pi * (steps[warmup:] - warmup) / (num_steps - warmup)))\n",
    "    lr *= 3e-4  # Scale to actual LR\n",
    "    \n",
    "    # GPU memory and utilization\n",
    "    gpu_memory = 70 + 10 * np.sin(steps / 100) + np.random.normal(0, 2, num_steps)\n",
    "    gpu_util = 85 + 10 * np.sin(steps / 150 + 1) + np.random.normal(0, 3, num_steps)\n",
    "    \n",
    "    return {\n",
    "        'steps': steps,\n",
    "        'loss': loss,\n",
    "        'grad_norm': grad_norm,\n",
    "        'learning_rate': lr,\n",
    "        'gpu_memory': np.clip(gpu_memory, 0, 100),\n",
    "        'gpu_utilization': np.clip(gpu_util, 0, 100)\n",
    "    }\n",
    "\n",
    "# Simulate metrics\n",
    "metrics = simulate_training_metrics()\n",
    "\n",
    "# Create dashboard\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.plot(metrics['steps'], metrics['loss'], 'b-', alpha=0.7, linewidth=1)\n",
    "ax1.set_ylabel('Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss with Anomaly Detection', fontsize=14)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Mark anomalies\n",
    "anomaly_mask = metrics['loss'] > np.percentile(metrics['loss'], 95)\n",
    "ax1.scatter(metrics['steps'][anomaly_mask], metrics['loss'][anomaly_mask], \n",
    "           color='red', s=20, label='Anomalies')\n",
    "ax1.legend()\n",
    "\n",
    "# Gradient norm\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.plot(metrics['steps'], metrics['grad_norm'], 'g-', alpha=0.7)\n",
    "ax2.axhline(y=5, color='red', linestyle='--', label='Clip threshold')\n",
    "ax2.set_ylabel('Gradient Norm', fontsize=12)\n",
    "ax2.set_xlabel('Step', fontsize=12)\n",
    "ax2.set_title('Gradient Norm', fontsize=14)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "# Learning rate\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.plot(metrics['steps'], metrics['learning_rate'], 'orange', alpha=0.7)\n",
    "ax3.set_ylabel('Learning Rate', fontsize=12)\n",
    "ax3.set_xlabel('Step', fontsize=12)\n",
    "ax3.set_title('Learning Rate Schedule', fontsize=14)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# GPU metrics\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "ax4.plot(metrics['steps'], metrics['gpu_memory'], 'purple', alpha=0.7, label='Memory')\n",
    "ax4.plot(metrics['steps'], metrics['gpu_utilization'], 'brown', alpha=0.7, label='Utilization')\n",
    "ax4.set_ylabel('Percentage (%)', fontsize=12)\n",
    "ax4.set_xlabel('Step', fontsize=12)\n",
    "ax4.set_title('GPU Metrics', fontsize=14)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "ax4.set_ylim(0, 105)\n",
    "\n",
    "# Training speed\n",
    "ax5 = fig.add_subplot(gs[2, :])\n",
    "tokens_per_second = 50000 + 10000 * np.sin(metrics['steps'] / 500) + \\\n",
    "                   np.random.normal(0, 2000, len(metrics['steps']))\n",
    "ax5.plot(metrics['steps'], tokens_per_second, 'teal', alpha=0.7)\n",
    "ax5.set_ylabel('Tokens/Second', fontsize=12)\n",
    "ax5.set_xlabel('Step', fontsize=12)\n",
    "ax5.set_title('Training Throughput', fontsize=14)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# Add mean line\n",
    "mean_throughput = np.mean(tokens_per_second)\n",
    "ax5.axhline(y=mean_throughput, color='red', linestyle='--', \n",
    "           label=f'Mean: {mean_throughput:.0f} tokens/s')\n",
    "ax5.legend()\n",
    "\n",
    "plt.suptitle('LLM Pretraining Monitoring Dashboard', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nTraining Summary Statistics:\")\n",
    "print(f\"Final Loss: {metrics['loss'][-1]:.4f}\")\n",
    "print(f\"Average Gradient Norm: {np.mean(metrics['grad_norm']):.4f}\")\n",
    "print(f\"Number of Anomalies Detected: {np.sum(anomaly_mask)}\")\n",
    "print(f\"Average GPU Memory Usage: {np.mean(metrics['gpu_memory']):.1f}%\")\n",
    "print(f\"Average Training Speed: {mean_throughput:.0f} tokens/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cost Estimation and Optimization\n",
    "\n",
    "Understanding and optimizing training costs is crucial for large-scale pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_training_costs(model_params: int, training_tokens: int, \n",
    "                           gpu_type: str = 'A100') -> Dict:\n",
    "    \"\"\"Calculate estimated training costs for different configurations.\"\"\"\n",
    "    \n",
    "    # GPU specifications\n",
    "    gpu_specs = {\n",
    "        'V100': {'tflops': 125, 'memory': 32, 'cost_per_hour': 1.5},\n",
    "        'A100': {'tflops': 312, 'memory': 80, 'cost_per_hour': 2.5},\n",
    "        'H100': {'tflops': 1000, 'memory': 80, 'cost_per_hour': 5.0}\n",
    "    }\n",
    "    \n",
    "    # Estimate FLOPs (6 * params * tokens for transformer training)\n",
    "    total_flops = 6 * model_params * training_tokens\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for gpu_name, specs in gpu_specs.items():\n",
    "        # Model flops utilization (MFU) - typically 30-50% for real training\n",
    "        mfu = 0.35\n",
    "        effective_tflops = specs['tflops'] * mfu\n",
    "        \n",
    "        # Calculate time\n",
    "        gpu_seconds = total_flops / (effective_tflops * 1e12)\n",
    "        gpu_hours = gpu_seconds / 3600\n",
    "        \n",
    "        # Number of GPUs needed (based on memory)\n",
    "        model_memory_gb = model_params * 18 / 1e9  # Rough estimate\n",
    "        gpus_needed = max(1, int(np.ceil(model_memory_gb / specs['memory'])))\n",
    "        \n",
    "        # Total cost\n",
    "        total_cost = gpu_hours * specs['cost_per_hour'] * gpus_needed\n",
    "        \n",
    "        results[gpu_name] = {\n",
    "            'gpu_hours': gpu_hours,\n",
    "            'gpus_needed': gpus_needed,\n",
    "            'total_hours': gpu_hours / gpus_needed,\n",
    "            'total_cost': total_cost,\n",
    "            'cost_per_million_params': total_cost / (model_params / 1e6)\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate costs for different model sizes\n",
    "model_configs = [\n",
    "    {'name': '1B Model', 'params': 1e9, 'tokens': 20e9},\n",
    "    {'name': '7B Model', 'params': 7e9, 'tokens': 1e12},\n",
    "    {'name': '13B Model', 'params': 13e9, 'tokens': 1e12},\n",
    "    {'name': '70B Model', 'params': 70e9, 'tokens': 1.4e12},\n",
    "]\n",
    "\n",
    "cost_data = []\n",
    "\n",
    "for config in model_configs:\n",
    "    costs = calculate_training_costs(config['params'], config['tokens'])\n",
    "    \n",
    "    for gpu_type, cost_info in costs.items():\n",
    "        cost_data.append({\n",
    "            'Model': config['name'],\n",
    "            'GPU Type': gpu_type,\n",
    "            'GPU Hours': cost_info['gpu_hours'],\n",
    "            'GPUs Needed': cost_info['gpus_needed'],\n",
    "            'Total Cost ($)': cost_info['total_cost']\n",
    "        })\n",
    "\n",
    "df_costs = pd.DataFrame(cost_data)\n",
    "\n",
    "# Visualize costs\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Cost by model size and GPU type\n",
    "pivot_costs = df_costs.pivot(index='Model', columns='GPU Type', values='Total Cost ($)')\n",
    "pivot_costs.plot(kind='bar', ax=ax1)\n",
    "ax1.set_ylabel('Total Cost ($)', fontsize=12)\n",
    "ax1.set_title('Training Cost by Model Size and GPU Type', fontsize=14)\n",
    "ax1.set_yscale('log')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "ax1.legend(title='GPU Type')\n",
    "\n",
    "# Time vs Cost tradeoff\n",
    "for model in ['7B Model', '70B Model']:\n",
    "    model_data = df_costs[df_costs['Model'] == model]\n",
    "    ax2.scatter(model_data['GPU Hours'] / model_data['GPUs Needed'], \n",
    "               model_data['Total Cost ($)'],\n",
    "               label=model, s=100)\n",
    "    \n",
    "    # Add GPU type labels\n",
    "    for _, row in model_data.iterrows():\n",
    "        ax2.annotate(row['GPU Type'], \n",
    "                    (row['GPU Hours'] / row['GPUs Needed'], row['Total Cost ($)']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "ax2.set_xlabel('Wall Clock Time (Hours)', fontsize=12)\n",
    "ax2.set_ylabel('Total Cost ($)', fontsize=12)\n",
    "ax2.set_title('Time vs Cost Tradeoff', fontsize=14)\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cost optimization strategies\n",
    "print(\"\\nCost Optimization Strategies:\")\n",
    "optimization_strategies = pd.DataFrame({\n",
    "    'Strategy': [\n",
    "        'Mixed Precision Training',\n",
    "        'Gradient Checkpointing',\n",
    "        'Efficient Attention',\n",
    "        'Data Packing',\n",
    "        'Learning Rate Tuning',\n",
    "        'Spot Instances'\n",
    "    ],\n",
    "    'Cost Reduction': ['20-30%', '15-25%', '10-20%', '5-10%', '10-15%', '60-70%'],\n",
    "    'Trade-off': [\n",
    "        'Minimal quality impact',\n",
    "        'Slower training',\n",
    "        'Implementation complexity',\n",
    "        'Slightly less randomness',\n",
    "        'Requires experimentation',\n",
    "        'Can be interrupted'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(optimization_strategies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary and Best Practices\n",
    "\n",
    "Let's summarize the key takeaways for successful LLM pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive pretraining checklist\n",
    "checklist = {\n",
    "    'Stage': [\n",
    "        '1. Data Preparation',\n",
    "        '2. Model Architecture',\n",
    "        '3. Training Setup',\n",
    "        '4. Distributed Strategy',\n",
    "        '5. Monitoring',\n",
    "        '6. Checkpointing',\n",
    "        '7. Evaluation'\n",
    "    ],\n",
    "    'Key Tasks': [\n",
    "        'Collect, filter, deduplicate data',\n",
    "        'Choose architecture, size, config',\n",
    "        'Set hyperparameters, schedule',\n",
    "        'Select parallelism strategy',\n",
    "        'Setup logging, alerts, dashboards',\n",
    "        'Regular saves, test recovery',\n",
    "        'Validation loss, benchmarks'\n",
    "    ],\n",
    "    'Critical Considerations': [\n",
    "        'Quality > Quantity, diversity matters',\n",
    "        'Memory requirements, efficiency',\n",
    "        'Learning rate, warmup crucial',\n",
    "        'Communication overhead, scaling',\n",
    "        'Catch issues early, track everything',\n",
    "        'Failure recovery, reproducibility',\n",
    "        'Downstream task performance'\n",
    "    ],\n",
    "    'Common Pitfalls': [\n",
    "        'Low quality or biased data',\n",
    "        'Model too large for hardware',\n",
    "        'Unstable training, divergence',\n",
    "        'Inefficient parallelization',\n",
    "        'Missing critical failures',\n",
    "        'Lost work from crashes',\n",
    "        'Overfitting to validation'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_checklist = pd.DataFrame(checklist)\n",
    "\n",
    "print(\"🚀 LLM Pretraining Checklist\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for _, row in df_checklist.iterrows():\n",
    "    print(f\"\\n{row['Stage']}\")\n",
    "    print(f\"  ✓ Tasks: {row['Key Tasks']}\")\n",
    "    print(f\"  ⚠️  Consider: {row['Critical Considerations']}\")\n",
    "    print(f\"  ❌ Avoid: {row['Common Pitfalls']}\")\n",
    "\n",
    "# Key metrics to track\n",
    "print(\"\\n\\n📊 Key Metrics to Track During Pretraining:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "metrics_table = pd.DataFrame({\n",
    "    'Metric': [\n",
    "        'Loss (train/val)',\n",
    "        'Perplexity',\n",
    "        'Gradient Norm',\n",
    "        'Learning Rate',\n",
    "        'Tokens/Second',\n",
    "        'GPU Utilization',\n",
    "        'Memory Usage'\n",
    "    ],\n",
    "    'Expected Range': [\n",
    "        'Decreasing, gap < 0.5',\n",
    "        '< 10 for good models',\n",
    "        '0.1 - 5.0',\n",
    "        'Following schedule',\n",
    "        '> 10k for efficiency',\n",
    "        '> 80%',\n",
    "        '< 90% of available'\n",
    "    ],\n",
    "    'Warning Signs': [\n",
    "        'Sudden spikes, divergence',\n",
    "        'Not decreasing, > 100',\n",
    "        '> 10 or near 0',\n",
    "        'Not changing as expected',\n",
    "        'Decreasing over time',\n",
    "        '< 50%',\n",
    "        'OOM errors'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(metrics_table)\n",
    "\n",
    "print(\"\\n\\n✅ Pretraining Pipeline Complete!\")\n",
    "print(\"\\nRemember: Successful pretraining requires patience, careful monitoring, and robust infrastructure.\")\n",
    "print(\"Start small, validate your pipeline, then scale up!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
