{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Transformer from Scratch\n",
    "\n",
    "Welcome to Topic 5! In this notebook, we'll build a complete transformer model from scratch, implementing every component ourselves. This hands-on approach will solidify your understanding of how transformers work.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement a transformer from the ground up\n",
    "- Understand every component in detail\n",
    "- Train a small transformer on a toy task\n",
    "- Debug and visualize the training process\n",
    "- Gain practical implementation experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building Blocks: Scaled Dot-Product Attention\n",
    "\n",
    "Let's start with the fundamental building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query: torch.Tensor, \n",
    "                               key: torch.Tensor, \n",
    "                               value: torch.Tensor,\n",
    "                               mask: Optional[torch.Tensor] = None,\n",
    "                               dropout: Optional[nn.Dropout] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        query: [batch_size, n_heads, seq_len, d_k]\n",
    "        key: [batch_size, n_heads, seq_len, d_k]\n",
    "        value: [batch_size, n_heads, seq_len, d_v]\n",
    "        mask: [batch_size, 1, 1, seq_len] or [batch_size, 1, seq_len, seq_len]\n",
    "        dropout: Dropout layer\n",
    "    \n",
    "    Returns:\n",
    "        output: [batch_size, n_heads, seq_len, d_v]\n",
    "        attention_weights: [batch_size, n_heads, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    \n",
    "    # Apply softmax\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    \n",
    "    # Apply dropout if provided\n",
    "    if dropout is not None:\n",
    "        attention_weights = dropout(attention_weights)\n",
    "    \n",
    "    # Apply attention to values\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "batch_size = 2\n",
    "n_heads = 8\n",
    "seq_len = 10\n",
    "d_k = 64\n",
    "\n",
    "# Create random tensors\n",
    "query = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "key = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "value = torch.randn(batch_size, n_heads, seq_len, d_k)\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention_weights[0, 0].detach().numpy(), cmap='Blues', cbar=True)\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights Visualization')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Head Attention Implementation\n",
    "\n",
    "Now let's build the multi-head attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        # Xavier initialization\n",
    "        for module in [self.W_q, self.W_k, self.W_v, self.W_o]:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,\n",
    "                mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        batch_size = query.size(0)\n",
    "        seq_len = query.size(1)\n",
    "        \n",
    "        # Project and reshape to [batch_size, n_heads, seq_len, d_k]\n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask, self.dropout\n",
    "        )\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attention_output = attention_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = self.W_o(attention_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "mha = MultiHeadAttention(d_model, n_heads)\n",
    "\n",
    "# Create input\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output, attn_weights = mha(x, x, x)  # Self-attention\n",
    "\n",
    "print(f\"Multi-Head Attention:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in mha.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Position-wise Feed-Forward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.GELU()  # Using GELU instead of ReLU (modern choice)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Test feed-forward network\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "x = torch.randn(2, 10, d_model)\n",
    "output = ffn(x)\n",
    "\n",
    "print(f\"Feed-Forward Network:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in ffn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Create positional encoding matrix\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        \n",
    "        # Create div_term for sinusoidal pattern\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           -(math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Apply sine to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices  \n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # Register as buffer (not a parameter)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add positional encoding\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "# Visualize positional encoding\n",
    "d_model = 128\n",
    "max_len = 100\n",
    "pe_layer = PositionalEncoding(d_model, max_len, dropout=0)\n",
    "\n",
    "# Get positional encodings\n",
    "pe_matrix = pe_layer.pe[0, :, :].numpy()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pe_matrix.T, aspect='auto', cmap='RdBu')\n",
    "plt.colorbar(label='Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding Visualization')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot specific dimensions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "for i, (ax, dim) in enumerate(zip(axes.flat, [0, 1, 20, 21])):\n",
    "    ax.plot(pe_matrix[:, dim])\n",
    "    ax.set_title(f'Dimension {dim}')\n",
    "    ax.set_xlabel('Position')\n",
    "    ax.set_ylabel('Encoding Value')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self-attention block\n",
    "        attn_output, _ = self.self_attention(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, encoder_output: torch.Tensor,\n",
    "                tgt_mask: Optional[torch.Tensor] = None,\n",
    "                src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # Self-attention block (masked)\n",
    "        attn_output, _ = self.self_attention(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Cross-attention block\n",
    "        attn_output, _ = self.cross_attention(x, encoder_output, encoder_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        \n",
    "        # Feed-forward block\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 src_vocab_size: int,\n",
    "                 tgt_vocab_size: int,\n",
    "                 d_model: int = 512,\n",
    "                 n_heads: int = 8,\n",
    "                 num_encoder_layers: int = 6,\n",
    "                 num_decoder_layers: int = 6,\n",
    "                 d_ff: int = 2048,\n",
    "                 max_seq_length: int = 100,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length, dropout)\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_encoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final linear layer\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def generate_mask(self, src: torch.Tensor, tgt: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask.to(tgt.device)\n",
    "        return src_mask, tgt_mask\n",
    "    \n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        # Encoder\n",
    "        src_embedded = self.dropout(self.encoder_embedding(src) * math.sqrt(self.encoder_embedding.embedding_dim))\n",
    "        src_embedded = self.positional_encoding(src_embedded)\n",
    "        \n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "            \n",
    "        # Decoder\n",
    "        tgt_embedded = self.dropout(self.decoder_embedding(tgt) * math.sqrt(self.decoder_embedding.embedding_dim))\n",
    "        tgt_embedded = self.positional_encoding(tgt_embedded)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, tgt_mask, src_mask)\n",
    "            \n",
    "        # Final projection\n",
    "        output = self.fc_out(dec_output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create model\n",
    "model = Transformer(\n",
    "    src_vocab_size=1000,\n",
    "    tgt_vocab_size=1000,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    num_encoder_layers=3,\n",
    "    num_decoder_layers=3,\n",
    "    d_ff=1024,\n",
    "    max_seq_length=100,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "\n",
    "# Test forward pass\n",
    "src = torch.randint(1, 1000, (2, 10)).to(device)\n",
    "tgt = torch.randint(1, 1000, (2, 8)).to(device)\n",
    "output = model(src, tgt)\n",
    "print(f\"\\nForward pass:\")\n",
    "print(f\"Source shape: {src.shape}\")\n",
    "print(f\"Target shape: {tgt.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Training on a Toy Task: Number Sorting\n",
    "\n",
    "Let's train our transformer on a simple task to verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy dataset: sorting sequences of numbers\n",
    "class SortingDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, seq_length: int, vocab_size: int):\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Generate data\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            # Generate random sequence (excluding 0 which is padding)\n",
    "            seq = torch.randint(1, vocab_size, (seq_length,))\n",
    "            sorted_seq = torch.sort(seq)[0]\n",
    "            self.data.append((seq, sorted_seq))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Create dataset\n",
    "train_dataset = SortingDataset(1000, seq_length=8, vocab_size=50)\n",
    "val_dataset = SortingDataset(200, seq_length=8, vocab_size=50)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Show example\n",
    "src_example, tgt_example = train_dataset[0]\n",
    "print(f\"Example:\")\n",
    "print(f\"Input:  {src_example.tolist()}\")\n",
    "print(f\"Target: {tgt_example.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create smaller model for toy task\n",
    "model = Transformer(\n",
    "    src_vocab_size=50,\n",
    "    tgt_vocab_size=50,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    num_encoder_layers=2,\n",
    "    num_decoder_layers=2,\n",
    "    d_ff=512,\n",
    "    max_seq_length=20,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(step, d_model, warmup_steps):\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return d_model ** (-0.5) * min(step ** (-0.5), step * warmup_steps ** (-1.5))\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lambda step: get_lr(step, model.encoder_embedding.embedding_dim, warmup_steps=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, data_loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    progress_bar = tqdm(data_loader, desc='Training')\n",
    "    for src, tgt in progress_bar:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        # Teacher forcing: use ground truth as decoder input\n",
    "        tgt_input = tgt\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(src, tgt_input)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt.reshape(-1))\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = output.argmax(dim=-1)\n",
    "        correct = (predictions == tgt).sum().item()\n",
    "        tokens = tgt.numel()\n",
    "        \n",
    "        total_loss += loss.item() * tokens\n",
    "        total_correct += correct\n",
    "        total_tokens += tokens\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{correct/tokens:.2%}'\n",
    "        })\n",
    "    \n",
    "    return total_loss / total_tokens, total_correct / total_tokens\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for src, tgt in data_loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        \n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), tgt.reshape(-1))\n",
    "        \n",
    "        predictions = output.argmax(dim=-1)\n",
    "        correct = (predictions == tgt).sum().item()\n",
    "        tokens = tgt.numel()\n",
    "        \n",
    "        total_loss += loss.item() * tokens\n",
    "        total_correct += correct\n",
    "        total_tokens += tokens\n",
    "    \n",
    "    return total_loss / total_tokens, total_correct / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 10\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, scheduler)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2%}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2%}\")\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Acc')\n",
    "ax2.plot(val_accs, label='Val Acc')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Inference and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def greedy_decode(model, src, max_length=20):\n",
    "    \"\"\"Greedy decoding for sequence generation.\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Start with zeros (we'll overwrite with actual predictions)\n",
    "    tgt = torch.zeros(1, max_length, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Get encoder output once\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    src_embedded = model.encoder_embedding(src) * math.sqrt(model.encoder_embedding.embedding_dim)\n",
    "    src_embedded = model.positional_encoding(src_embedded)\n",
    "    \n",
    "    enc_output = src_embedded\n",
    "    for enc_layer in model.encoder_layers:\n",
    "        enc_output = enc_layer(enc_output, src_mask)\n",
    "    \n",
    "    # Generate tokens one by one\n",
    "    for i in range(max_length):\n",
    "        # Create mask\n",
    "        tgt_mask = torch.triu(torch.ones(i+1, i+1), diagonal=1).bool().to(device)\n",
    "        tgt_mask = ~tgt_mask\n",
    "        tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Decode\n",
    "        tgt_embedded = model.decoder_embedding(tgt[:, :i+1]) * math.sqrt(model.decoder_embedding.embedding_dim)\n",
    "        tgt_embedded = model.positional_encoding(tgt_embedded)\n",
    "        \n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in model.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, tgt_mask, src_mask)\n",
    "        \n",
    "        # Get next token\n",
    "        output = model.fc_out(dec_output)\n",
    "        next_token = output[0, i].argmax()\n",
    "        \n",
    "        if i < max_length - 1:\n",
    "            tgt[0, i+1] = next_token\n",
    "            \n",
    "        # Stop if we predict padding\n",
    "        if next_token == 0:\n",
    "            break\n",
    "    \n",
    "    return tgt[0, 1:i+1]  # Return without the initial zero\n",
    "\n",
    "# Test on some examples\n",
    "print(\"Testing model predictions:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in range(5):\n",
    "    src, tgt = val_dataset[i]\n",
    "    src_tensor = src.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get prediction\n",
    "    pred = greedy_decode(model, src_tensor, max_length=len(tgt))\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Input:      {src.tolist()}\")\n",
    "    print(f\"Target:     {tgt.tolist()}\")\n",
    "    print(f\"Prediction: {pred.tolist()}\")\n",
    "    print(f\"Correct:    {torch.equal(pred[:len(tgt)], tgt.to(device))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention weights from the model\n",
    "def get_attention_weights(model, src, tgt):\n",
    "    \"\"\"Extract attention weights from all layers.\"\"\"\n",
    "    model.eval()\n",
    "    attention_weights = {'encoder': [], 'decoder': [], 'cross': []}\n",
    "    \n",
    "    # Hook functions to capture attention weights\n",
    "    def make_hook(name, layer_idx):\n",
    "        def hook(module, input, output):\n",
    "            if hasattr(module, 'self_attention'):\n",
    "                attention_weights[name].append(output[1])\n",
    "        return hook\n",
    "    \n",
    "    # Register hooks\n",
    "    handles = []\n",
    "    for i, layer in enumerate(model.encoder_layers):\n",
    "        handle = layer.self_attention.register_forward_hook(make_hook('encoder', i))\n",
    "        handles.append(handle)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(src, tgt)\n",
    "    \n",
    "    # Remove hooks\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "    \n",
    "    return attention_weights\n",
    "\n",
    "# Note: This is a simplified version. In practice, you'd need to modify the model\n",
    "# to return attention weights properly.\n",
    "\n",
    "print(\"Attention visualization would show:\")\n",
    "print(\"1. Self-attention patterns in encoder\")\n",
    "print(\"2. Masked self-attention in decoder\")\n",
    "print(\"3. Cross-attention between encoder and decoder\")\n",
    "print(\"\\nThis helps understand what the model is 'looking at' during processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've built a complete transformer from scratch:\n",
    "\n",
    "1. **Scaled Dot-Product Attention**: The core attention mechanism\n",
    "2. **Multi-Head Attention**: Parallel attention with different representations\n",
    "3. **Feed-Forward Networks**: Position-wise transformations\n",
    "4. **Positional Encoding**: Adding position information\n",
    "5. **Encoder/Decoder Layers**: Complete transformer blocks\n",
    "6. **Full Model**: Putting everything together\n",
    "7. **Training**: Successfully trained on a toy sorting task\n",
    "8. **Inference**: Generated sequences using the trained model\n",
    "\n",
    "Key takeaways:\n",
    "- Transformers are built from simple, composable blocks\n",
    "- Attention is the key mechanism enabling long-range dependencies\n",
    "- Residual connections and layer normalization are crucial for training\n",
    "- The architecture is highly parallelizable\n",
    "\n",
    "Next, we'll explore tokenization and embeddings in detail!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}