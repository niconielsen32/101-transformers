{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference at Scale: Interactive Tutorial\n",
    "\n",
    "This notebook provides hands-on experience with serving transformer models at scale, including batching strategies, optimization techniques, and production deployment patterns.\n",
    "\n",
    "## ðŸ“‹ Learning Objectives\n",
    "\n",
    "- **Understand** production serving architectures\n",
    "- **Implement** different batching strategies\n",
    "- **Apply** caching and memory optimization\n",
    "- **Design** load balancing and auto-scaling\n",
    "- **Deploy** models for edge and cloud environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import asyncio\n",
    "import threading\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Production Serving Fundamentals\n",
    "\n",
    "Let's start by understanding the key metrics and challenges in serving models at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionMetrics:\n",
    "    \"\"\"Track and visualize production serving metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'latency': [],\n",
    "            'throughput': [],\n",
    "            'memory_usage': [],\n",
    "            'cpu_usage': [],\n",
    "            'error_rate': []\n",
    "        }\n",
    "        \n",
    "    def record(self, metric_name: str, value: float):\n",
    "        \"\"\"Record a metric value.\"\"\"\n",
    "        if metric_name in self.metrics:\n",
    "            self.metrics[metric_name].append(value)\n",
    "            \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get summary statistics.\"\"\"\n",
    "        summary = {}\n",
    "        for name, values in self.metrics.items():\n",
    "            if values:\n",
    "                summary[name] = {\n",
    "                    'mean': np.mean(values),\n",
    "                    'p50': np.percentile(values, 50),\n",
    "                    'p95': np.percentile(values, 95),\n",
    "                    'p99': np.percentile(values, 99)\n",
    "                }\n",
    "        return summary\n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        \"\"\"Plot key metrics.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        \n",
    "        # Latency histogram\n",
    "        if self.metrics['latency']:\n",
    "            axes[0, 0].hist(self.metrics['latency'], bins=30, alpha=0.7)\n",
    "            axes[0, 0].set_title('Latency Distribution')\n",
    "            axes[0, 0].set_xlabel('Latency (ms)')\n",
    "            axes[0, 0].set_ylabel('Frequency')\n",
    "            \n",
    "        # Throughput over time\n",
    "        if self.metrics['throughput']:\n",
    "            axes[0, 1].plot(self.metrics['throughput'])\n",
    "            axes[0, 1].set_title('Throughput Over Time')\n",
    "            axes[0, 1].set_xlabel('Time Step')\n",
    "            axes[0, 1].set_ylabel('Requests/sec')\n",
    "            \n",
    "        # Memory usage\n",
    "        if self.metrics['memory_usage']:\n",
    "            axes[0, 2].plot(self.metrics['memory_usage'], color='red')\n",
    "            axes[0, 2].set_title('Memory Usage')\n",
    "            axes[0, 2].set_xlabel('Time Step')\n",
    "            axes[0, 2].set_ylabel('Memory (GB)')\n",
    "            \n",
    "        # CPU usage\n",
    "        if self.metrics['cpu_usage']:\n",
    "            axes[1, 0].plot(self.metrics['cpu_usage'], color='green')\n",
    "            axes[1, 0].set_title('CPU Usage')\n",
    "            axes[1, 0].set_xlabel('Time Step')\n",
    "            axes[1, 0].set_ylabel('CPU %')\n",
    "            \n",
    "        # Error rate\n",
    "        if self.metrics['error_rate']:\n",
    "            axes[1, 1].plot(self.metrics['error_rate'], color='orange')\n",
    "            axes[1, 1].set_title('Error Rate')\n",
    "            axes[1, 1].set_xlabel('Time Step')\n",
    "            axes[1, 1].set_ylabel('Error %')\n",
    "            \n",
    "        # SLA compliance\n",
    "        if self.metrics['latency']:\n",
    "            sla_threshold = 100  # 100ms SLA\n",
    "            compliance = [1 if lat <= sla_threshold else 0 for lat in self.metrics['latency']]\n",
    "            axes[1, 2].plot(np.cumsum(compliance) / np.arange(1, len(compliance) + 1) * 100)\n",
    "            axes[1, 2].axhline(y=99.9, color='r', linestyle='--', label='99.9% SLA')\n",
    "            axes[1, 2].set_title('SLA Compliance')\n",
    "            axes[1, 2].set_xlabel('Request')\n",
    "            axes[1, 2].set_ylabel('Compliance %')\n",
    "            axes[1, 2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create metrics tracker\n",
    "metrics = ProductionMetrics()\n",
    "\n",
    "# Simulate production traffic\n",
    "print(\"Simulating production traffic...\")\n",
    "for i in tqdm(range(1000)):\n",
    "    # Simulate varying load\n",
    "    base_latency = 50\n",
    "    load_factor = 1 + np.sin(i / 100) * 0.5  # Periodic load\n",
    "    \n",
    "    latency = base_latency * load_factor + np.random.normal(0, 10)\n",
    "    throughput = 1000 / max(latency, 1)  # Inverse relationship\n",
    "    memory = 4 + np.random.normal(0, 0.5)  # 4GB baseline\n",
    "    cpu = min(95, 30 + latency * 0.5 + np.random.normal(0, 5))\n",
    "    error_rate = max(0, min(10, (latency - 100) * 0.1 + np.random.normal(0, 0.5)))\n",
    "    \n",
    "    metrics.record('latency', max(0, latency))\n",
    "    metrics.record('throughput', max(0, throughput))\n",
    "    metrics.record('memory_usage', max(0, memory))\n",
    "    metrics.record('cpu_usage', max(0, cpu))\n",
    "    metrics.record('error_rate', max(0, error_rate))\n",
    "\n",
    "# Display summary\n",
    "summary = metrics.get_summary()\n",
    "print(\"\\nðŸ“Š Production Metrics Summary:\")\n",
    "for metric, stats in summary.items():\n",
    "    print(f\"\\n{metric.replace('_', ' ').title()}:\")\n",
    "    for stat, value in stats.items():\n",
    "        print(f\"  {stat}: {value:.2f}\")\n",
    "\n",
    "# Plot metrics\n",
    "metrics.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Batching Strategies\n",
    "\n",
    "Batching is crucial for throughput optimization. Let's explore different strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchingComparison:\n",
    "    \"\"\"Compare different batching strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def static_batching(self, requests, batch_size=32):\n",
    "        \"\"\"Process requests with fixed batch size.\"\"\"\n",
    "        batches = []\n",
    "        latencies = []\n",
    "        \n",
    "        for i in range(0, len(requests), batch_size):\n",
    "            batch = requests[i:i + batch_size]\n",
    "            \n",
    "            # Wait for full batch (except last)\n",
    "            if len(batch) < batch_size and i + batch_size < len(requests):\n",
    "                continue\n",
    "                \n",
    "            # Simulate processing time\n",
    "            max_seq_len = max(req['length'] for req in batch)\n",
    "            process_time = max_seq_len * 0.001 + len(batch) * 0.01\n",
    "            \n",
    "            # Calculate latency for each request\n",
    "            for req in batch:\n",
    "                wait_time = req['arrival_time']\n",
    "                total_latency = wait_time + process_time\n",
    "                latencies.append(total_latency)\n",
    "                \n",
    "            batches.append({\n",
    "                'size': len(batch),\n",
    "                'max_length': max_seq_len,\n",
    "                'process_time': process_time\n",
    "            })\n",
    "            \n",
    "        return batches, latencies\n",
    "        \n",
    "    def dynamic_batching(self, requests, max_batch_size=32, max_wait_ms=50):\n",
    "        \"\"\"Process requests with dynamic batching.\"\"\"\n",
    "        batches = []\n",
    "        latencies = []\n",
    "        queue = []\n",
    "        \n",
    "        for req in requests:\n",
    "            queue.append(req)\n",
    "            \n",
    "            # Check if should process batch\n",
    "            should_process = (\n",
    "                len(queue) >= max_batch_size or\n",
    "                (queue and (req['arrival_time'] - queue[0]['arrival_time']) >= max_wait_ms)\n",
    "            )\n",
    "            \n",
    "            if should_process:\n",
    "                batch = queue[:max_batch_size]\n",
    "                queue = queue[max_batch_size:]\n",
    "                \n",
    "                # Process batch\n",
    "                max_seq_len = max(r['length'] for r in batch)\n",
    "                process_time = max_seq_len * 0.001 + len(batch) * 0.01\n",
    "                \n",
    "                for r in batch:\n",
    "                    wait_time = req['arrival_time'] - r['arrival_time']\n",
    "                    total_latency = wait_time + process_time\n",
    "                    latencies.append(total_latency)\n",
    "                    \n",
    "                batches.append({\n",
    "                    'size': len(batch),\n",
    "                    'max_length': max_seq_len,\n",
    "                    'process_time': process_time\n",
    "                })\n",
    "                \n",
    "        # Process remaining requests\n",
    "        if queue:\n",
    "            max_seq_len = max(r['length'] for r in queue)\n",
    "            process_time = max_seq_len * 0.001 + len(queue) * 0.01\n",
    "            \n",
    "            for r in queue:\n",
    "                wait_time = requests[-1]['arrival_time'] - r['arrival_time']\n",
    "                total_latency = wait_time + process_time\n",
    "                latencies.append(total_latency)\n",
    "                \n",
    "        return batches, latencies\n",
    "        \n",
    "    def bucket_batching(self, requests, bucket_sizes=[128, 256, 512]):\n",
    "        \"\"\"Group requests by sequence length buckets.\"\"\"\n",
    "        buckets = {size: [] for size in bucket_sizes}\n",
    "        batches = []\n",
    "        latencies = []\n",
    "        \n",
    "        # Sort into buckets\n",
    "        for req in requests:\n",
    "            bucket_size = min(b for b in bucket_sizes if b >= req['length'])\n",
    "            buckets[bucket_size].append(req)\n",
    "            \n",
    "        # Process each bucket\n",
    "        for bucket_size, bucket_requests in buckets.items():\n",
    "            if not bucket_requests:\n",
    "                continue\n",
    "                \n",
    "            # Process in batches of 32\n",
    "            for i in range(0, len(bucket_requests), 32):\n",
    "                batch = bucket_requests[i:i + 32]\n",
    "                \n",
    "                # All requests in bucket have similar length\n",
    "                process_time = bucket_size * 0.001 + len(batch) * 0.01\n",
    "                \n",
    "                for req in batch:\n",
    "                    latencies.append(process_time)  # Simplified\n",
    "                    \n",
    "                batches.append({\n",
    "                    'size': len(batch),\n",
    "                    'max_length': bucket_size,\n",
    "                    'process_time': process_time\n",
    "                })\n",
    "                \n",
    "        return batches, latencies\n",
    "        \n",
    "    def compare_strategies(self, num_requests=1000):\n",
    "        \"\"\"Compare all batching strategies.\"\"\"\n",
    "        # Generate realistic request pattern\n",
    "        requests = []\n",
    "        for i in range(num_requests):\n",
    "            arrival_time = i * (50 + np.random.exponential(20))  # Poisson-like arrivals\n",
    "            length = int(np.random.lognormal(5, 1))  # Log-normal sequence lengths\n",
    "            length = min(max(length, 10), 1000)  # Clamp to reasonable range\n",
    "            \n",
    "            requests.append({\n",
    "                'id': i,\n",
    "                'arrival_time': arrival_time,\n",
    "                'length': length\n",
    "            })\n",
    "            \n",
    "        # Test strategies\n",
    "        strategies = {\n",
    "            'Static': lambda: self.static_batching(requests, 32),\n",
    "            'Dynamic': lambda: self.dynamic_batching(requests, 32, 50),\n",
    "            'Bucket': lambda: self.bucket_batching(requests, [128, 256, 512, 1024])\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, strategy in strategies.items():\n",
    "            print(f\"Testing {name} batching...\")\n",
    "            batches, latencies = strategy()\n",
    "            \n",
    "            # Calculate metrics\n",
    "            total_padding = sum(\n",
    "                batch['max_length'] * batch['size'] - \n",
    "                sum(req['length'] for req in requests[:batch['size']])\n",
    "                for batch in batches\n",
    "            )\n",
    "            \n",
    "            results[name] = {\n",
    "                'avg_latency': np.mean(latencies),\n",
    "                'p95_latency': np.percentile(latencies, 95),\n",
    "                'throughput': len(requests) / max(latencies) if latencies else 0,\n",
    "                'num_batches': len(batches),\n",
    "                'avg_batch_size': np.mean([b['size'] for b in batches]),\n",
    "                'total_padding': total_padding,\n",
    "                'latencies': latencies\n",
    "            }\n",
    "            \n",
    "        return results, requests\n",
    "        \n",
    "    def visualize_comparison(self, results):\n",
    "        \"\"\"Visualize batching strategy comparison.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        strategies = list(results.keys())\n",
    "        \n",
    "        # Average latency\n",
    "        latencies = [results[s]['avg_latency'] for s in strategies]\n",
    "        bars1 = axes[0, 0].bar(strategies, latencies)\n",
    "        axes[0, 0].set_title('Average Latency')\n",
    "        axes[0, 0].set_ylabel('Latency (ms)')\n",
    "        \n",
    "        # P95 latency\n",
    "        p95_latencies = [results[s]['p95_latency'] for s in strategies]\n",
    "        bars2 = axes[0, 1].bar(strategies, p95_latencies, color='orange')\n",
    "        axes[0, 1].set_title('P95 Latency')\n",
    "        axes[0, 1].set_ylabel('Latency (ms)')\n",
    "        \n",
    "        # Throughput\n",
    "        throughputs = [results[s]['throughput'] for s in strategies]\n",
    "        bars3 = axes[0, 2].bar(strategies, throughputs, color='green')\n",
    "        axes[0, 2].set_title('Throughput')\n",
    "        axes[0, 2].set_ylabel('Requests/sec')\n",
    "        \n",
    "        # Batch size distribution\n",
    "        avg_batch_sizes = [results[s]['avg_batch_size'] for s in strategies]\n",
    "        bars4 = axes[1, 0].bar(strategies, avg_batch_sizes, color='red')\n",
    "        axes[1, 0].set_title('Average Batch Size')\n",
    "        axes[1, 0].set_ylabel('Batch Size')\n",
    "        \n",
    "        # Padding overhead\n",
    "        paddings = [results[s]['total_padding'] for s in strategies]\n",
    "        bars5 = axes[1, 1].bar(strategies, paddings, color='purple')\n",
    "        axes[1, 1].set_title('Total Padding Tokens')\n",
    "        axes[1, 1].set_ylabel('Padding Tokens')\n",
    "        \n",
    "        # Latency distribution\n",
    "        for i, strategy in enumerate(strategies):\n",
    "            axes[1, 2].hist(\n",
    "                results[strategy]['latencies'][:100],  # Sample for visibility\n",
    "                alpha=0.6,\n",
    "                label=strategy,\n",
    "                bins=20\n",
    "            )\n",
    "        axes[1, 2].set_title('Latency Distribution')\n",
    "        axes[1, 2].set_xlabel('Latency (ms)')\n",
    "        axes[1, 2].set_ylabel('Frequency')\n",
    "        axes[1, 2].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nðŸ“Š Batching Strategy Comparison:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Strategy':<12} {'Avg Latency':<12} {'P95 Latency':<12} {'Throughput':<12} {'Avg Batch':<12}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            r = results[strategy]\n",
    "            print(f\"{strategy:<12} {r['avg_latency']:<12.2f} {r['p95_latency']:<12.2f} \"\n",
    "                  f\"{r['throughput']:<12.2f} {r['avg_batch_size']:<12.2f}\")\n",
    "\n",
    "# Run batching comparison\n",
    "comparison = BatchingComparison()\n",
    "results, requests = comparison.compare_strategies(1000)\n",
    "comparison.visualize_comparison(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. KV Cache Management\n",
    "\n",
    "For generation tasks, KV cache is critical for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KVCacheDemo:\n",
    "    \"\"\"Demonstrate KV cache efficiency.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache_stats = defaultdict(list)\n",
    "        \n",
    "    def simulate_generation_without_cache(self, seq_length=100, vocab_size=30000):\n",
    "        \"\"\"Simulate generation without KV cache.\"\"\"\n",
    "        hidden_size = 768\n",
    "        num_heads = 12\n",
    "        head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Start with prompt\n",
    "        input_ids = torch.randint(0, vocab_size, (1, 10))\n",
    "        \n",
    "        total_flops = 0\n",
    "        memory_usage = []\n",
    "        \n",
    "        for step in range(seq_length):\n",
    "            current_length = 10 + step\n",
    "            \n",
    "            # Without cache: recompute everything each time\n",
    "            # Attention computation: O(LÂ²) where L is sequence length\n",
    "            attention_flops = current_length ** 2 * hidden_size\n",
    "            total_flops += attention_flops\n",
    "            \n",
    "            # Memory: store full attention matrices\n",
    "            attention_memory = current_length ** 2 * num_heads * 4  # bytes\n",
    "            memory_usage.append(attention_memory / 1e6)  # MB\n",
    "            \n",
    "            self.cache_stats['without_cache_flops'].append(attention_flops)\n",
    "            \n",
    "        return total_flops, memory_usage\n",
    "        \n",
    "    def simulate_generation_with_cache(self, seq_length=100, vocab_size=30000):\n",
    "        \"\"\"Simulate generation with KV cache.\"\"\"\n",
    "        hidden_size = 768\n",
    "        num_heads = 12\n",
    "        head_dim = hidden_size // num_heads\n",
    "        \n",
    "        # Start with prompt\n",
    "        input_ids = torch.randint(0, vocab_size, (1, 10))\n",
    "        prompt_length = 10\n",
    "        \n",
    "        total_flops = 0\n",
    "        memory_usage = []\n",
    "        \n",
    "        # Initial computation for prompt\n",
    "        initial_flops = prompt_length ** 2 * hidden_size\n",
    "        total_flops += initial_flops\n",
    "        \n",
    "        # Cache memory: store K,V for all positions\n",
    "        cache_memory_per_token = num_heads * head_dim * 2 * 4  # K+V, float32\n",
    "        \n",
    "        for step in range(seq_length):\n",
    "            current_length = prompt_length + step\n",
    "            \n",
    "            # With cache: only compute attention for new token\n",
    "            # New token attends to all previous tokens: O(L)\n",
    "            attention_flops = current_length * hidden_size\n",
    "            total_flops += attention_flops\n",
    "            \n",
    "            # Memory: cache grows linearly\n",
    "            cache_memory = current_length * cache_memory_per_token\n",
    "            memory_usage.append(cache_memory / 1e6)  # MB\n",
    "            \n",
    "            self.cache_stats['with_cache_flops'].append(attention_flops)\n",
    "            \n",
    "        return total_flops, memory_usage\n",
    "        \n",
    "    def compare_cache_strategies(self):\n",
    "        \"\"\"Compare generation with and without cache.\"\"\"\n",
    "        seq_lengths = [50, 100, 200, 500]\n",
    "        \n",
    "        results = {\n",
    "            'without_cache': {'flops': [], 'memory': [], 'time': []},\n",
    "            'with_cache': {'flops': [], 'memory': [], 'time': []}\n",
    "        }\n",
    "        \n",
    "        for seq_len in seq_lengths:\n",
    "            print(f\"Testing sequence length: {seq_len}\")\n",
    "            \n",
    "            # Without cache\n",
    "            start_time = time.time()\n",
    "            flops_no_cache, memory_no_cache = self.simulate_generation_without_cache(seq_len)\n",
    "            time_no_cache = time.time() - start_time\n",
    "            \n",
    "            # With cache\n",
    "            start_time = time.time()\n",
    "            flops_with_cache, memory_with_cache = self.simulate_generation_with_cache(seq_len)\n",
    "            time_with_cache = time.time() - start_time\n",
    "            \n",
    "            results['without_cache']['flops'].append(flops_no_cache)\n",
    "            results['without_cache']['memory'].append(max(memory_no_cache))\n",
    "            results['without_cache']['time'].append(time_no_cache)\n",
    "            \n",
    "            results['with_cache']['flops'].append(flops_with_cache)\n",
    "            results['with_cache']['memory'].append(max(memory_with_cache))\n",
    "            results['with_cache']['time'].append(time_with_cache)\n",
    "            \n",
    "        return results, seq_lengths\n",
    "        \n",
    "    def visualize_cache_efficiency(self, results, seq_lengths):\n",
    "        \"\"\"Visualize KV cache benefits.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # FLOPS comparison\n",
    "        axes[0, 0].plot(seq_lengths, results['without_cache']['flops'], \n",
    "                       'ro-', label='Without Cache', linewidth=2)\n",
    "        axes[0, 0].plot(seq_lengths, results['with_cache']['flops'], \n",
    "                       'go-', label='With Cache', linewidth=2)\n",
    "        axes[0, 0].set_xlabel('Sequence Length')\n",
    "        axes[0, 0].set_ylabel('Total FLOPs')\n",
    "        axes[0, 0].set_title('Computation Complexity')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].set_yscale('log')\n",
    "        \n",
    "        # Memory usage\n",
    "        axes[0, 1].plot(seq_lengths, results['without_cache']['memory'], \n",
    "                       'ro-', label='Without Cache', linewidth=2)\n",
    "        axes[0, 1].plot(seq_lengths, results['with_cache']['memory'], \n",
    "                       'go-', label='With Cache', linewidth=2)\n",
    "        axes[0, 1].set_xlabel('Sequence Length')\n",
    "        axes[0, 1].set_ylabel('Peak Memory (MB)')\n",
    "        axes[0, 1].set_title('Memory Usage')\n",
    "        axes[0, 1].legend()\n",
    "        \n",
    "        # Speedup\n",
    "        speedups = [no_cache / with_cache for no_cache, with_cache in \n",
    "                   zip(results['without_cache']['flops'], results['with_cache']['flops'])]\n",
    "        axes[1, 0].bar(range(len(seq_lengths)), speedups, color='blue', alpha=0.7)\n",
    "        axes[1, 0].set_xlabel('Sequence Length')\n",
    "        axes[1, 0].set_ylabel('Speedup Factor')\n",
    "        axes[1, 0].set_title('KV Cache Speedup')\n",
    "        axes[1, 0].set_xticks(range(len(seq_lengths)))\n",
    "        axes[1, 0].set_xticklabels(seq_lengths)\n",
    "        \n",
    "        # Per-step FLOPS comparison\n",
    "        if self.cache_stats['without_cache_flops'] and self.cache_stats['with_cache_flops']:\n",
    "            steps = range(min(50, len(self.cache_stats['without_cache_flops'])))\n",
    "            axes[1, 1].plot(steps, self.cache_stats['without_cache_flops'][:50], \n",
    "                           'ro-', label='Without Cache', alpha=0.7)\n",
    "            axes[1, 1].plot(steps, self.cache_stats['with_cache_flops'][:50], \n",
    "                           'go-', label='With Cache', alpha=0.7)\n",
    "            axes[1, 1].set_xlabel('Generation Step')\n",
    "            axes[1, 1].set_ylabel('FLOPs per Step')\n",
    "            axes[1, 1].set_title('Per-Step Computation')\n",
    "            axes[1, 1].legend()\n",
    "            axes[1, 1].set_yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print efficiency summary\n",
    "        print(\"\\nðŸš€ KV Cache Efficiency Summary:\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"{'Seq Length':<12} {'Speedup':<12} {'Memory Ratio':<15} {'FLOP Ratio':<12}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for i, seq_len in enumerate(seq_lengths):\n",
    "            speedup = results['without_cache']['flops'][i] / results['with_cache']['flops'][i]\n",
    "            memory_ratio = results['with_cache']['memory'][i] / results['without_cache']['memory'][i]\n",
    "            flop_ratio = results['with_cache']['flops'][i] / results['without_cache']['flops'][i]\n",
    "            \n",
    "            print(f\"{seq_len:<12} {speedup:<12.2f} {memory_ratio:<15.2f} {flop_ratio:<12.4f}\")\n",
    "\n",
    "# Run KV cache demo\n",
    "cache_demo = KVCacheDemo()\n",
    "results, seq_lengths = cache_demo.compare_cache_strategies()\n",
    "cache_demo.visualize_cache_efficiency(results, seq_lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Balancing and Auto-scaling\n",
    "\n",
    "Let's explore how to distribute load and automatically scale based on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadBalancingDemo:\n",
    "    \"\"\"Demonstrate load balancing strategies.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_servers=4):\n",
    "        self.num_servers = num_servers\n",
    "        self.server_loads = [0] * num_servers\n",
    "        self.server_latencies = [[] for _ in range(num_servers)]\n",
    "        self.server_capacities = [100 + i * 20 for i in range(num_servers)]  # Varying capacity\n",
    "        self.request_history = []\n",
    "        \n",
    "    def round_robin(self, request_id):\n",
    "        \"\"\"Simple round-robin load balancing.\"\"\"\n",
    "        server_id = request_id % self.num_servers\n",
    "        return server_id\n",
    "        \n",
    "    def least_loaded(self, request_id):\n",
    "        \"\"\"Route to least loaded server.\"\"\"\n",
    "        server_id = self.server_loads.index(min(self.server_loads))\n",
    "        return server_id\n",
    "        \n",
    "    def weighted_least_loaded(self, request_id):\n",
    "        \"\"\"Route based on load and capacity.\"\"\"\n",
    "        # Calculate load ratios\n",
    "        load_ratios = [\n",
    "            load / capacity \n",
    "            for load, capacity in zip(self.server_loads, self.server_capacities)\n",
    "        ]\n",
    "        server_id = load_ratios.index(min(load_ratios))\n",
    "        return server_id\n",
    "        \n",
    "    def latency_aware(self, request_id):\n",
    "        \"\"\"Route based on recent latencies.\"\"\"\n",
    "        avg_latencies = []\n",
    "        for latencies in self.server_latencies:\n",
    "            if latencies:\n",
    "                avg = sum(latencies[-10:]) / min(len(latencies), 10)\n",
    "            else:\n",
    "                avg = 0\n",
    "            avg_latencies.append(avg)\n",
    "            \n",
    "        server_id = avg_latencies.index(min(avg_latencies))\n",
    "        return server_id\n",
    "        \n",
    "    def simulate_request(self, server_id, request_size=1):\n",
    "        \"\"\"Simulate processing a request on a server.\"\"\"\n",
    "        # Update load\n",
    "        self.server_loads[server_id] += request_size\n",
    "        \n",
    "        # Simulate latency based on load\n",
    "        load_factor = self.server_loads[server_id] / self.server_capacities[server_id]\n",
    "        base_latency = 50  # ms\n",
    "        latency = base_latency * (1 + load_factor ** 2) + np.random.normal(0, 5)\n",
    "        latency = max(10, latency)  # Minimum latency\n",
    "        \n",
    "        # Record latency\n",
    "        self.server_latencies[server_id].append(latency)\n",
    "        \n",
    "        # Process request (reduce load)\n",
    "        self.server_loads[server_id] = max(0, self.server_loads[server_id] - request_size)\n",
    "        \n",
    "        return latency\n",
    "        \n",
    "    def test_strategy(self, strategy_name, strategy_func, num_requests=1000):\n",
    "        \"\"\"Test a load balancing strategy.\"\"\"\n",
    "        # Reset state\n",
    "        self.server_loads = [0] * self.num_servers\n",
    "        self.server_latencies = [[] for _ in range(self.num_servers)]\n",
    "        \n",
    "        latencies = []\n",
    "        server_assignments = []\n",
    "        \n",
    "        for i in range(num_requests):\n",
    "            # Generate request with varying size\n",
    "            request_size = 1 + np.random.poisson(0.5)  # 1-3 units typically\n",
    "            \n",
    "            # Route request\n",
    "            server_id = strategy_func(i)\n",
    "            server_assignments.append(server_id)\n",
    "            \n",
    "            # Process request\n",
    "            latency = self.simulate_request(server_id, request_size)\n",
    "            latencies.append(latency)\n",
    "            \n",
    "        return {\n",
    "            'strategy': strategy_name,\n",
    "            'latencies': latencies,\n",
    "            'server_assignments': server_assignments,\n",
    "            'avg_latency': np.mean(latencies),\n",
    "            'p95_latency': np.percentile(latencies, 95),\n",
    "            'server_utilization': [\n",
    "                len([s for s in server_assignments if s == i]) / num_requests\n",
    "                for i in range(self.num_servers)\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "    def compare_strategies(self):\n",
    "        \"\"\"Compare all load balancing strategies.\"\"\"\n",
    "        strategies = {\n",
    "            'Round Robin': self.round_robin,\n",
    "            'Least Loaded': self.least_loaded,\n",
    "            'Weighted Least Loaded': self.weighted_least_loaded,\n",
    "            'Latency Aware': self.latency_aware\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        for name, func in strategies.items():\n",
    "            print(f\"Testing {name} strategy...\")\n",
    "            results[name] = self.test_strategy(name, func)\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def visualize_load_balancing(self, results):\n",
    "        \"\"\"Visualize load balancing results.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        strategies = list(results.keys())\n",
    "        \n",
    "        # Average latency\n",
    "        avg_latencies = [results[s]['avg_latency'] for s in strategies]\n",
    "        bars1 = axes[0, 0].bar(strategies, avg_latencies, color='skyblue')\n",
    "        axes[0, 0].set_title('Average Latency by Strategy')\n",
    "        axes[0, 0].set_ylabel('Latency (ms)')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # P95 latency\n",
    "        p95_latencies = [results[s]['p95_latency'] for s in strategies]\n",
    "        bars2 = axes[0, 1].bar(strategies, p95_latencies, color='lightcoral')\n",
    "        axes[0, 1].set_title('P95 Latency by Strategy')\n",
    "        axes[0, 1].set_ylabel('Latency (ms)')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Latency distribution\n",
    "        for strategy in strategies:\n",
    "            latencies = results[strategy]['latencies'][:200]  # Sample for visibility\n",
    "            axes[0, 2].hist(latencies, alpha=0.6, label=strategy, bins=20)\n",
    "        axes[0, 2].set_title('Latency Distribution')\n",
    "        axes[0, 2].set_xlabel('Latency (ms)')\n",
    "        axes[0, 2].set_ylabel('Frequency')\n",
    "        axes[0, 2].legend()\n",
    "        \n",
    "        # Server utilization heatmap\n",
    "        utilization_matrix = np.array([\n",
    "            results[s]['server_utilization'] for s in strategies\n",
    "        ])\n",
    "        \n",
    "        im = axes[1, 0].imshow(utilization_matrix, cmap='YlOrRd', aspect='auto')\n",
    "        axes[1, 0].set_title('Server Utilization')\n",
    "        axes[1, 0].set_xlabel('Server ID')\n",
    "        axes[1, 0].set_ylabel('Strategy')\n",
    "        axes[1, 0].set_yticks(range(len(strategies)))\n",
    "        axes[1, 0].set_yticklabels(strategies)\n",
    "        axes[1, 0].set_xticks(range(self.num_servers))\n",
    "        plt.colorbar(im, ax=axes[1, 0], label='Utilization %')\n",
    "        \n",
    "        # Server capacity vs utilization\n",
    "        for i, strategy in enumerate(strategies):\n",
    "            utilization = results[strategy]['server_utilization']\n",
    "            axes[1, 1].scatter(\n",
    "                self.server_capacities, utilization, \n",
    "                label=strategy, s=60, alpha=0.7\n",
    "            )\n",
    "        axes[1, 1].set_title('Capacity vs Utilization')\n",
    "        axes[1, 1].set_xlabel('Server Capacity')\n",
    "        axes[1, 1].set_ylabel('Utilization %')\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        # Efficiency score (lower latency + balanced utilization)\n",
    "        efficiency_scores = []\n",
    "        for strategy in strategies:\n",
    "            r = results[strategy]\n",
    "            # Normalize metrics\n",
    "            latency_score = 1 / (r['avg_latency'] / min(avg_latencies))\n",
    "            balance_score = 1 / (np.std(r['server_utilization']) + 0.01)\n",
    "            efficiency = latency_score * balance_score\n",
    "            efficiency_scores.append(efficiency)\n",
    "            \n",
    "        bars6 = axes[1, 2].bar(strategies, efficiency_scores, color='lightgreen')\n",
    "        axes[1, 2].set_title('Overall Efficiency')\n",
    "        axes[1, 2].set_ylabel('Efficiency Score')\n",
    "        axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\nâš–ï¸ Load Balancing Strategy Comparison:\")\n",
    "        print(\"-\" * 90)\n",
    "        print(f\"{'Strategy':<20} {'Avg Latency':<12} {'P95 Latency':<12} {'Std Dev Util':<15} {'Best Server':<12}\")\n",
    "        print(\"-\" * 90)\n",
    "        \n",
    "        for strategy in strategies:\n",
    "            r = results[strategy]\n",
    "            std_util = np.std(r['server_utilization'])\n",
    "            best_server = np.argmax(r['server_utilization'])\n",
    "            \n",
    "            print(f\"{strategy:<20} {r['avg_latency']:<12.2f} {r['p95_latency']:<12.2f} \"\n",
    "                  f\"{std_util:<15.3f} {best_server:<12}\")\n",
    "\n",
    "# Run load balancing demo\n",
    "lb_demo = LoadBalancingDemo(num_servers=4)\n",
    "lb_results = lb_demo.compare_strategies()\n",
    "lb_demo.visualize_load_balancing(lb_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Auto-scaling Simulation\n",
    "\n",
    "Let's see how auto-scaling responds to changing load patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoScalingSimulation:\n",
    "    \"\"\"Simulate auto-scaling behavior.\"\"\"\n",
    "    \n",
    "    def __init__(self, initial_instances=2, min_instances=1, max_instances=10):\n",
    "        self.current_instances = initial_instances\n",
    "        self.min_instances = min_instances\n",
    "        self.max_instances = max_instances\n",
    "        \n",
    "        # Scaling thresholds\n",
    "        self.scale_up_cpu = 70\n",
    "        self.scale_down_cpu = 30\n",
    "        self.scale_up_latency = 100  # ms\n",
    "        self.scale_down_latency = 20  # ms\n",
    "        \n",
    "        # History tracking\n",
    "        self.history = {\n",
    "            'instances': [],\n",
    "            'cpu_usage': [],\n",
    "            'latency': [],\n",
    "            'throughput': [],\n",
    "            'cost': [],\n",
    "            'scaling_events': []\n",
    "        }\n",
    "        \n",
    "    def generate_traffic_pattern(self, time_steps=288):  # 24 hours in 5-min intervals\n",
    "        \"\"\"Generate realistic traffic pattern.\"\"\"\n",
    "        traffic = []\n",
    "        \n",
    "        for t in range(time_steps):\n",
    "            hour = (t * 5 / 60) % 24  # Convert to hour of day\n",
    "            \n",
    "            # Base traffic pattern (higher during business hours)\n",
    "            if 9 <= hour <= 17:  # Business hours\n",
    "                base_traffic = 80 + 20 * np.sin((hour - 9) / 8 * np.pi)\n",
    "            elif 18 <= hour <= 22:  # Evening\n",
    "                base_traffic = 60 + 10 * np.sin((hour - 18) / 4 * np.pi)\n",
    "            else:  # Night/early morning\n",
    "                base_traffic = 20 + 10 * np.random.normal(0, 1)\n",
    "                \n",
    "            # Add weekly pattern (lower on weekends)\n",
    "            day_of_week = (t // 288) % 7\n",
    "            if day_of_week >= 5:  # Weekend\n",
    "                base_traffic *= 0.6\n",
    "                \n",
    "            # Add random spikes\n",
    "            if np.random.random() < 0.05:  # 5% chance of spike\n",
    "                base_traffic *= (2 + np.random.random())\n",
    "                \n",
    "            # Add noise\n",
    "            traffic.append(max(10, base_traffic + np.random.normal(0, 5)))\n",
    "            \n",
    "        return traffic\n",
    "        \n",
    "    def calculate_metrics(self, traffic_load):\n",
    "        \"\"\"Calculate system metrics based on traffic and instances.\"\"\"\n",
    "        # CPU usage (inversely related to instance count)\n",
    "        cpu_per_instance = traffic_load / self.current_instances\n",
    "        cpu_usage = min(100, cpu_per_instance)\n",
    "        \n",
    "        # Latency (increases with CPU usage)\n",
    "        if cpu_usage < 50:\n",
    "            latency = 30 + cpu_usage * 0.5\n",
    "        elif cpu_usage < 80:\n",
    "            latency = 55 + (cpu_usage - 50) * 1.5\n",
    "        else:\n",
    "            latency = 100 + (cpu_usage - 80) * 3  # Exponential increase\n",
    "            \n",
    "        # Add random variation\n",
    "        latency += np.random.normal(0, 5)\n",
    "        latency = max(10, latency)\n",
    "        \n",
    "        # Throughput (decreases with high latency)\n",
    "        throughput = min(1000, traffic_load * self.current_instances / max(latency, 30) * 30)\n",
    "        \n",
    "        # Cost (linear with instances)\n",
    "        cost_per_instance_per_hour = 2.0  # $2/hour per instance\n",
    "        cost = self.current_instances * cost_per_instance_per_hour / 12  # 5-min interval\n",
    "        \n",
    "        return cpu_usage, latency, throughput, cost\n",
    "        \n",
    "    def make_scaling_decision(self, cpu_usage, latency):\n",
    "        \"\"\"Decide whether to scale up, down, or stay the same.\"\"\"\n",
    "        # Scale up conditions\n",
    "        if (cpu_usage > self.scale_up_cpu or latency > self.scale_up_latency) and \\\n",
    "           self.current_instances < self.max_instances:\n",
    "            return 1  # Scale up\n",
    "            \n",
    "        # Scale down conditions (more conservative)\n",
    "        if cpu_usage < self.scale_down_cpu and latency < self.scale_down_latency and \\\n",
    "           self.current_instances > self.min_instances:\n",
    "            return -1  # Scale down\n",
    "            \n",
    "        return 0  # No scaling\n",
    "        \n",
    "    def run_simulation(self):\n",
    "        \"\"\"Run the auto-scaling simulation.\"\"\"\n",
    "        traffic_pattern = self.generate_traffic_pattern()\n",
    "        \n",
    "        for t, traffic_load in enumerate(traffic_pattern):\n",
    "            # Calculate current metrics\n",
    "            cpu_usage, latency, throughput, cost = self.calculate_metrics(traffic_load)\n",
    "            \n",
    "            # Record metrics\n",
    "            self.history['instances'].append(self.current_instances)\n",
    "            self.history['cpu_usage'].append(cpu_usage)\n",
    "            self.history['latency'].append(latency)\n",
    "            self.history['throughput'].append(throughput)\n",
    "            self.history['cost'].append(cost)\n",
    "            \n",
    "            # Make scaling decision\n",
    "            scaling_decision = self.make_scaling_decision(cpu_usage, latency)\n",
    "            \n",
    "            if scaling_decision != 0:\n",
    "                old_instances = self.current_instances\n",
    "                self.current_instances += scaling_decision\n",
    "                self.current_instances = max(self.min_instances, \n",
    "                                           min(self.max_instances, self.current_instances))\n",
    "                \n",
    "                self.history['scaling_events'].append({\n",
    "                    'time': t,\n",
    "                    'action': 'scale_up' if scaling_decision > 0 else 'scale_down',\n",
    "                    'from': old_instances,\n",
    "                    'to': self.current_instances,\n",
    "                    'trigger': f\"CPU: {cpu_usage:.1f}%, Latency: {latency:.1f}ms\"\n",
    "                })\n",
    "                \n",
    "        return self.history\n",
    "        \n",
    "    def visualize_simulation(self):\n",
    "        \"\"\"Visualize the auto-scaling simulation results.\"\"\"\n",
    "        time_hours = np.arange(len(self.history['instances'])) * 5 / 60  # Convert to hours\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "        \n",
    "        # Instance count over time\n",
    "        axes[0, 0].plot(time_hours, self.history['instances'], 'bo-', linewidth=2, markersize=3)\n",
    "        axes[0, 0].set_title('Instance Count Over Time')\n",
    "        axes[0, 0].set_xlabel('Time (hours)')\n",
    "        axes[0, 0].set_ylabel('Number of Instances')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Mark scaling events\n",
    "        for event in self.history['scaling_events']:\n",
    "            time_point = event['time'] * 5 / 60\n",
    "            color = 'green' if event['action'] == 'scale_up' else 'red'\n",
    "            axes[0, 0].axvline(x=time_point, color=color, alpha=0.5, linestyle='--')\n",
    "            \n",
    "        # CPU usage\n",
    "        axes[0, 1].plot(time_hours, self.history['cpu_usage'], color='orange', linewidth=1)\n",
    "        axes[0, 1].axhline(y=self.scale_up_cpu, color='red', linestyle='--', label='Scale Up Threshold')\n",
    "        axes[0, 1].axhline(y=self.scale_down_cpu, color='green', linestyle='--', label='Scale Down Threshold')\n",
    "        axes[0, 1].set_title('CPU Usage')\n",
    "        axes[0, 1].set_xlabel('Time (hours)')\n",
    "        axes[0, 1].set_ylabel('CPU Usage (%)')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Latency\n",
    "        axes[1, 0].plot(time_hours, self.history['latency'], color='red', linewidth=1)\n",
    "        axes[1, 0].axhline(y=self.scale_up_latency, color='red', linestyle='--', label='Scale Up Threshold')\n",
    "        axes[1, 0].axhline(y=self.scale_down_latency, color='green', linestyle='--', label='Scale Down Threshold')\n",
    "        axes[1, 0].set_title('Latency')\n",
    "        axes[1, 0].set_xlabel('Time (hours)')\n",
    "        axes[1, 0].set_ylabel('Latency (ms)')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Throughput\n",
    "        axes[1, 1].plot(time_hours, self.history['throughput'], color='green', linewidth=1)\n",
    "        axes[1, 1].set_title('Throughput')\n",
    "        axes[1, 1].set_xlabel('Time (hours)')\n",
    "        axes[1, 1].set_ylabel('Requests/sec')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cumulative cost\n",
    "        cumulative_cost = np.cumsum(self.history['cost'])\n",
    "        axes[2, 0].plot(time_hours, cumulative_cost, color='purple', linewidth=2)\n",
    "        axes[2, 0].set_title('Cumulative Cost')\n",
    "        axes[2, 0].set_xlabel('Time (hours)')\n",
    "        axes[2, 0].set_ylabel('Cost ($)')\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scaling events timeline\n",
    "        scale_up_times = []\n",
    "        scale_down_times = []\n",
    "        \n",
    "        for event in self.history['scaling_events']:\n",
    "            time_point = event['time'] * 5 / 60\n",
    "            if event['action'] == 'scale_up':\n",
    "                scale_up_times.append(time_point)\n",
    "            else:\n",
    "                scale_down_times.append(time_point)\n",
    "                \n",
    "        if scale_up_times:\n",
    "            axes[2, 1].scatter(scale_up_times, [1] * len(scale_up_times), \n",
    "                             color='green', s=50, label='Scale Up', alpha=0.7)\n",
    "        if scale_down_times:\n",
    "            axes[2, 1].scatter(scale_down_times, [0] * len(scale_down_times), \n",
    "                             color='red', s=50, label='Scale Down', alpha=0.7)\n",
    "            \n",
    "        axes[2, 1].set_title('Scaling Events')\n",
    "        axes[2, 1].set_xlabel('Time (hours)')\n",
    "        axes[2, 1].set_ylabel('Event Type')\n",
    "        axes[2, 1].set_yticks([0, 1])\n",
    "        axes[2, 1].set_yticklabels(['Scale Down', 'Scale Up'])\n",
    "        axes[2, 1].legend()\n",
    "        axes[2, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        total_cost = sum(self.history['cost'])\n",
    "        avg_latency = np.mean(self.history['latency'])\n",
    "        p95_latency = np.percentile(self.history['latency'], 95)\n",
    "        avg_cpu = np.mean(self.history['cpu_usage'])\n",
    "        num_scale_events = len(self.history['scaling_events'])\n",
    "        \n",
    "        print(\"\\nðŸ“ˆ Auto-scaling Simulation Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Total simulation time: {len(self.history['instances']) * 5 / 60:.1f} hours\")\n",
    "        print(f\"Total cost: ${total_cost:.2f}\")\n",
    "        print(f\"Average latency: {avg_latency:.2f} ms\")\n",
    "        print(f\"P95 latency: {p95_latency:.2f} ms\")\n",
    "        print(f\"Average CPU usage: {avg_cpu:.1f}%\")\n",
    "        print(f\"Number of scaling events: {num_scale_events}\")\n",
    "        print(f\"Instance range: {min(self.history['instances'])} - {max(self.history['instances'])}\")\n",
    "        \n",
    "        # Show recent scaling events\n",
    "        if self.history['scaling_events']:\n",
    "            print(\"\\nðŸ”„ Recent Scaling Events:\")\n",
    "            for event in self.history['scaling_events'][-5:]:  # Last 5 events\n",
    "                time_str = f\"{event['time'] * 5 / 60:.1f}h\"\n",
    "                print(f\"  {time_str}: {event['action']} ({event['from']} â†’ {event['to']}) - {event['trigger']}\")\n",
    "\n",
    "# Run auto-scaling simulation\n",
    "print(\"Running auto-scaling simulation...\")\n",
    "scaling_sim = AutoScalingSimulation(initial_instances=2, min_instances=1, max_instances=8)\n",
    "history = scaling_sim.run_simulation()\n",
    "scaling_sim.visualize_simulation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Edge Deployment Optimization\n",
    "\n",
    "Let's explore optimizations specifically for edge deployment scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeOptimizationDemo:\n",
    "    \"\"\"Demonstrate edge deployment optimizations.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_results = {}\n",
    "        \n",
    "    def simulate_model_sizes(self):\n",
    "        \"\"\"Simulate different model compression techniques.\"\"\"\n",
    "        # Base model stats\n",
    "        base_model = {\n",
    "            'size_mb': 1200,  # 1.2GB model\n",
    "            'latency_ms': 150,\n",
    "            'accuracy': 95.5,\n",
    "            'memory_mb': 2400\n",
    "        }\n",
    "        \n",
    "        optimizations = {\n",
    "            'Original': {\n",
    "                'size_mb': base_model['size_mb'],\n",
    "                'latency_ms': base_model['latency_ms'],\n",
    "                'accuracy': base_model['accuracy'],\n",
    "                'memory_mb': base_model['memory_mb']\n",
    "            },\n",
    "            'Quantization (INT8)': {\n",
    "                'size_mb': base_model['size_mb'] * 0.25,  # 4x smaller\n",
    "                'latency_ms': base_model['latency_ms'] * 0.7,  # 30% faster\n",
    "                'accuracy': base_model['accuracy'] - 0.5,  # Slight accuracy drop\n",
    "                'memory_mb': base_model['memory_mb'] * 0.4\n",
    "            },\n",
    "            'Pruning (50%)': {\n",
    "                'size_mb': base_model['size_mb'] * 0.5,  # 50% smaller\n",
    "                'latency_ms': base_model['latency_ms'] * 0.6,  # 40% faster\n",
    "                'accuracy': base_model['accuracy'] - 1.0,  # Moderate accuracy drop\n",
    "                'memory_mb': base_model['memory_mb'] * 0.6\n",
    "            },\n",
    "            'Distillation': {\n",
    "                'size_mb': base_model['size_mb'] * 0.3,  # 70% smaller\n",
    "                'latency_ms': base_model['latency_ms'] * 0.4,  # 60% faster\n",
    "                'accuracy': base_model['accuracy'] - 2.0,  # Larger accuracy drop\n",
    "                'memory_mb': base_model['memory_mb'] * 0.5\n",
    "            },\n",
    "            'Quantization + Pruning': {\n",
    "                'size_mb': base_model['size_mb'] * 0.125,  # 8x smaller\n",
    "                'latency_ms': base_model['latency_ms'] * 0.45,  # 55% faster\n",
    "                'accuracy': base_model['accuracy'] - 1.8,  # Combined accuracy drop\n",
    "                'memory_mb': base_model['memory_mb'] * 0.3\n",
    "            },\n",
    "            'Mobile Optimized': {\n",
    "                'size_mb': base_model['size_mb'] * 0.08,  # 12.5x smaller\n",
    "                'latency_ms': base_model['latency_ms'] * 0.3,  # 70% faster\n",
    "                'accuracy': base_model['accuracy'] - 3.5,  # Significant accuracy drop\n",
    "                'memory_mb': base_model['memory_mb'] * 0.2\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return optimizations\n",
    "        \n",
    "    def simulate_device_constraints(self):\n",
    "        \"\"\"Simulate different edge device capabilities.\"\"\"\n",
    "        devices = {\n",
    "            'High-End Mobile': {\n",
    "                'max_memory_mb': 8000,\n",
    "                'max_model_size_mb': 500,\n",
    "                'target_latency_ms': 100,\n",
    "                'power_budget_w': 5\n",
    "            },\n",
    "            'Mid-Range Mobile': {\n",
    "                'max_memory_mb': 4000,\n",
    "                'max_model_size_mb': 200,\n",
    "                'target_latency_ms': 200,\n",
    "                'power_budget_w': 3\n",
    "            },\n",
    "            'IoT Device': {\n",
    "                'max_memory_mb': 1000,\n",
    "                'max_model_size_mb': 50,\n",
    "                'target_latency_ms': 500,\n",
    "                'power_budget_w': 1\n",
    "            },\n",
    "            'Edge Server': {\n",
    "                'max_memory_mb': 16000,\n",
    "                'max_model_size_mb': 2000,\n",
    "                'target_latency_ms': 50,\n",
    "                'power_budget_w': 100\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return devices\n",
    "        \n",
    "    def check_compatibility(self, model_stats, device_constraints):\n",
    "        \"\"\"Check if model is compatible with device constraints.\"\"\"\n",
    "        compatible = True\n",
    "        issues = []\n",
    "        \n",
    "        if model_stats['size_mb'] > device_constraints['max_model_size_mb']:\n",
    "            compatible = False\n",
    "            issues.append(\"Model too large\")\n",
    "            \n",
    "        if model_stats['memory_mb'] > device_constraints['max_memory_mb']:\n",
    "            compatible = False\n",
    "            issues.append(\"Memory requirement too high\")\n",
    "            \n",
    "        if model_stats['latency_ms'] > device_constraints['target_latency_ms']:\n",
    "            compatible = False\n",
    "            issues.append(\"Latency too high\")\n",
    "            \n",
    "        return compatible, issues\n",
    "        \n",
    "    def analyze_edge_deployment(self):\n",
    "        \"\"\"Analyze which optimizations work for which devices.\"\"\"\n",
    "        models = self.simulate_model_sizes()\n",
    "        devices = self.simulate_device_constraints()\n",
    "        \n",
    "        compatibility_matrix = {}\n",
    "        \n",
    "        for device_name, device_constraints in devices.items():\n",
    "            compatibility_matrix[device_name] = {}\n",
    "            \n",
    "            for model_name, model_stats in models.items():\n",
    "                compatible, issues = self.check_compatibility(model_stats, device_constraints)\n",
    "                \n",
    "                compatibility_matrix[device_name][model_name] = {\n",
    "                    'compatible': compatible,\n",
    "                    'issues': issues,\n",
    "                    'efficiency_score': self._calculate_efficiency_score(\n",
    "                        model_stats, device_constraints\n",
    "                    )\n",
    "                }\n",
    "                \n",
    "        return models, devices, compatibility_matrix\n",
    "        \n",
    "    def _calculate_efficiency_score(self, model_stats, device_constraints):\n",
    "        \"\"\"Calculate efficiency score for model-device combination.\"\"\"\n",
    "        # Normalize metrics\n",
    "        size_score = min(1.0, device_constraints['max_model_size_mb'] / model_stats['size_mb'])\n",
    "        memory_score = min(1.0, device_constraints['max_memory_mb'] / model_stats['memory_mb'])\n",
    "        latency_score = min(1.0, device_constraints['target_latency_ms'] / model_stats['latency_ms'])\n",
    "        accuracy_score = model_stats['accuracy'] / 100.0\n",
    "        \n",
    "        # Weighted combination\n",
    "        efficiency = (size_score * 0.2 + memory_score * 0.2 + \n",
    "                     latency_score * 0.3 + accuracy_score * 0.3)\n",
    "        \n",
    "        return efficiency\n",
    "        \n",
    "    def visualize_edge_analysis(self, models, devices, compatibility_matrix):\n",
    "        \"\"\"Visualize edge deployment analysis.\"\"\"\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        model_names = list(models.keys())\n",
    "        device_names = list(devices.keys())\n",
    "        \n",
    "        # Model size vs accuracy trade-off\n",
    "        sizes = [models[m]['size_mb'] for m in model_names]\n",
    "        accuracies = [models[m]['accuracy'] for m in model_names]\n",
    "        \n",
    "        scatter = axes[0, 0].scatter(sizes, accuracies, c=range(len(model_names)), \n",
    "                                   s=100, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            axes[0, 0].annotate(name, (sizes[i], accuracies[i]), \n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[0, 0].set_xlabel('Model Size (MB)')\n",
    "        axes[0, 0].set_ylabel('Accuracy (%)')\n",
    "        axes[0, 0].set_title('Size vs Accuracy Trade-off')\n",
    "        axes[0, 0].set_xscale('log')\n",
    "        \n",
    "        # Latency vs memory usage\n",
    "        latencies = [models[m]['latency_ms'] for m in model_names]\n",
    "        memories = [models[m]['memory_mb'] for m in model_names]\n",
    "        \n",
    "        scatter2 = axes[0, 1].scatter(latencies, memories, c=range(len(model_names)), \n",
    "                                    s=100, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            axes[0, 1].annotate(name, (latencies[i], memories[i]), \n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[0, 1].set_xlabel('Latency (ms)')\n",
    "        axes[0, 1].set_ylabel('Memory Usage (MB)')\n",
    "        axes[0, 1].set_title('Latency vs Memory Usage')\n",
    "        \n",
    "        # Compatibility heatmap\n",
    "        compatibility_data = np.zeros((len(device_names), len(model_names)))\n",
    "        \n",
    "        for i, device in enumerate(device_names):\n",
    "            for j, model in enumerate(model_names):\n",
    "                compatibility_data[i, j] = compatibility_matrix[device][model]['compatible']\n",
    "                \n",
    "        im = axes[0, 2].imshow(compatibility_data, cmap='RdYlGn', aspect='auto')\n",
    "        axes[0, 2].set_title('Model-Device Compatibility')\n",
    "        axes[0, 2].set_xlabel('Model')\n",
    "        axes[0, 2].set_ylabel('Device')\n",
    "        axes[0, 2].set_xticks(range(len(model_names)))\n",
    "        axes[0, 2].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[0, 2].set_yticks(range(len(device_names)))\n",
    "        axes[0, 2].set_yticklabels(device_names)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(device_names)):\n",
    "            for j in range(len(model_names)):\n",
    "                text = 'âœ“' if compatibility_data[i, j] else 'âœ—'\n",
    "                axes[0, 2].text(j, i, text, ha='center', va='center', \n",
    "                               color='white', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # Efficiency scores heatmap\n",
    "        efficiency_data = np.zeros((len(device_names), len(model_names)))\n",
    "        \n",
    "        for i, device in enumerate(device_names):\n",
    "            for j, model in enumerate(model_names):\n",
    "                efficiency_data[i, j] = compatibility_matrix[device][model]['efficiency_score']\n",
    "                \n",
    "        im2 = axes[1, 0].imshow(efficiency_data, cmap='viridis', aspect='auto')\n",
    "        axes[1, 0].set_title('Efficiency Scores')\n",
    "        axes[1, 0].set_xlabel('Model')\n",
    "        axes[1, 0].set_ylabel('Device')\n",
    "        axes[1, 0].set_xticks(range(len(model_names)))\n",
    "        axes[1, 0].set_xticklabels(model_names, rotation=45, ha='right')\n",
    "        axes[1, 0].set_yticks(range(len(device_names)))\n",
    "        axes[1, 0].set_yticklabels(device_names)\n",
    "        plt.colorbar(im2, ax=axes[1, 0], label='Efficiency Score')\n",
    "        \n",
    "        # Device constraints comparison\n",
    "        constraints = ['max_memory_mb', 'max_model_size_mb', 'target_latency_ms']\n",
    "        x = np.arange(len(device_names))\n",
    "        width = 0.25\n",
    "        \n",
    "        for i, constraint in enumerate(constraints):\n",
    "            values = [devices[d][constraint] for d in device_names]\n",
    "            # Normalize for visualization\n",
    "            if constraint == 'max_memory_mb':\n",
    "                values = [v / 1000 for v in values]  # Convert to GB\n",
    "            elif constraint == 'max_model_size_mb':\n",
    "                values = [v / 100 for v in values]  # Scale down\n",
    "            elif constraint == 'target_latency_ms':\n",
    "                values = [v / 100 for v in values]  # Scale down\n",
    "                \n",
    "            axes[1, 1].bar(x + i * width, values, width, \n",
    "                          label=constraint.replace('_', ' ').replace('mb', '').replace('ms', '').title())\n",
    "        \n",
    "        axes[1, 1].set_xlabel('Device Type')\n",
    "        axes[1, 1].set_ylabel('Normalized Constraint Value')\n",
    "        axes[1, 1].set_title('Device Constraints Comparison')\n",
    "        axes[1, 1].set_xticks(x + width)\n",
    "        axes[1, 1].set_xticklabels(device_names)\n",
    "        axes[1, 1].legend()\n",
    "        \n",
    "        # Optimization effectiveness\n",
    "        original_size = models['Original']['size_mb']\n",
    "        compression_ratios = [original_size / models[m]['size_mb'] for m in model_names]\n",
    "        accuracy_drops = [models['Original']['accuracy'] - models[m]['accuracy'] for m in model_names]\n",
    "        \n",
    "        scatter3 = axes[1, 2].scatter(compression_ratios, accuracy_drops, \n",
    "                                    c=range(len(model_names)), s=100, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        for i, name in enumerate(model_names):\n",
    "            axes[1, 2].annotate(name, (compression_ratios[i], accuracy_drops[i]), \n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[1, 2].set_xlabel('Compression Ratio')\n",
    "        axes[1, 2].set_ylabel('Accuracy Drop (%)')\n",
    "        axes[1, 2].set_title('Compression vs Accuracy Trade-off')\n",
    "        axes[1, 2].set_xscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print deployment recommendations\n",
    "        print(\"\\nðŸ“± Edge Deployment Recommendations:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for device_name in device_names:\n",
    "            print(f\"\\n{device_name}:\")\n",
    "            \n",
    "            # Find best compatible model\n",
    "            best_model = None\n",
    "            best_score = 0\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                compat_info = compatibility_matrix[device_name][model_name]\n",
    "                if compat_info['compatible'] and compat_info['efficiency_score'] > best_score:\n",
    "                    best_model = model_name\n",
    "                    best_score = compat_info['efficiency_score']\n",
    "                    \n",
    "            if best_model:\n",
    "                model_stats = models[best_model]\n",
    "                print(f\"  âœ… Recommended: {best_model}\")\n",
    "                print(f\"     Size: {model_stats['size_mb']:.1f} MB\")\n",
    "                print(f\"     Latency: {model_stats['latency_ms']:.1f} ms\")\n",
    "                print(f\"     Accuracy: {model_stats['accuracy']:.1f}%\")\n",
    "                print(f\"     Efficiency Score: {best_score:.3f}\")\n",
    "            else:\n",
    "                print(f\"  âŒ No compatible model found\")\n",
    "                print(f\"     Consider: More aggressive optimization\")\n",
    "\n",
    "# Run edge optimization analysis\n",
    "edge_demo = EdgeOptimizationDemo()\n",
    "models, devices, compatibility_matrix = edge_demo.analyze_edge_deployment()\n",
    "edge_demo.visualize_edge_analysis(models, devices, compatibility_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Inference Pipeline\n",
    "\n",
    "Let's put it all together in a complete inference pipeline demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferencePipelineDemo:\n",
    "    \"\"\"Demonstrate a complete inference pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.pipeline_metrics = defaultdict(list)\n",
    "        \n",
    "    def simulate_end_to_end_pipeline(self, num_requests=1000):\n",
    "        \"\"\"Simulate complete inference pipeline.\"\"\"\n",
    "        \n",
    "        # Pipeline stages with their latencies\n",
    "        stages = {\n",
    "            'request_parsing': {'min_ms': 1, 'max_ms': 5, 'std_ms': 1},\n",
    "            'tokenization': {'min_ms': 2, 'max_ms': 10, 'std_ms': 2},\n",
    "            'batching_wait': {'min_ms': 0, 'max_ms': 50, 'std_ms': 15},\n",
    "            'model_inference': {'min_ms': 30, 'max_ms': 200, 'std_ms': 20},\n",
    "            'post_processing': {'min_ms': 1, 'max_ms': 8, 'std_ms': 2},\n",
    "            'response_formatting': {'min_ms': 1, 'max_ms': 3, 'std_ms': 0.5}\n",
    "        }\n",
    "        \n",
    "        results = {\n",
    "            'total_latency': [],\n",
    "            'stage_latencies': {stage: [] for stage in stages.keys()},\n",
    "            'cache_hits': [],\n",
    "            'batch_sizes': [],\n",
    "            'queue_times': []\n",
    "        }\n",
    "        \n",
    "        # Simulate cache\n",
    "        cache_hit_rate = 0.15  # 15% cache hit rate\n",
    "        \n",
    "        # Simulate batching\n",
    "        batch_queue = []\n",
    "        max_batch_size = 32\n",
    "        \n",
    "        print(\"Simulating end-to-end inference pipeline...\")\n",
    "        \n",
    "        for i in tqdm(range(num_requests)):\n",
    "            total_latency = 0\n",
    "            stage_times = {}\n",
    "            \n",
    "            # Check cache hit\n",
    "            cache_hit = np.random.random() < cache_hit_rate\n",
    "            results['cache_hits'].append(cache_hit)\n",
    "            \n",
    "            if cache_hit:\n",
    "                # Cache hit - much faster\n",
    "                total_latency = np.random.normal(5, 1)  # 5ms average for cache hit\n",
    "                for stage in stages.keys():\n",
    "                    stage_times[stage] = 0 if stage != 'request_parsing' else 1\n",
    "            else:\n",
    "                # Full pipeline\n",
    "                for stage, timing in stages.items():\n",
    "                    if stage == 'batching_wait':\n",
    "                        # Simulate dynamic batching\n",
    "                        batch_queue.append(i)\n",
    "                        \n",
    "                        if len(batch_queue) >= max_batch_size or (i % 50 == 0):\n",
    "                            # Process batch\n",
    "                            batch_size = len(batch_queue)\n",
    "                            results['batch_sizes'].append(batch_size)\n",
    "                            \n",
    "                            # Batching reduces per-request inference time\n",
    "                            inference_speedup = min(2.0, 1 + (batch_size - 1) * 0.02)\n",
    "                            stage_times['model_inference'] = max(10, \n",
    "                                np.random.normal(timing['min_ms'] + 50, timing['std_ms']) / inference_speedup\n",
    "                            )\n",
    "                            \n",
    "                            batch_queue = []\n",
    "                            wait_time = np.random.normal(timing['min_ms'] + 20, timing['std_ms'])\n",
    "                        else:\n",
    "                            wait_time = np.random.normal(timing['min_ms'] + 10, timing['std_ms'])\n",
    "                            \n",
    "                        stage_times[stage] = max(0, wait_time)\n",
    "                        \n",
    "                    else:\n",
    "                        # Normal stage processing\n",
    "                        if stage not in stage_times:  # Skip if already set by batching\n",
    "                            latency = np.random.normal(\n",
    "                                (timing['min_ms'] + timing['max_ms']) / 2,\n",
    "                                timing['std_ms']\n",
    "                            )\n",
    "                            stage_times[stage] = max(timing['min_ms'], latency)\n",
    "                            \n",
    "                total_latency = sum(stage_times.values())\n",
    "                \n",
    "            # Record results\n",
    "            results['total_latency'].append(total_latency)\n",
    "            for stage, latency in stage_times.items():\n",
    "                results['stage_latencies'][stage].append(latency)\n",
    "                \n",
    "        return results\n",
    "        \n",
    "    def visualize_pipeline_analysis(self, results):\n",
    "        \"\"\"Visualize pipeline performance analysis.\"\"\"\n",
    "        fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "        \n",
    "        # Total latency distribution\n",
    "        axes[0, 0].hist(results['total_latency'], bins=50, alpha=0.7, color='skyblue')\n",
    "        axes[0, 0].axvline(np.mean(results['total_latency']), color='red', \n",
    "                          linestyle='--', label=f'Mean: {np.mean(results[\"total_latency\"]):.1f}ms')\n",
    "        axes[0, 0].axvline(np.percentile(results['total_latency'], 95), color='orange', \n",
    "                          linestyle='--', label=f'P95: {np.percentile(results[\"total_latency\"], 95):.1f}ms')\n",
    "        axes[0, 0].set_title('Total Latency Distribution')\n",
    "        axes[0, 0].set_xlabel('Latency (ms)')\n",
    "        axes[0, 0].set_ylabel('Frequency')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Stage latency breakdown\n",
    "        stage_names = list(results['stage_latencies'].keys())\n",
    "        stage_means = [np.mean(results['stage_latencies'][stage]) for stage in stage_names]\n",
    "        \n",
    "        bars = axes[0, 1].bar(range(len(stage_names)), stage_means)\n",
    "        axes[0, 1].set_title('Average Latency by Stage')\n",
    "        axes[0, 1].set_xlabel('Pipeline Stage')\n",
    "        axes[0, 1].set_ylabel('Average Latency (ms)')\n",
    "        axes[0, 1].set_xticks(range(len(stage_names)))\n",
    "        axes[0, 1].set_xticklabels([s.replace('_', '\\n') for s in stage_names], rotation=0)\n",
    "        \n",
    "        # Cache hit impact\n",
    "        cache_hits = np.array(results['cache_hits'])\n",
    "        cache_hit_latencies = np.array(results['total_latency'])[cache_hits]\n",
    "        cache_miss_latencies = np.array(results['total_latency'])[~cache_hits]\n",
    "        \n",
    "        axes[1, 0].hist(cache_hit_latencies, bins=30, alpha=0.7, \n",
    "                       label=f'Cache Hits (n={len(cache_hit_latencies)})', color='green')\n",
    "        axes[1, 0].hist(cache_miss_latencies, bins=30, alpha=0.7, \n",
    "                       label=f'Cache Misses (n={len(cache_miss_latencies)})', color='red')\n",
    "        axes[1, 0].set_title('Latency: Cache Hits vs Misses')\n",
    "        axes[1, 0].set_xlabel('Latency (ms)')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Batch size distribution\n",
    "        if results['batch_sizes']:\n",
    "            axes[1, 1].hist(results['batch_sizes'], bins=range(1, max(results['batch_sizes']) + 2), \n",
    "                           alpha=0.7, color='purple', edgecolor='black')\n",
    "            axes[1, 1].set_title('Batch Size Distribution')\n",
    "            axes[1, 1].set_xlabel('Batch Size')\n",
    "            axes[1, 1].set_ylabel('Frequency')\n",
    "            axes[1, 1].axvline(np.mean(results['batch_sizes']), color='red', \n",
    "                              linestyle='--', label=f'Mean: {np.mean(results[\"batch_sizes\"]):.1f}')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        # Latency over time (showing trends)\n",
    "        window_size = 50\n",
    "        rolling_latency = []\n",
    "        for i in range(window_size, len(results['total_latency'])):\n",
    "            rolling_latency.append(\n",
    "                np.mean(results['total_latency'][i-window_size:i])\n",
    "            )\n",
    "            \n",
    "        axes[2, 0].plot(rolling_latency, alpha=0.8)\n",
    "        axes[2, 0].set_title(f'Rolling Average Latency (window={window_size})')\n",
    "        axes[2, 0].set_xlabel('Request Number')\n",
    "        axes[2, 0].set_ylabel('Latency (ms)')\n",
    "        axes[2, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Performance percentiles\n",
    "        percentiles = [50, 75, 90, 95, 99]\n",
    "        latency_percentiles = [np.percentile(results['total_latency'], p) for p in percentiles]\n",
    "        \n",
    "        bars = axes[2, 1].bar(range(len(percentiles)), latency_percentiles, \n",
    "                             color='lightcoral', alpha=0.7)\n",
    "        axes[2, 1].set_title('Latency Percentiles')\n",
    "        axes[2, 1].set_xlabel('Percentile')\n",
    "        axes[2, 1].set_ylabel('Latency (ms)')\n",
    "        axes[2, 1].set_xticks(range(len(percentiles)))\n",
    "        axes[2, 1].set_xticklabels([f'P{p}' for p in percentiles])\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, value) in enumerate(zip(bars, latency_percentiles)):\n",
    "            axes[2, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                           f'{value:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        print(\"\\nðŸ­ End-to-End Pipeline Performance Summary:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Overall metrics\n",
    "        total_requests = len(results['total_latency'])\n",
    "        cache_hit_rate = np.mean(results['cache_hits']) * 100\n",
    "        avg_latency = np.mean(results['total_latency'])\n",
    "        p95_latency = np.percentile(results['total_latency'], 95)\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Overall Performance:\")\n",
    "        print(f\"  Total Requests: {total_requests:,}\")\n",
    "        print(f\"  Cache Hit Rate: {cache_hit_rate:.1f}%\")\n",
    "        print(f\"  Average Latency: {avg_latency:.2f} ms\")\n",
    "        print(f\"  P95 Latency: {p95_latency:.2f} ms\")\n",
    "        print(f\"  Theoretical Throughput: {1000 / avg_latency:.1f} RPS\")\n",
    "        \n",
    "        # Stage breakdown\n",
    "        print(f\"\\nâ±ï¸ Stage Latency Breakdown:\")\n",
    "        total_stage_time = sum(np.mean(results['stage_latencies'][stage]) for stage in stage_names)\n",
    "        \n",
    "        for stage in stage_names:\n",
    "            avg_time = np.mean(results['stage_latencies'][stage])\n",
    "            percentage = (avg_time / total_stage_time) * 100\n",
    "            print(f\"  {stage.replace('_', ' ').title():<20}: {avg_time:>6.2f} ms ({percentage:>5.1f}%)\")\n",
    "        \n",
    "        # Cache impact analysis\n",
    "        if len(cache_hit_latencies) > 0 and len(cache_miss_latencies) > 0:\n",
    "            cache_speedup = np.mean(cache_miss_latencies) / np.mean(cache_hit_latencies)\n",
    "            print(f\"\\nðŸ’¾ Cache Impact:\")\n",
    "            print(f\"  Cache Hit Latency: {np.mean(cache_hit_latencies):.2f} ms\")\n",
    "            print(f\"  Cache Miss Latency: {np.mean(cache_miss_latencies):.2f} ms\")\n",
    "            print(f\"  Cache Speedup: {cache_speedup:.1f}x\")\n",
    "        \n",
    "        # Batching analysis\n",
    "        if results['batch_sizes']:\n",
    "            avg_batch_size = np.mean(results['batch_sizes'])\n",
    "            batch_efficiency = avg_batch_size / max(results['batch_sizes'])\n",
    "            print(f\"\\nðŸ“¦ Batching Analysis:\")\n",
    "            print(f\"  Average Batch Size: {avg_batch_size:.1f}\")\n",
    "            print(f\"  Batch Efficiency: {batch_efficiency:.1%}\")\n",
    "            print(f\"  Total Batches: {len(results['batch_sizes'])}\")\n",
    "        \n",
    "        # SLA compliance\n",
    "        sla_threshold = 100  # 100ms SLA\n",
    "        sla_compliance = np.mean(np.array(results['total_latency']) <= sla_threshold) * 100\n",
    "        print(f\"\\nðŸ“‹ SLA Compliance:\")\n",
    "        print(f\"  SLA Threshold: {sla_threshold} ms\")\n",
    "        print(f\"  Compliance Rate: {sla_compliance:.1f}%\")\n",
    "        \n",
    "        if sla_compliance < 99.0:\n",
    "            print(f\"  âš ï¸  SLA compliance below 99% - consider optimization\")\n",
    "        else:\n",
    "            print(f\"  âœ… SLA compliance meets target\")\n",
    "\n",
    "# Run complete pipeline demonstration\n",
    "pipeline_demo = InferencePipelineDemo()\n",
    "pipeline_results = pipeline_demo.simulate_end_to_end_pipeline(num_requests=2000)\n",
    "pipeline_demo.visualize_pipeline_analysis(pipeline_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Key Takeaways\n",
    "\n",
    "**Production Serving Fundamentals:**\n",
    "- Monitor latency, throughput, availability, and cost\n",
    "- Design for 99.9%+ availability with proper error handling\n",
    "- Balance resource utilization (70-90% target)\n",
    "\n",
    "**Batching Strategies:**\n",
    "- Static batching: Simple but may cause delays\n",
    "- Dynamic batching: Better latency-throughput balance\n",
    "- Continuous batching: Optimal for generation tasks\n",
    "- Bucket batching: Reduces padding overhead\n",
    "\n",
    "**Memory Optimizations:**\n",
    "- KV cache provides exponential speedup for generation\n",
    "- Response caching reduces duplicate computations\n",
    "- Memory pools prevent allocation overhead\n",
    "\n",
    "**Scaling and Load Balancing:**\n",
    "- Least-loaded and latency-aware routing perform best\n",
    "- Auto-scaling based on multiple metrics (CPU, latency, queue)\n",
    "- Conservative scale-down to avoid thrashing\n",
    "\n",
    "**Edge Deployment:**\n",
    "- Aggressive optimization needed for resource constraints\n",
    "- Quantization + pruning provides best size reduction\n",
    "- Trade-off analysis crucial for deployment decisions\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "Ready to evaluate your scaled models? Continue to [Topic 14: Evaluation and Safety](../14-evaluation-safety/) to learn comprehensive evaluation methodologies and safety measures for production LLMs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
