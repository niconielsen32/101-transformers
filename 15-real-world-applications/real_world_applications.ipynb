{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world Applications\n",
    "\n",
    "Welcome to the final topic! In this notebook, we'll explore practical implementations of transformer models across various domains. We'll build production-ready applications including chatbots, translation systems, code generators, and more.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Build production-ready transformer applications\n",
    "- Implement conversational AI systems\n",
    "- Create machine translation pipelines\n",
    "- Develop code generation assistants\n",
    "- Deploy content creation systems\n",
    "- Understand enterprise integration patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "import asyncio\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Application Architecture Overview\n",
    "\n",
    "Let's start by understanding the common architecture patterns for transformer applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize typical application architecture\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Components\n",
    "components = {\n",
    "    'User Interface': (2, 8, 'lightblue'),\n",
    "    'API Gateway': (4, 8, 'lightgreen'),\n",
    "    'Load Balancer': (6, 8, 'lightyellow'),\n",
    "    'Model Servers': (8, 8, 'lightcoral'),\n",
    "    'Cache': (10, 8, 'lightgray'),\n",
    "    'Auth Service': (4, 6, 'lightblue'),\n",
    "    'Rate Limiter': (4, 4, 'lightgreen'),\n",
    "    'Vector DB': (8, 6, 'lightyellow'),\n",
    "    'Monitoring': (6, 2, 'lightcoral')\n",
    "}\n",
    "\n",
    "# Draw components\n",
    "for comp, (x, y, color) in components.items():\n",
    "    plt.scatter(x, y, s=2000, c=color, edgecolors='black', linewidth=2)\n",
    "    plt.text(x, y, comp, ha='center', va='center', fontsize=10, weight='bold')\n",
    "\n",
    "# Draw connections\n",
    "connections = [\n",
    "    ((2, 8), (4, 8)),\n",
    "    ((4, 8), (6, 8)),\n",
    "    ((6, 8), (8, 8)),\n",
    "    ((8, 8), (10, 8)),\n",
    "    ((4, 8), (4, 6)),\n",
    "    ((4, 8), (4, 4)),\n",
    "    ((8, 8), (8, 6)),\n",
    "    ((6, 8), (6, 2)),\n",
    "    ((8, 8), (6, 2))\n",
    "]\n",
    "\n",
    "for (x1, y1), (x2, y2) in connections:\n",
    "    plt.arrow(x1, y1, x2-x1, y2-y1, head_width=0.2, head_length=0.1, \n",
    "              fc='gray', ec='gray', alpha=0.5)\n",
    "\n",
    "plt.xlim(0, 12)\n",
    "plt.ylim(0, 10)\n",
    "plt.axis('off')\n",
    "plt.title('Transformer Application Architecture', fontsize=16, weight='bold')\n",
    "plt.show()\n",
    "\n",
    "print(\"Key architectural components:\")\n",
    "print(\"1. API Gateway: Routes requests and handles authentication\")\n",
    "print(\"2. Load Balancer: Distributes traffic across model servers\")\n",
    "print(\"3. Model Servers: Run transformer inference\")\n",
    "print(\"4. Cache: Stores frequent responses\")\n",
    "print(\"5. Vector DB: Stores embeddings for retrieval\")\n",
    "print(\"6. Monitoring: Tracks performance and usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a Conversational AI System\n",
    "\n",
    "Let's implement a production-ready chatbot with advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple transformer model for demonstration\n",
    "class SimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size=30000, d_model=512, nhead=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.Parameter(torch.randn(1, 1000, d_model))\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.output_proj = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.transformer(x)\n",
    "        return self.output_proj(x)\n",
    "    \n",
    "    def generate(self, prompt_ids, max_length=50, temperature=0.7):\n",
    "        \"\"\"Simple generation method.\"\"\"\n",
    "        self.eval()\n",
    "        generated = prompt_ids.clone()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_length):\n",
    "                outputs = self(generated)\n",
    "                next_token_logits = outputs[:, -1, :] / temperature\n",
    "                probs = F.softmax(next_token_logits, dim=-1)\n",
    "                next_token = torch.multinomial(probs, 1)\n",
    "                generated = torch.cat([generated, next_token], dim=1)\n",
    "                \n",
    "                if next_token.item() == 2:  # EOS token\n",
    "                    break\n",
    "                    \n",
    "        return generated\n",
    "\n",
    "# Create a simple model\n",
    "model = SimpleTransformer().to(device)\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement conversation manager\n",
    "class ConversationManager:\n",
    "    \"\"\"Manage conversation history and context.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_history=10, context_window=2048):\n",
    "        self.conversations = {}\n",
    "        self.max_history = max_history\n",
    "        self.context_window = context_window\n",
    "        \n",
    "    def get_history(self, conversation_id: str) -> List[Dict]:\n",
    "        \"\"\"Get conversation history.\"\"\"\n",
    "        return self.conversations.get(conversation_id, [])\n",
    "    \n",
    "    def add_turn(self, conversation_id: str, user_msg: str, assistant_msg: str):\n",
    "        \"\"\"Add a conversation turn.\"\"\"\n",
    "        if conversation_id not in self.conversations:\n",
    "            self.conversations[conversation_id] = []\n",
    "            \n",
    "        self.conversations[conversation_id].append({\n",
    "            'user': user_msg,\n",
    "            'assistant': assistant_msg,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "        # Trim history if too long\n",
    "        if len(self.conversations[conversation_id]) > self.max_history:\n",
    "            self.conversations[conversation_id] = \\\n",
    "                self.conversations[conversation_id][-self.max_history:]\n",
    "    \n",
    "    def build_context(self, conversation_id: str, current_msg: str, \n",
    "                     persona: str = \"helpful assistant\") -> str:\n",
    "        \"\"\"Build context from history.\"\"\"\n",
    "        history = self.get_history(conversation_id)\n",
    "        \n",
    "        # Start with persona\n",
    "        context_parts = [f\"You are a {persona}.\\n\"]\n",
    "        \n",
    "        # Add recent history\n",
    "        for turn in history[-5:]:  # Last 5 turns\n",
    "            context_parts.append(f\"User: {turn['user']}\")\n",
    "            context_parts.append(f\"Assistant: {turn['assistant']}\")\n",
    "            \n",
    "        # Add current message\n",
    "        context_parts.append(f\"User: {current_msg}\")\n",
    "        context_parts.append(\"Assistant:\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "# Test conversation manager\n",
    "conv_manager = ConversationManager()\n",
    "\n",
    "# Simulate conversation\n",
    "conv_id = \"test_123\"\n",
    "conv_manager.add_turn(conv_id, \"Hello!\", \"Hi there! How can I help you today?\")\n",
    "conv_manager.add_turn(conv_id, \"What's the weather?\", \"I'd be happy to help with weather information.\")\n",
    "\n",
    "context = conv_manager.build_context(conv_id, \"Thanks!\")\n",
    "print(\"Built context:\")\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement safety filter\n",
    "class SafetyFilter:\n",
    "    \"\"\"Filter unsafe content.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.unsafe_patterns = [\n",
    "            'violence', 'hate', 'harassment', 'self-harm',\n",
    "            'sexual', 'illegal', 'deception'\n",
    "        ]\n",
    "        \n",
    "    def is_safe(self, text: str) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Check if text is safe.\"\"\"\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        # Check for unsafe patterns\n",
    "        for pattern in self.unsafe_patterns:\n",
    "            if pattern in text_lower:\n",
    "                return False, f\"Content may contain {pattern}\"\n",
    "                \n",
    "        return True, None\n",
    "    \n",
    "    def filter_response(self, response: str) -> str:\n",
    "        \"\"\"Filter or modify unsafe response.\"\"\"\n",
    "        is_safe, reason = self.is_safe(response)\n",
    "        \n",
    "        if not is_safe:\n",
    "            return \"I'm sorry, but I can't provide that type of response.\"\n",
    "            \n",
    "        return response\n",
    "\n",
    "# Implement response cache\n",
    "class ResponseCache:\n",
    "    \"\"\"Cache frequent responses.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_size=1000, ttl=3600):\n",
    "        self.cache = {}\n",
    "        self.access_times = {}\n",
    "        self.max_size = max_size\n",
    "        self.ttl = ttl\n",
    "        \n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get cached response.\"\"\"\n",
    "        if key in self.cache:\n",
    "            # Check if expired\n",
    "            if time.time() - self.access_times[key] < self.ttl:\n",
    "                self.access_times[key] = time.time()  # Update access time\n",
    "                return self.cache[key]\n",
    "            else:\n",
    "                # Remove expired entry\n",
    "                del self.cache[key]\n",
    "                del self.access_times[key]\n",
    "                \n",
    "        return None\n",
    "    \n",
    "    def set(self, key: str, value: str):\n",
    "        \"\"\"Cache response.\"\"\"\n",
    "        # Evict oldest if at capacity\n",
    "        if len(self.cache) >= self.max_size:\n",
    "            oldest_key = min(self.access_times, key=self.access_times.get)\n",
    "            del self.cache[oldest_key]\n",
    "            del self.access_times[oldest_key]\n",
    "            \n",
    "        self.cache[key] = value\n",
    "        self.access_times[key] = time.time()\n",
    "\n",
    "# Test safety and caching\n",
    "safety_filter = SafetyFilter()\n",
    "cache = ResponseCache()\n",
    "\n",
    "# Test safety filter\n",
    "test_messages = [\n",
    "    \"How do I bake a cake?\",\n",
    "    \"Tell me about violence\",  # Unsafe\n",
    "    \"What's the capital of France?\"\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    is_safe, reason = safety_filter.is_safe(msg)\n",
    "    print(f\"Message: '{msg}' - Safe: {is_safe}\")\n",
    "    if not is_safe:\n",
    "        print(f\"  Reason: {reason}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete conversational AI system\n",
    "class ConversationalAI:\n",
    "    \"\"\"Production-ready conversational AI.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config=None):\n",
    "        self.model = model\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # Components\n",
    "        self.conv_manager = ConversationManager()\n",
    "        self.safety_filter = SafetyFilter()\n",
    "        self.cache = ResponseCache()\n",
    "        \n",
    "        # Metrics\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'cache_hits': 0,\n",
    "            'safety_blocks': 0,\n",
    "            'avg_latency': 0\n",
    "        }\n",
    "        \n",
    "    def chat(self, message: str, conversation_id: str = \"default\") -> Dict[str, Any]:\n",
    "        \"\"\"Process chat message.\"\"\"\n",
    "        start_time = time.time()\n",
    "        self.metrics['total_requests'] += 1\n",
    "        \n",
    "        # Safety check on input\n",
    "        is_safe, reason = self.safety_filter.is_safe(message)\n",
    "        if not is_safe:\n",
    "            self.metrics['safety_blocks'] += 1\n",
    "            return {\n",
    "                'response': \"I'm sorry, but I can't process that type of message.\",\n",
    "                'filtered': True,\n",
    "                'reason': reason\n",
    "            }\n",
    "        \n",
    "        # Check cache\n",
    "        cache_key = f\"{conversation_id}:{message}\"\n",
    "        cached_response = self.cache.get(cache_key)\n",
    "        if cached_response:\n",
    "            self.metrics['cache_hits'] += 1\n",
    "            return {\n",
    "                'response': cached_response,\n",
    "                'cached': True,\n",
    "                'latency': time.time() - start_time\n",
    "            }\n",
    "        \n",
    "        # Build context\n",
    "        context = self.conv_manager.build_context(\n",
    "            conversation_id, message, \n",
    "            self.config.get('persona', 'helpful AI assistant')\n",
    "        )\n",
    "        \n",
    "        # Generate response (simplified)\n",
    "        response = self._generate_response(context)\n",
    "        \n",
    "        # Safety filter on output\n",
    "        response = self.safety_filter.filter_response(response)\n",
    "        \n",
    "        # Update conversation history\n",
    "        self.conv_manager.add_turn(conversation_id, message, response)\n",
    "        \n",
    "        # Cache response\n",
    "        self.cache.set(cache_key, response)\n",
    "        \n",
    "        # Update metrics\n",
    "        latency = time.time() - start_time\n",
    "        self.metrics['avg_latency'] = (\n",
    "            self.metrics['avg_latency'] * 0.9 + latency * 0.1\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'response': response,\n",
    "            'latency': latency,\n",
    "            'conversation_id': conversation_id\n",
    "        }\n",
    "    \n",
    "    def _generate_response(self, context: str) -> str:\n",
    "        \"\"\"Generate response from model.\"\"\"\n",
    "        # Simplified tokenization\n",
    "        tokens = [hash(word) % 30000 for word in context.split()]\n",
    "        input_ids = torch.tensor([tokens]).to(device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            output_ids = self.model.generate(input_ids, max_length=50)\n",
    "            \n",
    "        # Simplified decoding\n",
    "        response = f\"Generated response to: {context.split('User:')[-1].split('Assistant:')[0].strip()}\"\n",
    "        return response\n",
    "    \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get system metrics.\"\"\"\n",
    "        metrics = self.metrics.copy()\n",
    "        if metrics['total_requests'] > 0:\n",
    "            metrics['cache_hit_rate'] = metrics['cache_hits'] / metrics['total_requests']\n",
    "            metrics['safety_block_rate'] = metrics['safety_blocks'] / metrics['total_requests']\n",
    "        return metrics\n",
    "\n",
    "# Create and test chatbot\n",
    "chatbot = ConversationalAI(model, config={'persona': 'friendly AI assistant'})\n",
    "\n",
    "# Simulate conversation\n",
    "test_messages = [\n",
    "    \"Hello! How are you?\",\n",
    "    \"What can you help me with?\",\n",
    "    \"Tell me a joke\",\n",
    "    \"Hello! How are you?\"  # Duplicate to test caching\n",
    "]\n",
    "\n",
    "for msg in test_messages:\n",
    "    result = chatbot.chat(msg, \"demo_session\")\n",
    "    print(f\"\\nUser: {msg}\")\n",
    "    print(f\"Assistant: {result['response']}\")\n",
    "    print(f\"Latency: {result.get('latency', 0):.3f}s\")\n",
    "    if result.get('cached'):\n",
    "        print(\"(Retrieved from cache)\")\n",
    "\n",
    "# Show metrics\n",
    "print(\"\\nChatbot Metrics:\")\n",
    "for key, value in chatbot.get_metrics().items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Machine Translation System\n",
    "\n",
    "Let's build a production-ready translation system with quality estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language detection (simplified)\n",
    "class LanguageDetector:\n",
    "    \"\"\"Detect language of text.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Simple word-based detection\n",
    "        self.language_words = {\n",
    "            'en': ['the', 'is', 'are', 'and', 'to', 'of'],\n",
    "            'es': ['el', 'la', 'es', 'son', 'de', 'que'],\n",
    "            'fr': ['le', 'la', 'est', 'sont', 'de', 'que'],\n",
    "            'de': ['der', 'die', 'das', 'ist', 'sind', 'und']\n",
    "        }\n",
    "        \n",
    "    def detect(self, text: str) -> str:\n",
    "        \"\"\"Detect language of text.\"\"\"\n",
    "        text_lower = text.lower().split()\n",
    "        scores = {}\n",
    "        \n",
    "        for lang, words in self.language_words.items():\n",
    "            score = sum(1 for word in text_lower if word in words)\n",
    "            scores[lang] = score\n",
    "            \n",
    "        return max(scores, key=scores.get) if scores else 'en'\n",
    "\n",
    "# Quality estimator\n",
    "class TranslationQualityEstimator:\n",
    "    \"\"\"Estimate translation quality.\"\"\"\n",
    "    \n",
    "    def estimate(self, source: str, translation: str, \n",
    "                source_lang: str, target_lang: str) -> float:\n",
    "        \"\"\"Estimate quality score (0-1).\"\"\"\n",
    "        # Simple heuristics\n",
    "        score = 1.0\n",
    "        \n",
    "        # Length ratio check\n",
    "        length_ratio = len(translation.split()) / max(len(source.split()), 1)\n",
    "        if length_ratio < 0.5 or length_ratio > 2.0:\n",
    "            score *= 0.8\n",
    "            \n",
    "        # Check for repeated words (indication of errors)\n",
    "        words = translation.split()\n",
    "        if len(words) > 0:\n",
    "            unique_ratio = len(set(words)) / len(words)\n",
    "            if unique_ratio < 0.7:\n",
    "                score *= 0.9\n",
    "                \n",
    "        # Language pair difficulty\n",
    "        difficult_pairs = [('zh', 'en'), ('ar', 'en'), ('ja', 'en')]\n",
    "        if (source_lang, target_lang) in difficult_pairs:\n",
    "            score *= 0.95\n",
    "            \n",
    "        return max(0.0, min(1.0, score))\n",
    "\n",
    "# Test language detection\n",
    "detector = LanguageDetector()\n",
    "test_texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Hola, ¿cómo estás?\",\n",
    "    \"Bonjour, comment allez-vous?\",\n",
    "    \"Guten Tag, wie geht es Ihnen?\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    lang = detector.detect(text)\n",
    "    print(f\"Text: '{text}' -> Language: {lang}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation system\n",
    "class TranslationSystem:\n",
    "    \"\"\"Production translation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, models: Dict[str, nn.Module]):\n",
    "        self.models = models  # {\"en-es\": model, \"es-en\": model, ...}\n",
    "        self.detector = LanguageDetector()\n",
    "        self.quality_estimator = TranslationQualityEstimator()\n",
    "        self.cache = ResponseCache(max_size=5000)\n",
    "        \n",
    "        # Translation memories\n",
    "        self.translation_memory = self._load_translation_memory()\n",
    "        \n",
    "    def _load_translation_memory(self) -> Dict[str, Dict[str, str]]:\n",
    "        \"\"\"Load translation memory database.\"\"\"\n",
    "        # Simplified TM\n",
    "        return {\n",
    "            'en-es': {\n",
    "                'hello': 'hola',\n",
    "                'thank you': 'gracias',\n",
    "                'goodbye': 'adiós'\n",
    "            },\n",
    "            'es-en': {\n",
    "                'hola': 'hello',\n",
    "                'gracias': 'thank you',\n",
    "                'adiós': 'goodbye'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def translate(self, text: str, target_lang: str, \n",
    "                 source_lang: Optional[str] = None) -> Dict[str, Any]:\n",
    "        \"\"\"Translate text with quality estimation.\"\"\"\n",
    "        # Detect source language if not provided\n",
    "        if not source_lang:\n",
    "            source_lang = self.detector.detect(text)\n",
    "            \n",
    "        # Check if translation needed\n",
    "        if source_lang == target_lang:\n",
    "            return {\n",
    "                'translation': text,\n",
    "                'source_language': source_lang,\n",
    "                'target_language': target_lang,\n",
    "                'quality_score': 1.0,\n",
    "                'method': 'no_translation_needed'\n",
    "            }\n",
    "            \n",
    "        # Check cache\n",
    "        cache_key = f\"{source_lang}-{target_lang}:{text}\"\n",
    "        cached = self.cache.get(cache_key)\n",
    "        if cached:\n",
    "            return json.loads(cached)\n",
    "            \n",
    "        # Check translation memory\n",
    "        lang_pair = f\"{source_lang}-{target_lang}\"\n",
    "        if lang_pair in self.translation_memory:\n",
    "            tm = self.translation_memory[lang_pair]\n",
    "            if text.lower() in tm:\n",
    "                translation = tm[text.lower()]\n",
    "                result = {\n",
    "                    'translation': translation,\n",
    "                    'source_language': source_lang,\n",
    "                    'target_language': target_lang,\n",
    "                    'quality_score': 1.0,\n",
    "                    'method': 'translation_memory'\n",
    "                }\n",
    "                self.cache.set(cache_key, json.dumps(result))\n",
    "                return result\n",
    "                \n",
    "        # Use model\n",
    "        if lang_pair in self.models:\n",
    "            translation = self._translate_with_model(text, self.models[lang_pair])\n",
    "        else:\n",
    "            # Try pivot translation through English\n",
    "            translation = self._pivot_translate(text, source_lang, target_lang)\n",
    "            \n",
    "        # Estimate quality\n",
    "        quality_score = self.quality_estimator.estimate(\n",
    "            text, translation, source_lang, target_lang\n",
    "        )\n",
    "        \n",
    "        result = {\n",
    "            'translation': translation,\n",
    "            'source_language': source_lang,\n",
    "            'target_language': target_lang,\n",
    "            'quality_score': quality_score,\n",
    "            'method': 'neural_translation'\n",
    "        }\n",
    "        \n",
    "        # Cache result\n",
    "        self.cache.set(cache_key, json.dumps(result))\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def _translate_with_model(self, text: str, model: nn.Module) -> str:\n",
    "        \"\"\"Translate using neural model.\"\"\"\n",
    "        # Simplified - in practice, use proper tokenization\n",
    "        return f\"[Translated] {text}\"\n",
    "        \n",
    "    def _pivot_translate(self, text: str, source_lang: str, target_lang: str) -> str:\n",
    "        \"\"\"Translate through pivot language (English).\"\"\"\n",
    "        # First translate to English\n",
    "        if source_lang != 'en':\n",
    "            intermediate = self._translate_with_model(\n",
    "                text, self.models.get(f\"{source_lang}-en\", None)\n",
    "            )\n",
    "        else:\n",
    "            intermediate = text\n",
    "            \n",
    "        # Then translate to target\n",
    "        if target_lang != 'en':\n",
    "            final = self._translate_with_model(\n",
    "                intermediate, self.models.get(f\"en-{target_lang}\", None)\n",
    "            )\n",
    "        else:\n",
    "            final = intermediate\n",
    "            \n",
    "        return final\n",
    "    \n",
    "    def batch_translate(self, texts: List[str], target_lang: str,\n",
    "                       source_lang: Optional[str] = None) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Translate multiple texts efficiently.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # Group by detected language if not provided\n",
    "        if not source_lang:\n",
    "            lang_groups = defaultdict(list)\n",
    "            for text in texts:\n",
    "                lang = self.detector.detect(text)\n",
    "                lang_groups[lang].append(text)\n",
    "        else:\n",
    "            lang_groups = {source_lang: texts}\n",
    "            \n",
    "        # Translate each group\n",
    "        for lang, group_texts in lang_groups.items():\n",
    "            for text in group_texts:\n",
    "                result = self.translate(text, target_lang, lang)\n",
    "                results.append(result)\n",
    "                \n",
    "        return results\n",
    "\n",
    "# Create translation system with dummy models\n",
    "translation_models = {\n",
    "    'en-es': model,  # Reusing our simple model\n",
    "    'es-en': model,\n",
    "    'en-fr': model,\n",
    "    'fr-en': model\n",
    "}\n",
    "\n",
    "translator = TranslationSystem(translation_models)\n",
    "\n",
    "# Test translations\n",
    "test_cases = [\n",
    "    (\"Hello\", \"es\"),\n",
    "    (\"Thank you\", \"es\"),\n",
    "    (\"How are you today?\", \"fr\"),\n",
    "    (\"Hola\", \"en\", \"es\")\n",
    "]\n",
    "\n",
    "for test in test_cases:\n",
    "    if len(test) == 2:\n",
    "        text, target = test\n",
    "        result = translator.translate(text, target)\n",
    "    else:\n",
    "        text, target, source = test\n",
    "        result = translator.translate(text, target, source)\n",
    "        \n",
    "    print(f\"\\nSource: {result['source_language']} -> Target: {result['target_language']}\")\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translation: {result['translation']}\")\n",
    "    print(f\"Quality Score: {result['quality_score']:.2f}\")\n",
    "    print(f\"Method: {result['method']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Code Generation Assistant\n",
    "\n",
    "Let's build an AI-powered code generation and assistance system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code analyzer\n",
    "class CodeAnalyzer:\n",
    "    \"\"\"Analyze code for various properties.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.language_patterns = {\n",
    "            'python': {'def ', 'import ', 'class ', ':', 'print('},\n",
    "            'javascript': {'function ', 'const ', 'let ', '=>', 'console.log'},\n",
    "            'java': {'public ', 'class ', 'void ', 'static ', 'System.out'}\n",
    "        }\n",
    "        \n",
    "    def detect_language(self, code: str) -> str:\n",
    "        \"\"\"Detect programming language.\"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for lang, patterns in self.language_patterns.items():\n",
    "            score = sum(1 for pattern in patterns if pattern in code)\n",
    "            scores[lang] = score\n",
    "            \n",
    "        return max(scores, key=scores.get) if scores else 'unknown'\n",
    "    \n",
    "    def analyze_complexity(self, code: str) -> Dict[str, int]:\n",
    "        \"\"\"Analyze code complexity.\"\"\"\n",
    "        lines = code.split('\\n')\n",
    "        \n",
    "        return {\n",
    "            'lines_of_code': len(lines),\n",
    "            'non_empty_lines': len([l for l in lines if l.strip()]),\n",
    "            'functions': code.count('def ') + code.count('function '),\n",
    "            'classes': code.count('class '),\n",
    "            'loops': code.count('for ') + code.count('while '),\n",
    "            'conditions': code.count('if ') + code.count('else')\n",
    "        }\n",
    "    \n",
    "    def check_syntax_patterns(self, code: str, language: str) -> List[str]:\n",
    "        \"\"\"Check for common syntax issues.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        if language == 'python':\n",
    "            # Check indentation (simplified)\n",
    "            lines = code.split('\\n')\n",
    "            for i, line in enumerate(lines):\n",
    "                if line.strip() and line[0] not in ' \\t' and i > 0:\n",
    "                    prev_line = lines[i-1].strip()\n",
    "                    if prev_line.endswith(':'):\n",
    "                        issues.append(f\"Line {i+1}: Expected indentation after ':'\")\n",
    "                        \n",
    "        return issues\n",
    "\n",
    "# Security scanner\n",
    "class SecurityScanner:\n",
    "    \"\"\"Scan code for security issues.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vulnerability_patterns = {\n",
    "            'eval_usage': (r'\\beval\\s*\\(', 'Use of eval() is dangerous'),\n",
    "            'sql_injection': (r'\".*SELECT.*FROM.*\"\\s*\\+', 'Potential SQL injection'),\n",
    "            'hardcoded_secrets': (r'(password|api_key|secret)\\s*=\\s*[\"\\']\\w+[\"\\']', \n",
    "                                'Hardcoded secrets detected'),\n",
    "            'unsafe_random': (r'\\brandom\\.random\\(\\)', \n",
    "                            'Use of non-cryptographic random for security')\n",
    "        }\n",
    "        \n",
    "    def scan(self, code: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Scan code for security issues.\"\"\"\n",
    "        issues = []\n",
    "        \n",
    "        for vuln_type, (pattern, message) in self.vulnerability_patterns.items():\n",
    "            import re\n",
    "            if re.search(pattern, code, re.IGNORECASE):\n",
    "                issues.append({\n",
    "                    'type': vuln_type,\n",
    "                    'severity': 'high' if 'eval' in vuln_type else 'medium',\n",
    "                    'message': message\n",
    "                })\n",
    "                \n",
    "        return issues\n",
    "\n",
    "# Test code analyzer\n",
    "analyzer = CodeAnalyzer()\n",
    "scanner = SecurityScanner()\n",
    "\n",
    "test_code = \"\"\"def calculate_sum(numbers):\n",
    "    total = 0\n",
    "    for num in numbers:\n",
    "        total += num\n",
    "    return total\n",
    "\n",
    "class Calculator:\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "    \n",
    "    def add(self, x, y):\n",
    "        return x + y\n",
    "\"\"\"\n",
    "\n",
    "print(\"Code Analysis:\")\n",
    "print(f\"Detected language: {analyzer.detect_language(test_code)}\")\n",
    "print(f\"Complexity metrics: {analyzer.analyze_complexity(test_code)}\")\n",
    "\n",
    "# Test security scanner\n",
    "unsafe_code = 'result = eval(user_input)'\n",
    "security_issues = scanner.scan(unsafe_code)\n",
    "print(f\"\\nSecurity issues found: {len(security_issues)}\")\n",
    "for issue in security_issues:\n",
    "    print(f\"  - {issue['type']}: {issue['message']} (Severity: {issue['severity']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code generation assistant\n",
    "class CodeAssistant:\n",
    "    \"\"\"AI-powered code generation and assistance.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.analyzer = CodeAnalyzer()\n",
    "        self.scanner = SecurityScanner()\n",
    "        self.code_templates = self._load_templates()\n",
    "        \n",
    "    def _load_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"Load code templates.\"\"\"\n",
    "        return {\n",
    "            'python_function': '''def {name}({params}):\n",
    "    \"\"\"{docstring}\"\"\"\n",
    "    {body}\n",
    "''',\n",
    "            'python_class': '''class {name}:\n",
    "    \"\"\"{docstring}\"\"\"\n",
    "    \n",
    "    def __init__(self{params}):\n",
    "        {init_body}\n",
    "''',\n",
    "            'javascript_function': '''function {name}({params}) {{\n",
    "    // {description}\n",
    "    {body}\n",
    "}}\n",
    "'''\n",
    "        }\n",
    "        \n",
    "    def generate_code(self, request: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate code from request.\"\"\"\n",
    "        task_type = request.get('type', 'function')\n",
    "        language = request.get('language', 'python')\n",
    "        description = request.get('description', '')\n",
    "        \n",
    "        # Use template if available\n",
    "        template_key = f\"{language}_{task_type}\"\n",
    "        if template_key in self.code_templates:\n",
    "            code = self._generate_from_template(request, template_key)\n",
    "        else:\n",
    "            code = self._generate_freeform(description, language)\n",
    "            \n",
    "        # Analyze generated code\n",
    "        complexity = self.analyzer.analyze_complexity(code)\n",
    "        security_issues = self.scanner.scan(code)\n",
    "        \n",
    "        return {\n",
    "            'code': code,\n",
    "            'language': language,\n",
    "            'complexity': complexity,\n",
    "            'security_issues': security_issues,\n",
    "            'needs_review': len(security_issues) > 0\n",
    "        }\n",
    "        \n",
    "    def _generate_from_template(self, request: Dict, template_key: str) -> str:\n",
    "        \"\"\"Generate code using template.\"\"\"\n",
    "        template = self.code_templates[template_key]\n",
    "        \n",
    "        # Fill template (simplified)\n",
    "        code = template.format(\n",
    "            name=request.get('name', 'function_name'),\n",
    "            params=request.get('params', ''),\n",
    "            docstring=request.get('description', 'Description here'),\n",
    "            body='pass  # TODO: Implement',\n",
    "            init_body='pass  # TODO: Initialize',\n",
    "            description=request.get('description', '')\n",
    "        )\n",
    "        \n",
    "        return code\n",
    "        \n",
    "    def _generate_freeform(self, description: str, language: str) -> str:\n",
    "        \"\"\"Generate code without template.\"\"\"\n",
    "        # Simplified - would use model in practice\n",
    "        return f\"# Generated {language} code for: {description}\\n# TODO: Implement\\npass\"\n",
    "        \n",
    "    def complete_code(self, partial_code: str, context: Dict = None) -> Dict[str, Any]:\n",
    "        \"\"\"Complete partial code.\"\"\"\n",
    "        language = self.analyzer.detect_language(partial_code)\n",
    "        \n",
    "        # Find insertion point\n",
    "        lines = partial_code.split('\\n')\n",
    "        last_line = lines[-1] if lines else ''\n",
    "        \n",
    "        # Generate completion (simplified)\n",
    "        if 'def' in last_line and language == 'python':\n",
    "            completion = ':\\n    \"\"\"Function description.\"\"\"\\n    pass'\n",
    "        elif 'class' in last_line and language == 'python':\n",
    "            completion = ':\\n    \"\"\"Class description.\"\"\"\\n    \\n    def __init__(self):\\n        pass'\n",
    "        else:\n",
    "            completion = '  # TODO: Complete implementation'\n",
    "            \n",
    "        completed_code = partial_code + completion\n",
    "        \n",
    "        return {\n",
    "            'completed_code': completed_code,\n",
    "            'completion': completion,\n",
    "            'language': language,\n",
    "            'confidence': 0.85\n",
    "        }\n",
    "        \n",
    "    def explain_code(self, code: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate explanation for code.\"\"\"\n",
    "        language = self.analyzer.detect_language(code)\n",
    "        complexity = self.analyzer.analyze_complexity(code)\n",
    "        \n",
    "        # Generate explanation (simplified)\n",
    "        explanation = {\n",
    "            'summary': f\"This {language} code contains {complexity['functions']} functions and {complexity['classes']} classes.\",\n",
    "            'complexity': complexity,\n",
    "            'key_concepts': self._extract_concepts(code, language),\n",
    "            'suggestions': self._generate_suggestions(code, complexity)\n",
    "        }\n",
    "        \n",
    "        return explanation\n",
    "        \n",
    "    def _extract_concepts(self, code: str, language: str) -> List[str]:\n",
    "        \"\"\"Extract key programming concepts.\"\"\"\n",
    "        concepts = []\n",
    "        \n",
    "        if 'class' in code:\n",
    "            concepts.append('Object-Oriented Programming')\n",
    "        if 'for' in code or 'while' in code:\n",
    "            concepts.append('Loops')\n",
    "        if 'if' in code:\n",
    "            concepts.append('Conditional Logic')\n",
    "        if 'def' in code or 'function' in code:\n",
    "            concepts.append('Functions')\n",
    "            \n",
    "        return concepts\n",
    "        \n",
    "    def _generate_suggestions(self, code: str, complexity: Dict) -> List[str]:\n",
    "        \"\"\"Generate improvement suggestions.\"\"\"\n",
    "        suggestions = []\n",
    "        \n",
    "        if complexity['lines_of_code'] > 50:\n",
    "            suggestions.append(\"Consider breaking down into smaller functions\")\n",
    "        if complexity['functions'] == 0 and complexity['lines_of_code'] > 10:\n",
    "            suggestions.append(\"Consider organizing code into functions\")\n",
    "        if 'TODO' in code:\n",
    "            suggestions.append(\"Complete TODO items\")\n",
    "            \n",
    "        return suggestions\n",
    "\n",
    "# Create code assistant\n",
    "code_assistant = CodeAssistant(model)\n",
    "\n",
    "# Test code generation\n",
    "print(\"=== Code Generation ===\")\n",
    "generation_request = {\n",
    "    'type': 'function',\n",
    "    'language': 'python',\n",
    "    'name': 'calculate_fibonacci',\n",
    "    'params': 'n: int',\n",
    "    'description': 'Calculate the nth Fibonacci number'\n",
    "}\n",
    "\n",
    "result = code_assistant.generate_code(generation_request)\n",
    "print(f\"Generated {result['language']} code:\")\n",
    "print(result['code'])\n",
    "print(f\"Security issues: {len(result['security_issues'])}\")\n",
    "\n",
    "# Test code completion\n",
    "print(\"\\n=== Code Completion ===\")\n",
    "partial = \"def process_data(data\"\n",
    "completion_result = code_assistant.complete_code(partial)\n",
    "print(f\"Partial code: {partial}\")\n",
    "print(f\"Completion: {completion_result['completion']}\")\n",
    "print(f\"Confidence: {completion_result['confidence']}\")\n",
    "\n",
    "# Test code explanation\n",
    "print(\"\\n=== Code Explanation ===\")\n",
    "explanation = code_assistant.explain_code(test_code)\n",
    "print(f\"Summary: {explanation['summary']}\")\n",
    "print(f\"Key concepts: {', '.join(explanation['key_concepts'])}\")\n",
    "print(f\"Suggestions: {explanation['suggestions']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Content Generation System\n",
    "\n",
    "Let's build a system for generating various types of content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEO optimizer\n",
    "class SEOOptimizer:\n",
    "    \"\"\"Optimize content for search engines.\"\"\"\n",
    "    \n",
    "    def optimize(self, content: str, keywords: List[str]) -> str:\n",
    "        \"\"\"Optimize content for keywords.\"\"\"\n",
    "        optimized = content\n",
    "        \n",
    "        # Ensure keywords appear in content (simplified)\n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() not in optimized.lower():\n",
    "                # Add keyword naturally (simplified)\n",
    "                optimized += f\"\\n\\nThis content is relevant to {keyword}.\"\n",
    "                \n",
    "        return optimized\n",
    "        \n",
    "    def analyze_seo(self, content: str, keywords: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze SEO metrics.\"\"\"\n",
    "        content_lower = content.lower()\n",
    "        word_count = len(content.split())\n",
    "        \n",
    "        # Keyword density\n",
    "        keyword_counts = {}\n",
    "        for keyword in keywords:\n",
    "            count = content_lower.count(keyword.lower())\n",
    "            density = (count / word_count) * 100 if word_count > 0 else 0\n",
    "            keyword_counts[keyword] = {\n",
    "                'count': count,\n",
    "                'density': density\n",
    "            }\n",
    "            \n",
    "        # Calculate SEO score\n",
    "        total_keyword_presence = sum(1 for k in keywords if k.lower() in content_lower)\n",
    "        keyword_coverage = total_keyword_presence / len(keywords) if keywords else 0\n",
    "        \n",
    "        # Check for good SEO practices\n",
    "        has_headings = any(marker in content for marker in ['#', '<h1>', '<h2>'])\n",
    "        good_length = 300 <= word_count <= 2000\n",
    "        \n",
    "        seo_score = (\n",
    "            keyword_coverage * 0.4 +\n",
    "            (1.0 if has_headings else 0.0) * 0.3 +\n",
    "            (1.0 if good_length else 0.5) * 0.3\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'seo_score': seo_score,\n",
    "            'word_count': word_count,\n",
    "            'keyword_analysis': keyword_counts,\n",
    "            'has_headings': has_headings,\n",
    "            'recommendations': self._get_seo_recommendations(seo_score, keyword_counts)\n",
    "        }\n",
    "        \n",
    "    def _get_seo_recommendations(self, score: float, keyword_analysis: Dict) -> List[str]:\n",
    "        \"\"\"Generate SEO recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        if score < 0.7:\n",
    "            recommendations.append(\"Improve keyword usage and content structure\")\n",
    "            \n",
    "        for keyword, data in keyword_analysis.items():\n",
    "            if data['density'] < 0.5:\n",
    "                recommendations.append(f\"Increase usage of keyword '{keyword}'\")\n",
    "            elif data['density'] > 3.0:\n",
    "                recommendations.append(f\"Reduce keyword stuffing for '{keyword}'\")\n",
    "                \n",
    "        return recommendations\n",
    "\n",
    "# Test SEO optimizer\n",
    "seo_optimizer = SEOOptimizer()\n",
    "\n",
    "test_content = \"\"\"# Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn from data.\n",
    "In this guide, we'll explore the fundamentals of ML and its applications.\n",
    "\"\"\"\n",
    "\n",
    "keywords = ['machine learning', 'artificial intelligence', 'data science']\n",
    "\n",
    "seo_analysis = seo_optimizer.analyze_seo(test_content, keywords)\n",
    "print(\"SEO Analysis:\")\n",
    "print(f\"SEO Score: {seo_analysis['seo_score']:.2f}\")\n",
    "print(f\"Word Count: {seo_analysis['word_count']}\")\n",
    "print(\"\\nKeyword Analysis:\")\n",
    "for keyword, data in seo_analysis['keyword_analysis'].items():\n",
    "    print(f\"  {keyword}: {data['count']} occurrences ({data['density']:.1f}% density)\")\n",
    "print(f\"\\nRecommendations: {seo_analysis['recommendations']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content generator\n",
    "class ContentGenerator:\n",
    "    \"\"\"Multi-purpose content generation system.\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.seo_optimizer = SEOOptimizer()\n",
    "        self.templates = self._load_content_templates()\n",
    "        \n",
    "    def _load_content_templates(self) -> Dict[str, str]:\n",
    "        \"\"\"Load content templates.\"\"\"\n",
    "        return {\n",
    "            'blog_intro': \"\"\"{hook}\n",
    "\n",
    "In this article, we'll explore {topic} and discover {benefits}.\n",
    "\n",
    "## What is {topic}?\n",
    "\n",
    "{definition}\n",
    "\n",
    "## Why {topic} Matters\n",
    "\n",
    "{importance}\n",
    "\"\"\",\n",
    "            'product_description': \"\"\"{product_name} - {tagline}\n",
    "\n",
    "**Key Features:**\n",
    "{features}\n",
    "\n",
    "**Benefits:**\n",
    "{benefits}\n",
    "\n",
    "**Perfect for:** {target_audience}\n",
    "\"\"\",\n",
    "            'social_media': \"\"\"{emoji} {hook}\n",
    "\n",
    "{main_point}\n",
    "\n",
    "{call_to_action} {hashtags}\n",
    "\"\"\"\n",
    "        }\n",
    "        \n",
    "    def generate_article(self, topic: str, style: str = 'informative',\n",
    "                        keywords: List[str] = None, word_count: int = 500) -> Dict[str, Any]:\n",
    "        \"\"\"Generate complete article.\"\"\"\n",
    "        # Generate outline\n",
    "        outline = self._generate_outline(topic, word_count)\n",
    "        \n",
    "        # Generate content for each section\n",
    "        sections = []\n",
    "        current_words = 0\n",
    "        \n",
    "        for section in outline:\n",
    "            section_content = self._generate_section(\n",
    "                section['title'], \n",
    "                section['points'],\n",
    "                style,\n",
    "                target_words=(word_count - current_words) // (len(outline) - len(sections))\n",
    "            )\n",
    "            sections.append({\n",
    "                'title': section['title'],\n",
    "                'content': section_content\n",
    "            })\n",
    "            current_words += len(section_content.split())\n",
    "            \n",
    "        # Combine sections\n",
    "        article = self._combine_sections(sections)\n",
    "        \n",
    "        # SEO optimization\n",
    "        if keywords:\n",
    "            article = self.seo_optimizer.optimize(article, keywords)\n",
    "            \n",
    "        # Analyze final article\n",
    "        analysis = {\n",
    "            'word_count': len(article.split()),\n",
    "            'sections': len(sections),\n",
    "            'readability_score': self._calculate_readability(article)\n",
    "        }\n",
    "        \n",
    "        if keywords:\n",
    "            analysis['seo'] = self.seo_optimizer.analyze_seo(article, keywords)\n",
    "            \n",
    "        return {\n",
    "            'content': article,\n",
    "            'outline': outline,\n",
    "            'analysis': analysis\n",
    "        }\n",
    "        \n",
    "    def _generate_outline(self, topic: str, target_words: int) -> List[Dict]:\n",
    "        \"\"\"Generate article outline.\"\"\"\n",
    "        # Simplified outline generation\n",
    "        sections_count = max(3, min(7, target_words // 150))\n",
    "        \n",
    "        outline = [\n",
    "            {'title': 'Introduction', 'points': ['Hook', 'Overview', 'Thesis']},\n",
    "            {'title': f'Understanding {topic}', 'points': ['Definition', 'History', 'Importance']},\n",
    "        ]\n",
    "        \n",
    "        # Add middle sections\n",
    "        for i in range(sections_count - 3):\n",
    "            outline.append({\n",
    "                'title': f'Key Aspect {i+1}',\n",
    "                'points': ['Main idea', 'Examples', 'Analysis']\n",
    "            })\n",
    "            \n",
    "        outline.append({'title': 'Conclusion', 'points': ['Summary', 'Key takeaways', 'Call to action']})\n",
    "        \n",
    "        return outline\n",
    "        \n",
    "    def _generate_section(self, title: str, points: List[str], \n",
    "                         style: str, target_words: int) -> str:\n",
    "        \"\"\"Generate section content.\"\"\"\n",
    "        # Simplified content generation\n",
    "        content = f\"## {title}\\n\\n\"\n",
    "        \n",
    "        words_per_point = target_words // len(points)\n",
    "        \n",
    "        for point in points:\n",
    "            # Generate paragraph for each point\n",
    "            paragraph = f\"{point}: \" + \" \".join([\n",
    "                f\"This is content about {point.lower()} in {style} style.\"\n",
    "                for _ in range(words_per_point // 10)\n",
    "            ])\n",
    "            content += paragraph + \"\\n\\n\"\n",
    "            \n",
    "        return content\n",
    "        \n",
    "    def _combine_sections(self, sections: List[Dict]) -> str:\n",
    "        \"\"\"Combine sections into article.\"\"\"\n",
    "        article_parts = []\n",
    "        \n",
    "        for section in sections:\n",
    "            article_parts.append(section['content'])\n",
    "            \n",
    "        return \"\\n\".join(article_parts)\n",
    "        \n",
    "    def _calculate_readability(self, text: str) -> float:\n",
    "        \"\"\"Calculate readability score.\"\"\"\n",
    "        sentences = text.split('.')\n",
    "        words = text.split()\n",
    "        \n",
    "        if not sentences or not words:\n",
    "            return 0.0\n",
    "            \n",
    "        avg_sentence_length = len(words) / len(sentences)\n",
    "        \n",
    "        # Simple readability score (inverse of complexity)\n",
    "        if avg_sentence_length < 15:\n",
    "            return 0.9  # Very readable\n",
    "        elif avg_sentence_length < 20:\n",
    "            return 0.7  # Readable\n",
    "        elif avg_sentence_length < 25:\n",
    "            return 0.5  # Moderate\n",
    "        else:\n",
    "            return 0.3  # Complex\n",
    "            \n",
    "    def generate_marketing_copy(self, product_info: Dict[str, Any],\n",
    "                              platform: str = 'website') -> Dict[str, Any]:\n",
    "        \"\"\"Generate marketing copy.\"\"\"\n",
    "        if platform == 'website' and 'product_description' in self.templates:\n",
    "            template = self.templates['product_description']\n",
    "            \n",
    "            # Format features and benefits\n",
    "            features_text = '\\n'.join([f\"- {f}\" for f in product_info.get('features', [])])\n",
    "            benefits_text = '\\n'.join([f\"- {b}\" for b in product_info.get('benefits', [])])\n",
    "            \n",
    "            copy = template.format(\n",
    "                product_name=product_info.get('name', 'Product'),\n",
    "                tagline=product_info.get('tagline', 'Your perfect solution'),\n",
    "                features=features_text,\n",
    "                benefits=benefits_text,\n",
    "                target_audience=product_info.get('target_audience', 'everyone')\n",
    "            )\n",
    "        else:\n",
    "            # Generate without template\n",
    "            copy = f\"{product_info.get('name', 'Product')} - {product_info.get('tagline', 'Great product')}\\n\\n\"\n",
    "            copy += \"Discover the perfect solution for your needs.\"\n",
    "            \n",
    "        return {\n",
    "            'copy': copy,\n",
    "            'platform': platform,\n",
    "            'word_count': len(copy.split()),\n",
    "            'optimized_for': 'conversion'\n",
    "        }\n",
    "\n",
    "# Create content generator\n",
    "content_gen = ContentGenerator(model)\n",
    "\n",
    "# Test article generation\n",
    "print(\"=== Article Generation ===\")\n",
    "article_result = content_gen.generate_article(\n",
    "    topic=\"Artificial Intelligence\",\n",
    "    style=\"educational\",\n",
    "    keywords=[\"AI\", \"machine learning\", \"future\"],\n",
    "    word_count=300\n",
    ")\n",
    "\n",
    "print(f\"Generated article outline:\")\n",
    "for section in article_result['outline']:\n",
    "    print(f\"  - {section['title']}\")\n",
    "print(f\"\\nWord count: {article_result['analysis']['word_count']}\")\n",
    "print(f\"Readability score: {article_result['analysis']['readability_score']:.2f}\")\n",
    "\n",
    "# Test marketing copy\n",
    "print(\"\\n=== Marketing Copy Generation ===\")\n",
    "product_info = {\n",
    "    'name': 'SmartAssist Pro',\n",
    "    'tagline': 'Your AI-powered productivity companion',\n",
    "    'features': [\n",
    "        'Real-time suggestions',\n",
    "        'Multi-language support',\n",
    "        'Cloud synchronization'\n",
    "    ],\n",
    "    'benefits': [\n",
    "        'Save 2 hours daily',\n",
    "        'Reduce errors by 90%',\n",
    "        'Work from anywhere'\n",
    "    ],\n",
    "    'target_audience': 'busy professionals'\n",
    "}\n",
    "\n",
    "marketing_result = content_gen.generate_marketing_copy(product_info)\n",
    "print(\"Generated marketing copy:\")\n",
    "print(marketing_result['copy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Deployment Patterns\n",
    "\n",
    "Let's explore best practices for deploying transformer applications at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model server with batching\n",
    "class ModelServer:\n",
    "    \"\"\"Production model server with batching and optimization.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, config=None):\n",
    "        self.model = model\n",
    "        self.config = config or {}\n",
    "        \n",
    "        # Batching configuration\n",
    "        self.batch_size = self.config.get('batch_size', 32)\n",
    "        self.batch_timeout = self.config.get('batch_timeout', 0.1)  # seconds\n",
    "        \n",
    "        # Request queue\n",
    "        self.request_queue = []\n",
    "        self.results = {}\n",
    "        \n",
    "        # Performance monitoring\n",
    "        self.metrics = {\n",
    "            'total_requests': 0,\n",
    "            'total_batches': 0,\n",
    "            'avg_batch_size': 0,\n",
    "            'avg_latency': 0\n",
    "        }\n",
    "        \n",
    "    def process_request(self, request_id: str, input_data: Any) -> str:\n",
    "        \"\"\"Process single request.\"\"\"\n",
    "        self.metrics['total_requests'] += 1\n",
    "        \n",
    "        # Add to queue\n",
    "        self.request_queue.append({\n",
    "            'id': request_id,\n",
    "            'data': input_data,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "        \n",
    "        # Check if batch should be processed\n",
    "        if len(self.request_queue) >= self.batch_size or \\\n",
    "           (self.request_queue and \n",
    "            time.time() - self.request_queue[0]['timestamp'] > self.batch_timeout):\n",
    "            self._process_batch()\n",
    "            \n",
    "        # Wait for result (simplified - use async in practice)\n",
    "        while request_id not in self.results:\n",
    "            time.sleep(0.01)\n",
    "            \n",
    "        return self.results.pop(request_id)\n",
    "        \n",
    "    def _process_batch(self):\n",
    "        \"\"\"Process batch of requests.\"\"\"\n",
    "        if not self.request_queue:\n",
    "            return\n",
    "            \n",
    "        batch = self.request_queue[:self.batch_size]\n",
    "        self.request_queue = self.request_queue[self.batch_size:]\n",
    "        \n",
    "        self.metrics['total_batches'] += 1\n",
    "        self.metrics['avg_batch_size'] = (\n",
    "            self.metrics['avg_batch_size'] * 0.9 + len(batch) * 0.1\n",
    "        )\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Batch process (simplified)\n",
    "        batch_results = self._run_model_batch([r['data'] for r in batch])\n",
    "        \n",
    "        # Store results\n",
    "        for request, result in zip(batch, batch_results):\n",
    "            self.results[request['id']] = result\n",
    "            \n",
    "        # Update latency\n",
    "        latency = time.time() - start_time\n",
    "        self.metrics['avg_latency'] = (\n",
    "            self.metrics['avg_latency'] * 0.9 + latency * 0.1\n",
    "        )\n",
    "        \n",
    "    def _run_model_batch(self, batch_data: List[Any]) -> List[str]:\n",
    "        \"\"\"Run model on batch.\"\"\"\n",
    "        # Simplified batch processing\n",
    "        return [f\"Processed: {data}\" for data in batch_data]\n",
    "        \n",
    "    def get_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Get server metrics.\"\"\"\n",
    "        metrics = self.metrics.copy()\n",
    "        if metrics['total_batches'] > 0:\n",
    "            metrics['requests_per_batch'] = (\n",
    "                metrics['total_requests'] / metrics['total_batches']\n",
    "            )\n",
    "        return metrics\n",
    "\n",
    "# Load balancer\n",
    "class LoadBalancer:\n",
    "    \"\"\"Distribute requests across multiple model servers.\"\"\"\n",
    "    \n",
    "    def __init__(self, servers: List[ModelServer]):\n",
    "        self.servers = servers\n",
    "        self.current_server = 0\n",
    "        self.server_loads = [0] * len(servers)\n",
    "        \n",
    "    def route_request(self, request_id: str, input_data: Any) -> str:\n",
    "        \"\"\"Route request to least loaded server.\"\"\"\n",
    "        # Find least loaded server\n",
    "        min_load_idx = self.server_loads.index(min(self.server_loads))\n",
    "        \n",
    "        # Update load\n",
    "        self.server_loads[min_load_idx] += 1\n",
    "        \n",
    "        # Process request\n",
    "        result = self.servers[min_load_idx].process_request(request_id, input_data)\n",
    "        \n",
    "        # Update load\n",
    "        self.server_loads[min_load_idx] -= 1\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    def get_server_metrics(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Get metrics from all servers.\"\"\"\n",
    "        return [server.get_metrics() for server in self.servers]\n",
    "\n",
    "# Test deployment patterns\n",
    "print(\"=== Model Server Testing ===\")\n",
    "\n",
    "# Create model servers\n",
    "servers = [ModelServer(model, {'batch_size': 4}) for _ in range(3)]\n",
    "load_balancer = LoadBalancer(servers)\n",
    "\n",
    "# Simulate requests\n",
    "import uuid\n",
    "\n",
    "print(\"Simulating 20 requests...\")\n",
    "results = []\n",
    "for i in range(20):\n",
    "    request_id = str(uuid.uuid4())\n",
    "    result = load_balancer.route_request(request_id, f\"Request {i}\")\n",
    "    results.append(result)\n",
    "    \n",
    "print(f\"\\nProcessed {len(results)} requests\")\n",
    "\n",
    "# Show metrics\n",
    "print(\"\\nServer Metrics:\")\n",
    "for i, metrics in enumerate(load_balancer.get_server_metrics()):\n",
    "    print(f\"\\nServer {i+1}:\")\n",
    "    print(f\"  Total requests: {metrics['total_requests']}\")\n",
    "    print(f\"  Total batches: {metrics['total_batches']}\")\n",
    "    print(f\"  Avg batch size: {metrics['avg_batch_size']:.1f}\")\n",
    "    print(f\"  Avg latency: {metrics['avg_latency']:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production monitoring\n",
    "class ApplicationMonitor:\n",
    "    \"\"\"Monitor transformer applications in production.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = defaultdict(list)\n",
    "        self.alerts = []\n",
    "        self.thresholds = {\n",
    "            'error_rate': 0.05,\n",
    "            'latency_p99': 1.0,\n",
    "            'memory_usage': 0.9\n",
    "        }\n",
    "        \n",
    "    def record_metric(self, metric_name: str, value: float, timestamp: float = None):\n",
    "        \"\"\"Record metric value.\"\"\"\n",
    "        if timestamp is None:\n",
    "            timestamp = time.time()\n",
    "            \n",
    "        self.metrics[metric_name].append({\n",
    "            'value': value,\n",
    "            'timestamp': timestamp\n",
    "        })\n",
    "        \n",
    "        # Check thresholds\n",
    "        self._check_alerts(metric_name, value)\n",
    "        \n",
    "    def _check_alerts(self, metric_name: str, value: float):\n",
    "        \"\"\"Check if metric exceeds threshold.\"\"\"\n",
    "        if metric_name in self.thresholds:\n",
    "            if value > self.thresholds[metric_name]:\n",
    "                self.alerts.append({\n",
    "                    'metric': metric_name,\n",
    "                    'value': value,\n",
    "                    'threshold': self.thresholds[metric_name],\n",
    "                    'timestamp': time.time(),\n",
    "                    'severity': 'high' if value > self.thresholds[metric_name] * 1.5 else 'medium'\n",
    "                })\n",
    "                \n",
    "    def get_metrics_summary(self, window_minutes: int = 60) -> Dict[str, Dict[str, float]]:\n",
    "        \"\"\"Get metrics summary for time window.\"\"\"\n",
    "        current_time = time.time()\n",
    "        window_start = current_time - (window_minutes * 60)\n",
    "        \n",
    "        summary = {}\n",
    "        \n",
    "        for metric_name, values in self.metrics.items():\n",
    "            # Filter values in window\n",
    "            window_values = [\n",
    "                v['value'] for v in values \n",
    "                if v['timestamp'] >= window_start\n",
    "            ]\n",
    "            \n",
    "            if window_values:\n",
    "                summary[metric_name] = {\n",
    "                    'count': len(window_values),\n",
    "                    'mean': np.mean(window_values),\n",
    "                    'p50': np.percentile(window_values, 50),\n",
    "                    'p95': np.percentile(window_values, 95),\n",
    "                    'p99': np.percentile(window_values, 99),\n",
    "                    'max': max(window_values)\n",
    "                }\n",
    "                \n",
    "        return summary\n",
    "        \n",
    "    def get_recent_alerts(self, limit: int = 10) -> List[Dict]:\n",
    "        \"\"\"Get recent alerts.\"\"\"\n",
    "        return sorted(self.alerts, key=lambda x: x['timestamp'], reverse=True)[:limit]\n",
    "        \n",
    "    def visualize_metrics(self, metric_name: str, window_minutes: int = 60):\n",
    "        \"\"\"Visualize metric over time.\"\"\"\n",
    "        current_time = time.time()\n",
    "        window_start = current_time - (window_minutes * 60)\n",
    "        \n",
    "        # Get data points\n",
    "        data_points = [\n",
    "            v for v in self.metrics.get(metric_name, [])\n",
    "            if v['timestamp'] >= window_start\n",
    "        ]\n",
    "        \n",
    "        if not data_points:\n",
    "            print(f\"No data for metric: {metric_name}\")\n",
    "            return\n",
    "            \n",
    "        # Extract values and timestamps\n",
    "        timestamps = [(p['timestamp'] - window_start) / 60 for p in data_points]\n",
    "        values = [p['value'] for p in data_points]\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(timestamps, values, 'b-', alpha=0.7)\n",
    "        plt.scatter(timestamps, values, c='blue', alpha=0.5)\n",
    "        \n",
    "        # Add threshold line if exists\n",
    "        if metric_name in self.thresholds:\n",
    "            plt.axhline(y=self.thresholds[metric_name], color='r', \n",
    "                       linestyle='--', label=f'Threshold: {self.thresholds[metric_name]}')\n",
    "            \n",
    "        plt.xlabel('Time (minutes ago)')\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.title(f'{metric_name} over last {window_minutes} minutes')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Test monitoring\n",
    "monitor = ApplicationMonitor()\n",
    "\n",
    "# Simulate metrics over time\n",
    "print(\"Simulating application metrics...\")\n",
    "base_time = time.time() - 3600  # Start 1 hour ago\n",
    "\n",
    "for i in range(100):\n",
    "    timestamp = base_time + i * 36  # Every 36 seconds\n",
    "    \n",
    "    # Simulate metrics\n",
    "    latency = 0.2 + np.random.normal(0, 0.05) + (0.5 if i > 70 else 0)\n",
    "    error_rate = 0.01 + np.random.normal(0, 0.005) + (0.05 if i > 80 else 0)\n",
    "    memory_usage = 0.6 + np.random.normal(0, 0.1) + (0.3 if i > 90 else 0)\n",
    "    \n",
    "    monitor.record_metric('latency_p99', latency, timestamp)\n",
    "    monitor.record_metric('error_rate', error_rate, timestamp)\n",
    "    monitor.record_metric('memory_usage', memory_usage, timestamp)\n",
    "\n",
    "# Show metrics summary\n",
    "print(\"\\nMetrics Summary (last hour):\")\n",
    "summary = monitor.get_metrics_summary(60)\n",
    "for metric, stats in summary.items():\n",
    "    print(f\"\\n{metric}:\")\n",
    "    print(f\"  Mean: {stats['mean']:.3f}\")\n",
    "    print(f\"  P99: {stats['p99']:.3f}\")\n",
    "    print(f\"  Max: {stats['max']:.3f}\")\n",
    "\n",
    "# Show alerts\n",
    "alerts = monitor.get_recent_alerts()\n",
    "if alerts:\n",
    "    print(f\"\\nRecent Alerts ({len(alerts)} total):\")\n",
    "    for alert in alerts[:5]:\n",
    "        print(f\"  [{alert['severity'].upper()}] {alert['metric']}: {alert['value']:.3f} > {alert['threshold']}\")\n",
    "\n",
    "# Visualize metrics\n",
    "monitor.visualize_metrics('latency_p99', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "Let's summarize the key learnings and best practices for real-world transformer applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best practices summary\n",
    "best_practices = {\n",
    "    \"Architecture\": [\n",
    "        \"Use microservices architecture for scalability\",\n",
    "        \"Implement proper API gateway for routing\",\n",
    "        \"Use load balancing for high availability\",\n",
    "        \"Cache frequently requested responses\",\n",
    "        \"Implement circuit breakers for fault tolerance\"\n",
    "    ],\n",
    "    \"Performance\": [\n",
    "        \"Batch requests for efficient GPU utilization\",\n",
    "        \"Use model quantization for faster inference\",\n",
    "        \"Implement request prioritization\",\n",
    "        \"Monitor and optimize memory usage\",\n",
    "        \"Use asynchronous processing where possible\"\n",
    "    ],\n",
    "    \"Safety\": [\n",
    "        \"Implement comprehensive input/output filtering\",\n",
    "        \"Regular safety audits and red teaming\",\n",
    "        \"Monitor for harmful content\",\n",
    "        \"Implement rate limiting\",\n",
    "        \"Have incident response procedures\"\n",
    "    ],\n",
    "    \"Monitoring\": [\n",
    "        \"Track key performance metrics\",\n",
    "        \"Set up alerting for anomalies\",\n",
    "        \"Log all requests for debugging\",\n",
    "        \"Monitor model drift\",\n",
    "        \"Regular A/B testing\"\n",
    "    ],\n",
    "    \"User Experience\": [\n",
    "        \"Provide clear error messages\",\n",
    "        \"Implement graceful degradation\",\n",
    "        \"Show progress for long operations\",\n",
    "        \"Collect user feedback\",\n",
    "        \"Personalize responses when appropriate\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Visualize best practices\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Create hierarchical visualization\n",
    "y_pos = 0\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(best_practices)))\n",
    "\n",
    "for i, (category, practices) in enumerate(best_practices.items()):\n",
    "    # Category header\n",
    "    ax.text(0, y_pos, category, fontsize=14, weight='bold', color=colors[i])\n",
    "    y_pos -= 0.5\n",
    "    \n",
    "    # Practices\n",
    "    for practice in practices:\n",
    "        ax.text(0.5, y_pos, f\"• {practice}\", fontsize=10)\n",
    "        y_pos -= 0.3\n",
    "        \n",
    "    y_pos -= 0.3\n",
    "\n",
    "ax.set_xlim(-0.1, 10)\n",
    "ax.set_ylim(y_pos, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title('Best Practices for Real-world Transformer Applications', \n",
    "             fontsize=16, weight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n🎉 Congratulations!\")\n",
    "print(\"\\nYou've completed the comprehensive transformer tutorial series!\")\n",
    "print(\"\\nYou now have the knowledge to:\")\n",
    "print(\"✓ Build transformer models from scratch\")\n",
    "print(\"✓ Train and fine-tune at scale\")\n",
    "print(\"✓ Deploy production applications\")\n",
    "print(\"✓ Ensure safety and fairness\")\n",
    "print(\"✓ Monitor and optimize performance\")\n",
    "print(\"\\nContinue learning and building amazing applications with transformers!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}