{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Variants\n",
    "\n",
    "This notebook explores different transformer architectures including BERT, GPT, T5, and Vision Transformers. We'll implement each variant and understand their unique characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Tuple, Dict, List\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understanding the Three Paradigms\n",
    "\n",
    "Let's visualize the three main transformer paradigms: Encoder-only, Decoder-only, and Encoder-Decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three paradigms\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Encoder-only (BERT)\n",
    "ax = axes[0]\n",
    "ax.text(0.5, 0.9, 'Encoder-only (BERT)', ha='center', fontsize=14, weight='bold')\n",
    "ax.text(0.5, 0.7, 'Input: [CLS] The cat sat [SEP]', ha='center', fontsize=10)\n",
    "ax.arrow(0.5, 0.65, 0, -0.1, width=0.02, head_width=0.05, fc='blue', ec='blue')\n",
    "ax.text(0.5, 0.5, 'Bidirectional\\nSelf-Attention', ha='center', fontsize=10, \n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "ax.arrow(0.5, 0.35, 0, -0.1, width=0.02, head_width=0.05, fc='blue', ec='blue')\n",
    "ax.text(0.5, 0.2, 'Output: Contextualized\\nRepresentations', ha='center', fontsize=10)\n",
    "ax.text(0.5, 0.05, 'Use: Classification,\\nNER, QA', ha='center', fontsize=9, style='italic')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# Decoder-only (GPT)\n",
    "ax = axes[1]\n",
    "ax.text(0.5, 0.9, 'Decoder-only (GPT)', ha='center', fontsize=14, weight='bold')\n",
    "ax.text(0.5, 0.7, 'Input: The cat', ha='center', fontsize=10)\n",
    "ax.arrow(0.5, 0.65, 0, -0.1, width=0.02, head_width=0.05, fc='green', ec='green')\n",
    "ax.text(0.5, 0.5, 'Causal\\nSelf-Attention', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\"))\n",
    "ax.arrow(0.5, 0.35, 0, -0.1, width=0.02, head_width=0.05, fc='green', ec='green')\n",
    "ax.text(0.5, 0.2, 'Output: Next token\\npredictions', ha='center', fontsize=10)\n",
    "ax.text(0.5, 0.05, 'Use: Generation,\\nCompletion', ha='center', fontsize=9, style='italic')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# Encoder-Decoder (T5)\n",
    "ax = axes[2]\n",
    "ax.text(0.5, 0.9, 'Encoder-Decoder (T5)', ha='center', fontsize=14, weight='bold')\n",
    "ax.text(0.25, 0.7, 'Input:\\nTranslate:', ha='center', fontsize=9)\n",
    "ax.text(0.75, 0.7, 'Target:\\nTraduire:', ha='center', fontsize=9)\n",
    "ax.arrow(0.25, 0.6, 0, -0.08, width=0.015, head_width=0.04, fc='red', ec='red')\n",
    "ax.arrow(0.75, 0.6, 0, -0.08, width=0.015, head_width=0.04, fc='orange', ec='orange')\n",
    "ax.text(0.25, 0.45, 'Encoder', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightcoral\"))\n",
    "ax.text(0.75, 0.45, 'Decoder', ha='center', fontsize=10,\n",
    "        bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightsalmon\"))\n",
    "ax.arrow(0.35, 0.45, 0.3, 0, width=0.01, head_width=0.03, fc='purple', ec='purple')\n",
    "ax.text(0.5, 0.48, 'Cross-Attention', ha='center', fontsize=8)\n",
    "ax.arrow(0.25, 0.35, 0, -0.08, width=0.015, head_width=0.04, fc='red', ec='red')\n",
    "ax.arrow(0.75, 0.35, 0, -0.08, width=0.015, head_width=0.04, fc='orange', ec='orange')\n",
    "ax.text(0.5, 0.2, 'Output: Sequence-to-sequence', ha='center', fontsize=10)\n",
    "ax.text(0.5, 0.05, 'Use: Translation,\\nSummarization', ha='center', fontsize=9, style='italic')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BERT Implementation\n",
    "\n",
    "Let's implement BERT (Bidirectional Encoder Representations from Transformers) step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BERTConfig:\n",
    "    vocab_size: int = 30522\n",
    "    hidden_size: int = 768\n",
    "    num_hidden_layers: int = 12\n",
    "    num_attention_heads: int = 12\n",
    "    intermediate_size: int = 3072\n",
    "    hidden_dropout_prob: float = 0.1\n",
    "    attention_probs_dropout_prob: float = 0.1\n",
    "    max_position_embeddings: int = 512\n",
    "    type_vocab_size: int = 2  # For segment embeddings\n",
    "\n",
    "class BERTEmbeddings(nn.Module):\n",
    "    \"\"\"BERT embeddings: token + position + segment.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: BERTConfig):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids=None):\n",
    "        seq_length = input_ids.size(1)\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "        \n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = torch.zeros_like(input_ids)\n",
    "            \n",
    "        words_embeddings = self.word_embeddings(input_ids)\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
    "        \n",
    "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Test BERT embeddings\n",
    "config = BERTConfig(vocab_size=1000, hidden_size=128, num_hidden_layers=2)\n",
    "embeddings = BERTEmbeddings(config)\n",
    "\n",
    "# Example: two sentences\n",
    "input_ids = torch.tensor([[101, 7592, 1010, 2045, 102, 2129, 2024, 2017, 102]])\n",
    "token_type_ids = torch.tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1]])\n",
    "\n",
    "embedded = embeddings(input_ids, token_type_ids)\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Embedded shape: {embedded.shape}\")\n",
    "print(f\"\\nFirst 5 dimensions of first token embedding: {embedded[0, 0, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT's Masked Language Modeling (MLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlm_masks(input_ids, vocab_size, mask_prob=0.15, mask_token_id=103):\n",
    "    \"\"\"\n",
    "    Create masks for MLM training.\n",
    "    - 80% of the time: Replace with [MASK]\n",
    "    - 10% of the time: Replace with random token\n",
    "    - 10% of the time: Keep original\n",
    "    \"\"\"\n",
    "    labels = input_ids.clone()\n",
    "    \n",
    "    # Create random mask\n",
    "    probability_matrix = torch.full(labels.shape, mask_prob)\n",
    "    masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "    \n",
    "    # Don't mask special tokens (assuming [CLS]=101, [SEP]=102, [PAD]=0)\n",
    "    special_tokens_mask = (input_ids == 101) | (input_ids == 102) | (input_ids == 0)\n",
    "    masked_indices = masked_indices & ~special_tokens_mask\n",
    "    \n",
    "    # 80% of time, replace with [MASK]\n",
    "    indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "    input_ids[indices_replaced] = mask_token_id\n",
    "    \n",
    "    # 10% of time, replace with random token\n",
    "    indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "    random_words = torch.randint(vocab_size, labels.shape, dtype=torch.long)\n",
    "    input_ids[indices_random] = random_words[indices_random]\n",
    "    \n",
    "    # 10% of time, keep original (do nothing)\n",
    "    \n",
    "    # Only compute loss on masked tokens\n",
    "    labels[~masked_indices] = -100\n",
    "    \n",
    "    return input_ids, labels, masked_indices\n",
    "\n",
    "# Demonstrate MLM\n",
    "original_text = \"The quick brown fox jumps over the lazy dog\"\n",
    "tokens = original_text.split()\n",
    "vocab = {word: i+104 for i, word in enumerate(set(tokens))}  # Start from 104 to avoid special tokens\n",
    "vocab.update({'[CLS]': 101, '[SEP]': 102, '[MASK]': 103, '[PAD]': 0})\n",
    "\n",
    "# Convert to IDs\n",
    "input_ids = torch.tensor([[vocab['[CLS]']] + [vocab[word] for word in tokens] + [vocab['[SEP]']]])\n",
    "masked_input, labels, mask_indices = create_mlm_masks(input_ids.clone(), len(vocab))\n",
    "\n",
    "print(\"Original tokens:\", ['[CLS]'] + tokens + ['[SEP]'])\n",
    "print(\"\\nMasked input IDs:\", masked_input[0].tolist())\n",
    "print(\"\\nLabels (-100 means ignore):\", labels[0].tolist())\n",
    "print(\"\\nMasked positions:\", mask_indices[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPT Implementation\n",
    "\n",
    "Now let's implement GPT (Generative Pre-trained Transformer) with causal attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"GPT-style causal self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_embd, n_head, n_positions=1024, attn_pdrop=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embd % n_head == 0\n",
    "        \n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        \n",
    "        # Query, Key, Value projections\n",
    "        self.c_attn = nn.Linear(n_embd, 3 * n_embd)\n",
    "        self.c_proj = nn.Linear(n_embd, n_embd)\n",
    "        \n",
    "        self.attn_dropout = nn.Dropout(attn_pdrop)\n",
    "        \n",
    "        # Causal mask\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(n_positions, n_positions))\n",
    "                                     .view(1, 1, n_positions, n_positions))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        # Calculate query, key, values for all heads\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        \n",
    "        # Causal self-attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.c_proj(y)\n",
    "        \n",
    "        return y, att\n",
    "\n",
    "# Demonstrate causal attention\n",
    "seq_len = 8\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "\n",
    "causal_attn = CausalSelfAttention(n_embd, n_head, seq_len)\n",
    "x = torch.randn(1, seq_len, n_embd)\n",
    "output, attention = causal_attn(x)\n",
    "\n",
    "# Visualize causal mask\n",
    "plt.figure(figsize=(8, 6))\n",
    "mask = causal_attn.bias[0, 0, :seq_len, :seq_len]\n",
    "plt.imshow(mask, cmap='Blues', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Causal Attention Mask')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        plt.text(j, i, int(mask[i, j].item()), ha='center', va='center')\n",
    "plt.show()\n",
    "\n",
    "# Show actual attention pattern\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attention[0, 0].detach().numpy(), cmap='hot', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Causal Attention Pattern (Head 1)')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, input_ids, max_new_tokens=20, temperature=1.0, top_k=None):\n",
    "    \"\"\"\n",
    "    Generate text using GPT-style autoregressive generation.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids)\n",
    "            \n",
    "        # Get logits for the last position\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        \n",
    "        # Optional top-k sampling\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            \n",
    "        # Sample from the distribution\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Append to sequence\n",
    "        input_ids = torch.cat((input_ids, next_token), dim=1)\n",
    "        \n",
    "    return input_ids\n",
    "\n",
    "# Simple demo of generation process\n",
    "print(\"GPT Generation Process:\")\n",
    "print(\"1. Start with prompt: 'The cat'\")\n",
    "print(\"2. Model predicts next token probabilities\")\n",
    "print(\"3. Sample from distribution (e.g., 'sat' with p=0.3)\")\n",
    "print(\"4. Append to sequence: 'The cat sat'\")\n",
    "print(\"5. Repeat until desired length\")\n",
    "\n",
    "# Visualize generation probabilities\n",
    "vocab_example = ['the', 'cat', 'sat', 'on', 'mat', 'jumped', 'ran', 'slept']\n",
    "probs_example = [0.05, 0.02, 0.3, 0.2, 0.15, 0.1, 0.08, 0.1]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(vocab_example, probs_example)\n",
    "plt.title('Next Token Probabilities (Example)')\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Probability')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. T5 Implementation\n",
    "\n",
    "T5 (Text-to-Text Transfer Transformer) treats every NLP task as text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class T5RelativePositionBias(nn.Module):\n",
    "    \"\"\"T5 uses relative position biases instead of absolute position embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_buckets=32, max_distance=128, n_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_buckets = num_buckets\n",
    "        self.max_distance = max_distance\n",
    "        self.n_heads = n_heads\n",
    "        self.relative_attention_bias = nn.Embedding(num_buckets, n_heads)\n",
    "        \n",
    "    def _relative_position_bucket(self, relative_position):\n",
    "        \"\"\"Translate relative position to a bucket number.\"\"\"\n",
    "        ret = 0\n",
    "        n = -relative_position\n",
    "        \n",
    "        # Each bucket covers a range of positions\n",
    "        num_buckets = self.num_buckets\n",
    "        max_distance = self.max_distance\n",
    "        \n",
    "        # Half of the buckets are for positive positions\n",
    "        num_buckets //= 2\n",
    "        ret += (n < 0).long() * num_buckets\n",
    "        n = torch.abs(n)\n",
    "        \n",
    "        # Exact buckets for small positions\n",
    "        max_exact = num_buckets // 2\n",
    "        is_small = n < max_exact\n",
    "        \n",
    "        # Logarithmic buckets for large positions\n",
    "        val_if_large = max_exact + (\n",
    "            torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (num_buckets - max_exact)\n",
    "        ).long()\n",
    "        val_if_large = torch.min(val_if_large, torch.full_like(val_if_large, num_buckets - 1))\n",
    "        \n",
    "        ret += torch.where(is_small, n, val_if_large)\n",
    "        return ret\n",
    "    \n",
    "    def forward(self, query_length, key_length):\n",
    "        \"\"\"Compute relative position bias.\"\"\"\n",
    "        context_position = torch.arange(query_length, dtype=torch.long)[:, None]\n",
    "        memory_position = torch.arange(key_length, dtype=torch.long)[None, :]\n",
    "        \n",
    "        relative_position = memory_position - context_position\n",
    "        relative_position_bucket = self._relative_position_bucket(relative_position)\n",
    "        \n",
    "        values = self.relative_attention_bias(relative_position_bucket)\n",
    "        values = values.permute([2, 0, 1]).unsqueeze(0)\n",
    "        \n",
    "        return values\n",
    "\n",
    "# Demonstrate T5 relative position bias\n",
    "rel_pos_bias = T5RelativePositionBias()\n",
    "bias = rel_pos_bias(8, 8)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for head in range(4):\n",
    "    plt.subplot(2, 2, head + 1)\n",
    "    plt.imshow(bias[0, head].detach().numpy(), cmap='coolwarm', aspect='auto')\n",
    "    plt.colorbar()\n",
    "    plt.title(f'Relative Position Bias (Head {head + 1})')\n",
    "    plt.xlabel('Key Position')\n",
    "    plt.ylabel('Query Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# T5 task examples\n",
    "print(\"\\nT5 Text-to-Text Examples:\")\n",
    "t5_examples = [\n",
    "    (\"translate English to German: The house is wonderful.\", \"Das Haus ist wunderbar.\"),\n",
    "    (\"summarize: <long article text>\", \"<summary>\"),\n",
    "    (\"sentiment: This movie is terrible.\", \"negative\"),\n",
    "    (\"question: What is the capital? context: Paris is the capital of France.\", \"Paris\")\n",
    "]\n",
    "\n",
    "for input_text, output_text in t5_examples:\n",
    "    print(f\"\\nInput:  {input_text}\")\n",
    "    print(f\"Output: {output_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vision Transformer (ViT)\n",
    "\n",
    "Let's implement Vision Transformer which applies transformers to image patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"Split image into patches and embed them.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, \n",
    "                             kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "# Visualize patch extraction\n",
    "def visualize_patches(img_size=224, patch_size=16):\n",
    "    n_patches_per_dim = img_size // patch_size\n",
    "    \n",
    "    # Create a sample image with grid pattern\n",
    "    img = np.zeros((img_size, img_size, 3))\n",
    "    \n",
    "    # Add grid lines\n",
    "    for i in range(0, img_size, patch_size):\n",
    "        img[i:i+2, :] = [1, 0, 0]  # Red horizontal lines\n",
    "        img[:, i:i+2] = [1, 0, 0]  # Red vertical lines\n",
    "    \n",
    "    # Add some patches with different colors\n",
    "    colors = plt.cm.hsv(np.linspace(0, 1, n_patches_per_dim * n_patches_per_dim))[:, :3]\n",
    "    \n",
    "    for i in range(n_patches_per_dim):\n",
    "        for j in range(n_patches_per_dim):\n",
    "            patch_idx = i * n_patches_per_dim + j\n",
    "            y_start = i * patch_size + 2\n",
    "            y_end = (i + 1) * patch_size - 2\n",
    "            x_start = j * patch_size + 2\n",
    "            x_end = (j + 1) * patch_size - 2\n",
    "            \n",
    "            # Fill patch with color\n",
    "            img[y_start:y_end, x_start:x_end] = colors[patch_idx] * 0.5\n",
    "    \n",
    "    # Plot\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Original image\n",
    "    ax1.imshow(img)\n",
    "    ax1.set_title(f'Image divided into {n_patches_per_dim}x{n_patches_per_dim} patches')\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Patch sequence\n",
    "    ax2.text(0.5, 0.9, 'Patches as Sequence', ha='center', fontsize=14, weight='bold', transform=ax2.transAxes)\n",
    "    \n",
    "    # Draw patches as sequence\n",
    "    total_patches = n_patches_per_dim * n_patches_per_dim\n",
    "    for i in range(min(total_patches, 10)):  # Show first 10 patches\n",
    "        x = i * 0.08 + 0.1\n",
    "        rect = plt.Rectangle((x, 0.4), 0.06, 0.2, \n",
    "                           facecolor=colors[i], edgecolor='black', linewidth=2)\n",
    "        ax2.add_patch(rect)\n",
    "        ax2.text(x + 0.03, 0.3, str(i), ha='center', fontsize=10)\n",
    "    \n",
    "    ax2.text(0.92, 0.5, '...', ha='center', fontsize=16, transform=ax2.transAxes)\n",
    "    ax2.set_xlim(0, 1)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return total_patches\n",
    "\n",
    "# Visualize\n",
    "total_patches = visualize_patches(224, 16)\n",
    "print(f\"\\nTotal patches: {total_patches}\")\n",
    "print(f\"Each patch: 16x16 pixels\")\n",
    "print(f\"Sequence length: {total_patches} + 1 [CLS] token = {total_patches + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ViT Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Simplified Vision Transformer.\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
    "                 num_classes=1000, embed_dim=768, depth=12, num_heads=12):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Learnable [CLS] token and position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + num_patches, embed_dim))\n",
    "        \n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, \n",
    "            dim_feedforward=embed_dim * 4, activation='gelu'\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        # Classification head\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Patch embedding\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        # Add [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "        # Add positional embedding\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer encoding\n",
    "        x = x.transpose(0, 1)  # (seq_len, batch, dim)\n",
    "        x = self.transformer(x)\n",
    "        x = x.transpose(0, 1)  # (batch, seq_len, dim)\n",
    "        \n",
    "        # Classification: use [CLS] token\n",
    "        x = self.norm(x[:, 0])\n",
    "        x = self.head(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create mini ViT\n",
    "vit = VisionTransformer(img_size=32, patch_size=8, num_classes=10, \n",
    "                       embed_dim=128, depth=4, num_heads=4)\n",
    "\n",
    "# Test with random image\n",
    "img = torch.randn(2, 3, 32, 32)\n",
    "output = vit(img)\n",
    "print(f\"Input shape: {img.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in vit.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Architectures\n",
    "\n",
    "Let's compare the different transformer variants side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture comparison table\n",
    "comparison_data = {\n",
    "    'Architecture': ['BERT', 'GPT', 'T5', 'ViT'],\n",
    "    'Type': ['Encoder-only', 'Decoder-only', 'Encoder-Decoder', 'Encoder-only'],\n",
    "    'Attention': ['Bidirectional', 'Causal', 'Bidirectional + Causal', 'Bidirectional'],\n",
    "    'Position': ['Learned', 'Learned', 'Relative bias', 'Learned'],\n",
    "    'Pre-training': ['MLM + NSP', 'Next token', 'Span corruption', 'Supervised'],\n",
    "    'Best for': ['Understanding', 'Generation', 'Any seq2seq', 'Vision'],\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Style the dataframe\n",
    "styled_df = df.style.set_properties(**{\n",
    "    'text-align': 'center',\n",
    "    'font-size': '12px',\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('font-size', '14px'), ('text-align', 'center')]}\n",
    "])\n",
    "\n",
    "print(\"Transformer Architecture Comparison:\")\n",
    "display(styled_df)\n",
    "\n",
    "# Visualize attention patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# BERT: Bidirectional attention\n",
    "ax = axes[0, 0]\n",
    "bert_attn = np.ones((8, 8))\n",
    "ax.imshow(bert_attn, cmap='Blues', alpha=0.8)\n",
    "ax.set_title('BERT: Bidirectional Attention', fontsize=14)\n",
    "ax.set_xlabel('Keys')\n",
    "ax.set_ylabel('Queries')\n",
    "\n",
    "# GPT: Causal attention\n",
    "ax = axes[0, 1]\n",
    "gpt_attn = np.tril(np.ones((8, 8)))\n",
    "ax.imshow(gpt_attn, cmap='Greens', alpha=0.8)\n",
    "ax.set_title('GPT: Causal Attention', fontsize=14)\n",
    "ax.set_xlabel('Keys')\n",
    "ax.set_ylabel('Queries')\n",
    "\n",
    "# T5 Encoder: Bidirectional\n",
    "ax = axes[1, 0]\n",
    "t5_enc_attn = np.ones((8, 8))\n",
    "ax.imshow(t5_enc_attn, cmap='Reds', alpha=0.8)\n",
    "ax.set_title('T5 Encoder: Bidirectional', fontsize=14)\n",
    "ax.set_xlabel('Keys')\n",
    "ax.set_ylabel('Queries')\n",
    "\n",
    "# T5 Decoder: Causal + Cross-attention\n",
    "ax = axes[1, 1]\n",
    "# Show decoder self-attention (causal)\n",
    "t5_dec_attn = np.tril(np.ones((8, 8)))\n",
    "ax.imshow(t5_dec_attn, cmap='Oranges', alpha=0.8)\n",
    "ax.set_title('T5 Decoder: Causal Self-Attention', fontsize=14)\n",
    "ax.set_xlabel('Keys')\n",
    "ax.set_ylabel('Queries')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Choosing the Right Architecture\n",
    "\n",
    "Let's create a decision tree for selecting the appropriate transformer variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_architecture(task, requirements):\n",
    "    \"\"\"Recommend transformer architecture based on task and requirements.\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Task-based recommendations\n",
    "    if task == 'classification':\n",
    "        recommendations.append(('BERT', 'Best for understanding context'))\n",
    "    elif task == 'generation':\n",
    "        recommendations.append(('GPT', 'Designed for text generation'))\n",
    "    elif task == 'translation':\n",
    "        recommendations.append(('T5', 'Excellent for seq2seq tasks'))\n",
    "    elif task == 'image_classification':\n",
    "        recommendations.append(('ViT', 'State-of-the-art for vision'))\n",
    "    \n",
    "    # Requirement-based adjustments\n",
    "    if 'bidirectional' in requirements:\n",
    "        recommendations.append(('BERT/T5-Encoder', 'Full context awareness'))\n",
    "    if 'autoregressive' in requirements:\n",
    "        recommendations.append(('GPT/T5-Decoder', 'Sequential generation'))\n",
    "    if 'efficiency' in requirements:\n",
    "        recommendations.append(('DistilBERT/ALBERT', 'Compressed models'))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Interactive task selector\n",
    "tasks = {\n",
    "    'Text Classification': 'classification',\n",
    "    'Named Entity Recognition': 'classification',\n",
    "    'Question Answering': 'classification',\n",
    "    'Text Generation': 'generation',\n",
    "    'Story Completion': 'generation',\n",
    "    'Translation': 'translation',\n",
    "    'Summarization': 'translation',\n",
    "    'Image Classification': 'image_classification'\n",
    "}\n",
    "\n",
    "print(\"Task-Architecture Recommendations:\\n\")\n",
    "for task_name, task_type in tasks.items():\n",
    "    recs = recommend_architecture(task_type, [])\n",
    "    print(f\"{task_name}:\")\n",
    "    for arch, reason in recs:\n",
    "        print(f\"  → {arch}: {reason}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance and Efficiency Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size and performance comparison\n",
    "model_stats = {\n",
    "    'Model': ['BERT-Base', 'BERT-Large', 'GPT-2', 'GPT-3', 'T5-Small', 'T5-Large', 'ViT-Base', 'ViT-Large'],\n",
    "    'Parameters': ['110M', '340M', '1.5B', '175B', '60M', '770M', '86M', '307M'],\n",
    "    'Layers': [12, 24, 48, 96, 6, 24, 12, 24],\n",
    "    'Hidden Size': [768, 1024, 1600, 12288, 512, 1024, 768, 1024],\n",
    "    'Attention Heads': [12, 16, 25, 96, 8, 16, 12, 16],\n",
    "}\n",
    "\n",
    "df_models = pd.DataFrame(model_stats)\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Parameters comparison\n",
    "params_numeric = [110, 340, 1500, 175000, 60, 770, 86, 307]\n",
    "colors = ['blue', 'blue', 'green', 'green', 'red', 'red', 'purple', 'purple']\n",
    "\n",
    "ax1.bar(range(len(df_models)), params_numeric, color=colors, alpha=0.7)\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xticks(range(len(df_models)))\n",
    "ax1.set_xticklabels(df_models['Model'], rotation=45, ha='right')\n",
    "ax1.set_ylabel('Parameters (Millions)')\n",
    "ax1.set_title('Model Size Comparison (Log Scale)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Architecture comparison\n",
    "architectures = ['BERT', 'GPT', 'T5', 'ViT']\n",
    "properties = ['Bidirectional', 'Causal', 'Encoder-Decoder', 'Patch-based']\n",
    "use_cases = ['Understanding', 'Generation', 'Translation', 'Vision']\n",
    "\n",
    "x = np.arange(len(architectures))\n",
    "width = 0.25\n",
    "\n",
    "# Simple comparison metrics (illustrative)\n",
    "understanding = [0.9, 0.6, 0.8, 0.0]\n",
    "generation = [0.3, 0.95, 0.85, 0.0]\n",
    "efficiency = [0.7, 0.8, 0.6, 0.85]\n",
    "\n",
    "ax2.bar(x - width, understanding, width, label='Understanding', alpha=0.8)\n",
    "ax2.bar(x, generation, width, label='Generation', alpha=0.8)\n",
    "ax2.bar(x + width, efficiency, width, label='Efficiency', alpha=0.8)\n",
    "\n",
    "ax2.set_xlabel('Architecture')\n",
    "ax2.set_ylabel('Relative Score')\n",
    "ax2.set_title('Architecture Capabilities')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(architectures)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• BERT excels at understanding tasks but cannot generate text autoregressively\")\n",
    "print(\"• GPT is optimized for generation but lacks bidirectional context\")\n",
    "print(\"• T5 is versatile but requires more parameters for similar performance\")\n",
    "print(\"• ViT shows that transformers can excel beyond NLP\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **BERT (Encoder-only)**:\n",
    "   - Bidirectional attention sees full context\n",
    "   - Best for: Classification, NER, Question Answering\n",
    "   - Cannot generate text naturally\n",
    "\n",
    "2. **GPT (Decoder-only)**:\n",
    "   - Causal attention for autoregressive generation\n",
    "   - Best for: Text generation, completion, few-shot learning\n",
    "   - Limited understanding without bidirectional context\n",
    "\n",
    "3. **T5 (Encoder-Decoder)**:\n",
    "   - Flexible text-to-text framework\n",
    "   - Best for: Translation, summarization, any seq2seq task\n",
    "   - More parameters but very versatile\n",
    "\n",
    "4. **ViT (Vision Transformer)**:\n",
    "   - Applies transformers to image patches\n",
    "   - Best for: Image classification, vision tasks\n",
    "   - Shows transformers work beyond NLP\n",
    "\n",
    "### Selection Guide:\n",
    "\n",
    "```\n",
    "if task == \"understanding\":\n",
    "    use BERT or RoBERTa\n",
    "elif task == \"generation\":\n",
    "    use GPT-2, GPT-3, or GPT-4\n",
    "elif task == \"translation\" or \"summarization\":\n",
    "    use T5 or BART\n",
    "elif task == \"vision\":\n",
    "    use ViT or DEIT\n",
    "else:\n",
    "    consider task-specific variants\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}